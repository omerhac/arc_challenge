{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arc.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omerhac/arc_challenge/blob/master/arc_advanced_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YShrwCd2GUWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import json\n",
        "from google.cloud import storage\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import colors\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Conv2D, Lambda, Dense, Flatten, MaxPool2D, Input, BatchNormalization, Conv2DTranspose, UpSampling2D, Reshape\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uko3X-E_CrZr",
        "colab_type": "code",
        "outputId": "903c5b2d-3bef-492a-872a-c1d1b075e6ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# get repository from github\n",
        "!git clone https://github.com/omerhac/arc_challenge.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'arc_challenge'...\n",
            "remote: Enumerating objects: 114, done.\u001b[K\n",
            "remote: Counting objects: 100% (114/114), done.\u001b[K\n",
            "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
            "remote: Total 114 (delta 43), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (114/114), 10.10 MiB | 13.06 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcDHkw11CtSu",
        "colab_type": "code",
        "outputId": "dea0ba1d-e3dc-4054-c708-bd66dd606deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# navigate to directory\n",
        "cd arc_challenge"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/arc_challenge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX5E2ZNODM5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load repository dependencies\n",
        "import preprocess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF4Ovm97vXEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## constants ##\n",
        "BOARD_SIZE = (16,16) # board upperbound size\n",
        "SN_BATCH_SIZE = 64\n",
        "DECODER_BATCH_SIZE = 8\n",
        "DENSE_REP_SIZE = 1024 # dense vector represantation size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqOKR-POOXki",
        "colab_type": "text"
      },
      "source": [
        "# Load data / util funcitons\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-96BlT_hHVkF",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title load data\n",
        "def load_data():\n",
        "  \"\"\"\n",
        "  Loads all the data into training_tasks, eval_tasks and test_tasks\n",
        "  \"\"\"\n",
        "\n",
        "  ## get paths\n",
        "  GCS_PATH = \"gs://kds-d3cfb3d523ca35d2517017a78110126404d01fdea69417ce49950459\"\n",
        "  training_filenames = tf.io.gfile.glob(GCS_PATH + \"/training/*\")\n",
        "  test_filenames = tf.io.gfile.glob(GCS_PATH + \"/test/*\")\n",
        "  eval_filenames = tf.io.gfile.glob(GCS_PATH + \"/evaluation/*\")\n",
        "\n",
        "  # create datasets with filenames\n",
        "  training_dataset = tf.data.Dataset.list_files(training_filenames)\n",
        "  eval_dataset = tf.data.Dataset.list_files(eval_filenames)\n",
        "  test_dataset = tf.data.Dataset.list_files(test_filenames)\n",
        "\n",
        "  # load the jsons\n",
        "  def load_task(filename):\n",
        "    task_json = tf.io.read_file(filename)\n",
        "    return task_json\n",
        "\n",
        "  training_dataset = training_dataset.map(load_task)\n",
        "  eval_dataset = eval_dataset.map(load_task)\n",
        "  test_dataset = test_dataset.map(load_task)\n",
        "\n",
        "  training_dataset_numpy = tf.data.Dataset.as_numpy_iterator(training_dataset) # convert to numpy iterator\n",
        "  eval_dataset_numpy = tf.data.Dataset.as_numpy_iterator(eval_dataset)\n",
        "  test_dataset_numpy = tf.data.Dataset.as_numpy_iterator(test_dataset)\n",
        "\n",
        "  ## create a numpy array of tasks (n_tasks, )\n",
        "  def array_from_jsons(jsons_numpy_iterator):\n",
        "    \"\"\"\n",
        "      Create an array of task dictionaries from jsons numpy interator\n",
        "    \"\"\"\n",
        "\n",
        "    tasks = []\n",
        "    for task in jsons_numpy_iterator:\n",
        "      tasks.append(json.loads(task))\n",
        "\n",
        "    return np.stack(tasks)\n",
        "\n",
        "  ## get numpy arrays of datasets\n",
        "  training_tasks = array_from_jsons(training_dataset_numpy)\n",
        "  eval_tasks = array_from_jsons(eval_dataset_numpy)\n",
        "  test_tasks = array_from_jsons(test_dataset_numpy)\n",
        "\n",
        "  return training_tasks, eval_tasks, test_tasks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oOyaYHAJr1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e1f93b2f-7c24-493c-9dd6-8e0737a8cb2e"
      },
      "source": [
        "%%time\n",
        "training_tasks, eval_tasks, test_tasks = load_data()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 20.5 s, sys: 1.96 s, total: 22.5 s\n",
            "Wall time: 4min 36s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYLEt0yrPXf9",
        "colab_type": "text"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO9FwJjFNAnO",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Utilities\n",
        "def plot_board(board, ax, title=\"\"):\n",
        "  \"\"\"\n",
        "  Plot a board on a given axis\n",
        "  \"\"\"\n",
        "  cmap = colors.ListedColormap(\n",
        "      ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
        "        '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
        "  norm = colors.Normalize(vmin=0, vmax=9)\n",
        "  \n",
        "  ax.imshow(board, cmap=cmap, norm=norm)\n",
        "  ax.grid(True,which='both',color='lightgrey', linewidth=0.5)    \n",
        "  ax.set_yticks([x-0.5 for x in range(1+board.shape[0])])\n",
        "  ax.set_xticks([x-0.5 for x in range(1+board.shape[1])])     \n",
        "  ax.set_xticklabels([])\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_title(title)\n",
        "\n",
        "def plot_one(task, ax, i,train_or_test,input_or_output):\n",
        "  \"\"\"\n",
        "  Plot one task on a given axis\n",
        "  \"\"\"\n",
        "  cmap = colors.ListedColormap(\n",
        "      ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
        "        '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
        "  norm = colors.Normalize(vmin=0, vmax=9)\n",
        "  \n",
        "  input_matrix = task[train_or_test][i][input_or_output]\n",
        "  ax.imshow(input_matrix, cmap=cmap, norm=norm)\n",
        "  ax.grid(True,which='both',color='lightgrey', linewidth=0.5)    \n",
        "  ax.set_yticks([x-0.5 for x in range(1+len(input_matrix))])\n",
        "  ax.set_xticks([x-0.5 for x in range(1+len(input_matrix[0]))])     \n",
        "  ax.set_xticklabels([])\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_title(train_or_test + ' '+input_or_output)\n",
        "    \n",
        "\n",
        "def plot_task(task):\n",
        "    \"\"\"\n",
        "    Plots the first train and test pairs of a specified task,\n",
        "    using same color scheme as the ARC app\n",
        "    \"\"\"    \n",
        "    num_train = len(task['train'])\n",
        "    fig, axs = plt.subplots(2, num_train, figsize=(3*num_train,3*2))\n",
        "    for i in range(num_train):     \n",
        "        plot_one(task, axs[0,i],i,'train','input')\n",
        "        plot_one(task, axs[1,i],i,'train','output')        \n",
        "    plt.tight_layout()\n",
        "    plt.show()        \n",
        "        \n",
        "    num_test = len(task['test'])\n",
        "    fig, axs = plt.subplots(2, num_test, figsize=(3*num_test,3*2))\n",
        "    if num_test==1: \n",
        "        plot_one(task, axs[0],0,'test','input')\n",
        "        plot_one(task, axs[1],0,'test','output')     \n",
        "    else:\n",
        "        for i in range(num_test):      \n",
        "            plot_one(task, axs[0,i],i,'test','input')\n",
        "            plot_one(task, axs[1,i],i,'test','output')  \n",
        "    plt.tight_layout()\n",
        "    plt.show() \n",
        "  \n",
        "\n",
        "def plot_board_pairs(board_pairs, labels):\n",
        "  \"\"\"\n",
        "  Plots the board pairs (for siamese networks) with their label as a title\n",
        "  \"\"\"\n",
        "\n",
        "  fig, axs = plt.subplots(len(board_pairs), 2, figsize=(8, 3 * len(board_pairs)))\n",
        "  \n",
        "  for i, pair in enumerate(board_pairs):\n",
        "    # plot a pair on a given axis\n",
        "    plot_board(pair[0], axs[i, 0], title=\"anchor\") \n",
        "    plot_board(pair[1], axs[i, 1], title=labels[i])\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_decoder_boards(board_pairs):\n",
        "  \"\"\"\n",
        "  Plots the board pairs (for siamese networks) with their label as a title\n",
        "  \"\"\"\n",
        "\n",
        "  fig, axs = plt.subplots(len(board_pairs), 2, figsize=(8, 3 * len(board_pairs)))\n",
        "\n",
        "  for i, pair in enumerate(board_pairs):\n",
        "    # plot a pair on a given axis\n",
        "    plot_board(pair[0], axs[i, 0]) \n",
        "    plot_board(pair[1], axs[i, 1])\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def display_training_curves(hist, metric='accuracy', with_val=False):\n",
        "  \"\"\"display learning curves for keras history dict, args: history dict, with val --> boolean with/without val\"\"\"\n",
        "  plt.figure(figsize=(18,6))\n",
        "\n",
        "  # accuracy plots\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(hist[metric])\n",
        "  \n",
        "  if with_val:\n",
        "    plt.plot(hist['val_' + metric])\n",
        "    plt.legend(['Train', 'Validation'])\n",
        "  \n",
        "  else:\n",
        "    plt.legend(['Train'])\n",
        "  \n",
        "  plt.title('Model accuracy')\n",
        "  plt.xlabel('EPOCH')\n",
        "  plt.ylabel('Accuracy')\n",
        "\n",
        "  # loss plots\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(hist['loss'])\n",
        "\n",
        "  if with_val:\n",
        "    plt.plot(hist['val_loss'])\n",
        "    plt.legend(['Train loss', 'Val loss'])\n",
        "  \n",
        "  else:\n",
        "    plt.legend(['Train loss'])\n",
        "  \n",
        "  plt.title('Model loss')\n",
        "  plt.xlabel('EPOCH')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9AnAL0VU2Tu",
        "colab_type": "text"
      },
      "source": [
        "# Data generating functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuTeaiVASRhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset_shapes(dataset):\n",
        "  \"\"\"\n",
        "  Returns dataset board shapes. \n",
        "  \"\"\"\n",
        "\n",
        "  shape_0 = []\n",
        "  shape_1 = []\n",
        "  \n",
        "  # check every task\n",
        "  for task in dataset:\n",
        "    boards = get_task_boards(task) # get all boards\n",
        "    shape_0 += [board.shape[0] for board in boards]\n",
        "    shape_1 += [board.shape[1] for board in boards]\n",
        "  \n",
        "  return shape_0, shape_1\n",
        "\n",
        "def get_task_boards(task, threshold_shape=(40, 40), pad=None, test=False, divide_sets=False):\n",
        "  \"\"\"\n",
        "  Get the training / testing boards of every example in a specific task. \n",
        "\n",
        "  Args: threshold_shape --> threshold shape for which \n",
        "  biggger samples won't be returned. \n",
        "  pad --> functional padding function to pad boards with (up tp threshold_size).\n",
        "  test --> bool, whether a test task or not. for output of task example.\n",
        "  divide_sets --> bool, whether to divide boards sets to training - input / output, test - input / output.\n",
        "  \"\"\"\n",
        "  \n",
        "  training_input_boards, training_output_boards, test_input_boards, test_output_boards = [], [], [], []\n",
        "\n",
        "  # train boards\n",
        "  for example in task['train']:\n",
        "    input_board = np.array(example['input'])\n",
        "    output_board = np.array(example['output'])\n",
        "\n",
        "    if((input_board.shape[0] < threshold_shape[0]) and (input_board.shape[1] < threshold_shape[1])):\n",
        "      training_input_boards.append(pad(input_board, output_shape=threshold_shape) if pad else input_board) # check for padding func\n",
        "\n",
        "    if((output_board.shape[0] < threshold_shape[0]) and (output_board.shape[1] < threshold_shape[1])):\n",
        "      training_output_boards.append(pad(output_board, output_shape=threshold_shape) if pad else output_board) # check for padding func\n",
        "\n",
        "  # test boards\n",
        "  for example in task['test']:\n",
        "    input_board = np.array(example['input'])\n",
        "\n",
        "    if not test: # check if test example\n",
        "      output_board = np.array(example['output'])\n",
        "    \n",
        "    # check whether the board is smaller then threshold shape\n",
        "    if((input_board.shape[0] < threshold_shape[0]) and (input_board.shape[1] < threshold_shape[1])):\n",
        "      test_input_boards.append(pad(input_board, output_shape=threshold_shape) if pad else input_board) # check for padding func\n",
        "\n",
        "    if((output_board.shape[0] < threshold_shape[0]) and (output_board.shape[1] < threshold_shape[1]) and (not test)):\n",
        "      test_output_boards.append(pad(output_board, output_shape=threshold_shape) if pad else output_board) # check for padding func\n",
        "  \n",
        "  # whether to divide boards:\n",
        "  if divide_sets:\n",
        "    return training_input_boards, training_output_boards, test_input_boards, test_output_boards\n",
        "  else:\n",
        "    return training_input_boards + training_output_boards + test_input_boards + test_output_boards\n",
        "\n",
        "def pad(mat, output_shape, padder=0):\n",
        "  \"\"\"\n",
        "  Pad a matrix with padder up to output_shape. Insert matrix at upper left corner.\n",
        "  \n",
        "  Args:\n",
        "  mat - np.array matrix of rank 2\n",
        "  output_shape - tuple \n",
        "  padder - int\n",
        "  \"\"\"\n",
        "\n",
        "  output_board = np.zeros(shape=output_shape) + padder # create output board and pad it\n",
        "\n",
        "  # get input shape\n",
        "  input_rows = mat.shape[0]\n",
        "  input_cols = mat.shape[1]\n",
        "\n",
        "  # if random=False, insert input matrix in upper left corner\n",
        "  output_board[:input_rows, :input_cols] = mat\n",
        "  return output_board\n",
        "\n",
        "def random_pad(mat, output_shape, padder=0):\n",
        "  \"\"\"\n",
        "  Pad a matrix with padder up to output_shape. Insert the matrix at a random location\n",
        "  \n",
        "  Args:\n",
        "  mat - np.array matrix of rank 2\n",
        "  output_shape - tuple \n",
        "  padder - int\n",
        "  seed - int\n",
        "  \"\"\"\n",
        "\n",
        "  output_board = np.zeros(shape=output_shape) + padder # create output board and pad it\n",
        "\n",
        "  # get input shape\n",
        "  input_rows = mat.shape[0]\n",
        "  input_cols = mat.shape[1]\n",
        "\n",
        "  # insert mat at a random loacation\n",
        "  # get random location\n",
        "  start_row = np.random.randint(output_shape[0] - input_rows)\n",
        "  start_col = np.random.randint(output_shape[1] - input_cols)\n",
        "  # insert\n",
        "  output_board[start_row:start_row+input_rows, start_col:start_col+input_cols] = mat\n",
        "\n",
        "  return output_board\n",
        "\n",
        "def get_all_boards(training_tasks, eval_tasks, test_tasks):\n",
        "  \"\"\"\n",
        "  Extracts all the boards from all training/eval/testing tasks. \n",
        "  \"\"\"\n",
        "\n",
        "  training_boards = []\n",
        "  for task in training_tasks:\n",
        "    training_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=pad)\n",
        "\n",
        "    # augment training set (random pad)\n",
        "    training_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "    training_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "\n",
        "  eval_boards = []\n",
        "  for task in eval_tasks:\n",
        "    eval_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=pad)\n",
        "\n",
        "    # augment eval set (random pad)\n",
        "    eval_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "    eval_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "    \n",
        "\n",
        "  test_boards = []\n",
        "  for task in test_tasks:\n",
        "    test_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=pad, test=True)\n",
        "\n",
        "    # augment test set (random pad)\n",
        "    test_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad, test=True)\n",
        "    test_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad, test=True)\n",
        "\n",
        "  all_boards = training_boards+eval_boards+test_boards\n",
        "  return all_boards\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLXZsdbYodKl",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oofkpiCmt8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_rotated_views(board):\n",
        "  \"\"\"\n",
        "  Turns a board 90 deg counter clockwise 3 times. Returns a list length 4 with all the rotated views including the original one.\n",
        "  \"\"\"\n",
        "\n",
        "  rotates = [board]\n",
        "  for i in range(3):\n",
        "    board = np.rot90(board)\n",
        "    rotates.append(board)\n",
        "  \n",
        "  return rotates\n",
        "\n",
        "def get_rotated_data_pairs(rotated_views):\n",
        "  \"\"\"\n",
        "  Creates all the possible pairs out of the rotated views tensor and labels them. If a1..a4 are rotated vies (A.C.W) of a1, returns labels according to:\n",
        "   - (a1, a1) = 1 --> same board\n",
        "   - (a1, a2) = 2 --> 90 a.c.w rotate\n",
        "   - (a1, a3) = 3 --> 180 rotate\n",
        "   - (a1, a4) = 4 --> 270 a.c.w rotate\n",
        "   - ...\n",
        "   - (a3, a4) = 2 --> 90 a.c.w rotate\n",
        "\n",
        "  Args:\n",
        "  rotated_views = a list of 4 rotated views of the board\n",
        "\n",
        "  Returns:\n",
        "  pairs: a list of tuples of boards\n",
        "  labels: a list of labels matching each pair\n",
        "  \"\"\"\n",
        "\n",
        "  # stopping rule\n",
        "  if rotated_views == []:\n",
        "    return [], []\n",
        "\n",
        "  anchor = rotated_views[0]  # select board to compare with \n",
        "  pairs, labels = [], []\n",
        "  label = 1 # init label\n",
        "\n",
        "  # iterate over all remaining examples\n",
        "  for view in rotated_views:\n",
        "    pairs.append((anchor, view))\n",
        "    labels.append(label)\n",
        "    label += 1 # update label, views rotate a.c.w\n",
        "  \n",
        "  next_pairs, next_labels = get_rotated_data_pairs(rotated_views[1:]) # recursive call\n",
        "\n",
        "  return pairs + next_pairs, labels + next_labels\n",
        "\n",
        "def get_binary_board(board):\n",
        "  \"\"\"\n",
        "  Returns a binary board. Every non 0 value becomes 1. 0 stays 0.\n",
        "  \"\"\"\n",
        "\n",
        "  return (board != 0).astype('int32')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnKVXvD8TmZu",
        "colab_type": "text"
      },
      "source": [
        "## Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibYTel0ORlJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_shape_board(board):\n",
        "  \"\"\"\n",
        "  Reshapes board for moedl digestion. shape [*BOARD_SIZE, 1]\n",
        "  \"\"\"\n",
        "  return board.reshape([*BOARD_SIZE, 1])\n",
        "\n",
        "def plotting_shape_board(board):\n",
        "  \"\"\"\n",
        "  Reshapes the board for plotting. shape [*BOARD_SIZE]\n",
        "  \"\"\"\n",
        "  return board.reshape([*BOARD_SIZE])\n",
        "\n",
        "def get_all_pairs_reshaped(board, all_boards):\n",
        "  \"\"\"\n",
        "  Creates a list of pairs of data for the board and reshapes the boards to shape [*BOARD_SHAPE, 1] for nn digestion. For all the possible rotates and a false match\n",
        "\n",
        "  Args:\n",
        "  board - np.array\n",
        "  all_boards - all the other boards on the dataset not including this one\n",
        "\n",
        "  Returns:\n",
        "  a list of 5 tuples, 4 for the rotation part and 1 for a false match\n",
        "  \"\"\"\n",
        "\n",
        "  rotated_views = get_rotated_views(board)\n",
        "  rotated_pairs, rotated_labels = get_rotated_data_pairs(rotated_views)\n",
        "  rotated_pairs = [(model_shape_board(t[0]), model_shape_board(t[1])) for t in rotated_pairs] # reshape boards\n",
        "\n",
        "  # create false match pair\n",
        "  different_board = all_boards[np.random.randint(len(all_boards))] # generate random example\n",
        "  false_label = 0\n",
        "\n",
        "  return rotated_pairs + [[board.reshape([*BOARD_SIZE, 1]), different_board.reshape([*BOARD_SIZE, 1])]], rotated_labels + [false_label]\n",
        "\n",
        "def get_dataset_from_lists(pair_list, label_list, for_encoder=False):\n",
        "  \"\"\"\n",
        "  Creates an x,y dataset from a list of pairs and a list of labels. One hot encode labels\n",
        "\n",
        "  Args:\n",
        "  pair list: a list of pairs of images\n",
        "  label list: a list of ints - labels\n",
        "  for_encoder: bool - determains if the dataset if for the encoder\n",
        "\n",
        "  Retrurns:\n",
        "  x - [x1, x2] a list of two numpy array, each of shape [n_samples, *BOARD_SHAPE, 1] containing stacked inputs of one of the siamese twins OR only x1 if for encoder\n",
        "  y - one hot labels, only for siamese networks\n",
        "  \"\"\"\n",
        "\n",
        "  # prepare x\n",
        "  pairs_stacked = np.stack(pair_list)\n",
        "  x1 = pairs_stacked[:,0,:,:] # get one column of inputs\n",
        "  x2 = pairs_stacked[:,1,:,:] # get the second one\n",
        "  x = [x1, x2]\n",
        "\n",
        "  # prepare y\n",
        "  ohe = OneHotEncoder(sparse=False) # sparse = false is super importent!!\n",
        "  stacked_labels = np.stack(label_list).reshape([-1,1]) # stack and reshape label\n",
        "  y = ohe.fit_transform(stacked_labels)\n",
        "\n",
        "  # if the dataset is for the decoder return only one column of images\n",
        "  if not for_encoder:\n",
        "    return x, y\n",
        "  \n",
        "  else:\n",
        "    return x1\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZA3unNtUSBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_siamese_dataset(training_tasks, eval_tasks, test_tasks):\n",
        "  \"\"\"\n",
        "  Creates a dataset for the siamese networks\n",
        "  \"\"\"\n",
        "\n",
        "  # extract all_boards\n",
        "  all_boards = get_all_boards(training_tasks, eval_tasks, test_tasks)\n",
        "\n",
        "  # binirize all boards\n",
        "  all_boards_binary = map(get_binary_board, all_boards) ### use all!!!\n",
        "\n",
        "  # create a list of all boards augmentation data\n",
        "  pair_list = []\n",
        "  label_list = []\n",
        "\n",
        "  # iterate over all boards\n",
        "  for i, board in enumerate(all_boards_binary):\n",
        "    board_pairs, board_labels = get_all_pairs_reshaped(board, all_boards[i:]) # augment example. use only boards from here onward\n",
        "    pair_list += board_pairs\n",
        "    label_list += board_labels\n",
        "\n",
        "  ## create dataset\n",
        "  x, y = get_dataset_from_lists(pair_list, label_list)\n",
        "\n",
        "  return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV-DsYyV-W23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### just for this notebook\n",
        "x, y = get_siamese_dataset(training_tasks, eval_tasks, test_tasks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gHJfVQ69rDn",
        "colab_type": "text"
      },
      "source": [
        "# Siamese networks architecture\n",
        "- getting the right architecture\n",
        "- went for residual networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-kGomzhzlo7",
        "colab_type": "text"
      },
      "source": [
        "## Defining basic blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsL9Hpfazgxr",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title basic blocks\n",
        "def residual_encoder_block(filter_num, kernel_size, bn_moment):\n",
        "  \"\"\"\n",
        "  A functional style residual connection convolutional block.\n",
        "  \"\"\"\n",
        "\n",
        "  def block(x, filter_num, kernel_size=(3,3), bn_moment=0.9):\n",
        "    # first layer\n",
        "    c1 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(x)\n",
        "    c1 = BatchNormalization(momentum=bn_moment)(c1)\n",
        "\n",
        "    # second layer\n",
        "    c2 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(c1)\n",
        "    c2 = BatchNormalization(momentum=bn_moment)(c2)\n",
        "\n",
        "    # third layer\n",
        "    c3 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(c2)\n",
        "    c3 = BatchNormalization(momentum=bn_moment)(c3)\n",
        "\n",
        "    # residual connection\n",
        "    res = c1 + c3\n",
        "\n",
        "    return res\n",
        "  \n",
        "  return lambda x: block(x, filter_num, kernel_size, bn_moment)\n",
        "\n",
        "def residual_decoder_block(filter_num, kernel_size, bn_moment, first=False):\n",
        "  \"\"\"\n",
        "  A functional style residual connection deconvolutional block.\n",
        "\n",
        "  Args:\n",
        "  first - bool, determains if it is the first block of a network\n",
        "  \"\"\"\n",
        "  \n",
        "  def block(x, filter_num, kernel_size=(3,3), bn_moment=0.9):\n",
        "    \n",
        "    # first layer\n",
        "    if first:\n",
        "      dc1 = Dense(4*4*filter_num, activation='linear')(x) # num neurons is dependent upon the number of blocks\n",
        "      dc1 = BatchNormalization(momentum=bn_moment)(dc1)\n",
        "      dc1 = Reshape(target_shape=(4,4,filter_num))(dc1) # reshaping is dependent upon the number of blocks\n",
        "    \n",
        "    else:\n",
        "      dc1 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(x)\n",
        "      dc1 = BatchNormalization(momentum=bn_moment)(dc1)\n",
        "\n",
        "    # second layer\n",
        "    dc2 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(dc1)\n",
        "    dc2 = BatchNormalization(momentum=bn_moment)(dc2)\n",
        "\n",
        "    # third layer\n",
        "    dc3 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(dc2)\n",
        "    dc3 = BatchNormalization(momentum=bn_moment)(dc3)\n",
        "\n",
        "    # residual connection\n",
        "    res = dc1 + dc3\n",
        "\n",
        "    return res\n",
        "\n",
        "  return lambda x: block(x, filter_num, kernel_size, bn_moment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU9XOnUA99dD",
        "colab_type": "text"
      },
      "source": [
        "## Main architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISOAGouf9ym1",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title siamese\n",
        "def get_siamese_networks_model(input_shape):\n",
        "  \"\"\"\n",
        "  Creates siamese networks model. ref paper: https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf\n",
        "  \"\"\"\n",
        "\n",
        "  # define input vectors\n",
        "  input1 = Input(input_shape, name='input1')\n",
        "  input2 = Input(input_shape, name='input2')\n",
        "\n",
        "  def get_encoder():\n",
        "    \"\"\"\n",
        "    A helper function for sharing weights between inputs\n",
        "    \"\"\"\n",
        "\n",
        "    x = Input(input_shape, name='x')  \n",
        "\n",
        "    # first block\n",
        "    block1 = residual_encoder_block(16, kernel_size=(3,3), bn_moment=0.9)(x)\n",
        "    mp1 = MaxPool2D(pool_size=(2,2), name='max_pool1')(block1)\n",
        "\n",
        "    # second block\n",
        "    block2 = residual_encoder_block(32, kernel_size=(3,3), bn_moment=0.9)(mp1)\n",
        "    mp2 = MaxPool2D(pool_size=(2,2), name='max_pool2')(block2)\n",
        "\n",
        "    # third block\n",
        "    block3 = residual_encoder_block(64, kernel_size=(3,3), bn_moment=0.9)(mp2)\n",
        "    mp3 = MaxPool2D(pool_size=(2,2), name='max_pool3')(block3)\n",
        "\n",
        "    # flatten\n",
        "    flat = Flatten()(mp3)\n",
        "\n",
        "    # dense\n",
        "    dense = Dense(DENSE_REP_SIZE, activation='sigmoid', name='dense_rep')(flat)\n",
        "    bn_dense = BatchNormalization(momentum=0.9)(dense)\n",
        "\n",
        "    return tf.keras.Model(inputs=x, outputs=bn_dense)\n",
        "\n",
        "  # get shared encoder\n",
        "  encoder = get_encoder()\n",
        "  \n",
        "  # get feature vectors\n",
        "  v1 = encoder(input1)\n",
        "  v2 = encoder(input2)\n",
        "\n",
        "  # compute L1 loss\n",
        "  L1_Layer = Lambda(lambda tensors: tf.math.abs(tensors[0] - tensors[1]))\n",
        "  L1_diff = L1_Layer([v1, v2])\n",
        "\n",
        "  # compute probs\n",
        "  probs = Dense(5, activation='softmax')(L1_diff)\n",
        "\n",
        "  siamese_net = tf.keras.Model(inputs=[input1, input2], outputs=probs)\n",
        "\n",
        "  # compile\n",
        "  siamese_net.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
        "  return siamese_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEKcGPtKgbM6",
        "colab_type": "text"
      },
      "source": [
        "## Training siamese networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0kKJwuy2t2R",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "sn = get_siamese_networks_model([*BOARD_SIZE, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfvNty6FbxNA",
        "colab_type": "code",
        "outputId": "d301ed96-3c2b-4e51-f0fb-dd45e92b2330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        }
      },
      "source": [
        "hist = sn.fit(x=x, y=y, epochs=30, batch_size=8, shuffle=True, validation_split=0.2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9bd181cbd230>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDuSUF8lRP0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_training_curves(hist.history, metric='categorical_accuracy', with_val=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkiOZvd7gOkP",
        "colab_type": "text"
      },
      "source": [
        "# Board generator (decoder) architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ir0OqLyUPa_",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title decoder\n",
        "def get_encoder_from_siamese(siamese):\n",
        "  \"\"\"\n",
        "  Creates a board encoder from the siamese networks model. Maps board --> feature vector of dim 512\n",
        "  \"\"\"\n",
        "\n",
        "  # create encoder from all layers up to dense\n",
        "  model = tf.keras.models.Sequential([\n",
        "                                      tf.keras.layers.InputLayer(input_shape=[*BOARD_SIZE, 1]),\n",
        "                                      siamese.layers[2] # all sequential layers from siamese networks model\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "def get_decoder(siamese, copy_encoder_weights=True):\n",
        "  \"\"\"\n",
        "  Creates a decoder from siamese netwok model. Maps feature vector of dim 512 --> board. \n",
        "  Same architecture from encoder is preserved\n",
        "  \"\"\"\n",
        "\n",
        "  # build model\n",
        "  inp = Input([DENSE_REP_SIZE])\n",
        "\n",
        "  # first_block\n",
        "  block1 = residual_decoder_block(64, kernel_size=(3,3), bn_moment=0.9, first=True)(inp)\n",
        "  us1 = UpSampling2D(size=(2,2))(block1)\n",
        "\n",
        "  block2 = residual_decoder_block(32, kernel_size=(3,3), bn_moment=0.9)(us1)\n",
        "  us2 = UpSampling2D(size=(2,2))(block2)\n",
        "\n",
        "  block3 = residual_decoder_block(16, kernel_size=(3,3), bn_moment=0.9)(us2)\n",
        "\n",
        "  output = Conv2D(1, kernel_size=(1,1), activation='sigmoid')(block3)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inp, outputs=output)\n",
        "\n",
        "  if copy_encoder_weights:\n",
        "    # set weights from the encoder\n",
        "    enc_layers = siamese.layers[2].layers # get encoder layers\n",
        "    \n",
        "    for i, layer in enumerate(model.layers[::-1]):\n",
        "      if(2 >= len(layer.weights) > 0): # trainable layer and not BN layer\n",
        "        if layer.weights[0].shape[0] < 100: # deconv layers\n",
        "          enc_layer_weigths = enc_layers[i].get_weights() # get weights from encoder\n",
        "          bias = np.zeros(layer.get_weights()[1].shape) # init new bias\n",
        "          w = enc_layer_weigths[0].transpose([1,0,2,3]) # transpose weights\n",
        "          layer.set_weights([w, bias]) # set decoder weights\n",
        "\n",
        "        else:\n",
        "          # dense layer\n",
        "          enc_layer_weigths = enc_layers[i].get_weights() # get weights from encoder\n",
        "          bias = np.zeros(layer.get_weights()[1].shape) # init new bias\n",
        "          w = enc_layer_weigths[0].transpose() # transpose weights\n",
        "          layer.set_weights([w, bias]) # set decoder weights\n",
        "\n",
        "  # compile\n",
        "  model.compile(loss=pixelwise_error_loss, optimizer='adam', metrics=[pixelwise_auc])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def pixelwise_error_loss(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Custom loss function. Pixelwise error from the true image. ||y_true - y_pred||\n",
        "  \"\"\"\n",
        "\n",
        "  return tf.keras.backend.sum((y_true - y_pred) ** 2) / y_pred.shape[0]\n",
        "\n",
        "\n",
        "def pixelwise_accuracy(y_true, y_pred, threshold=0.8):\n",
        "  \"\"\"\n",
        "  Custom accuracy function. Pixelwise accuracy of predicting 0 / 1\n",
        "  \"\"\"\n",
        "  \n",
        "  tp = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred >= threshold), 'float32'), y_true)) # cast boolean to binary\n",
        "  tn = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred < threshold), 'float32'), tf.cast((y_true == 0), 'float32')))\n",
        "  n_predictions = BOARD_SIZE[0] * BOARD_SIZE[1]\n",
        "\n",
        "  return ((tp + tn) / n_predictions) / y_pred.shape[0]\n",
        "\n",
        "\n",
        "def pixelwise_sensitivity(y_true, y_pred, threshold=0.8):\n",
        "  \"\"\"\n",
        "  Pixelwise sensitivity of correctly predicting 1s. true positives / positives\n",
        "  \"\"\"\n",
        "\n",
        "  tp = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred >= threshold), 'float32'), y_true)) \n",
        "  p = tf.cast(tf.math.reduce_sum(y_true), 'float32')\n",
        "  \n",
        "  return tp / p\n",
        "\n",
        "\n",
        "def pixelwise_fpr(y_true, y_pred, threshold=0.8):\n",
        "  \"\"\"\n",
        "  Pixelwise false positive rate of wrongly predicting 1s. false positives / negatives\n",
        "  \"\"\"\n",
        "\n",
        "  fp = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred >= threshold), 'float32'), tf.cast((y_true==0), 'float32'))) # cast boolean to binary\n",
        "  n = tf.cast(tf.math.reduce_sum(tf.cast((y_true == 0), 'float32')), 'float32')\n",
        "  \n",
        "  return fp / n\n",
        "\n",
        "\n",
        "def pixelwise_auc(y_true, y_pred, num_thresholds=10):\n",
        "  \"\"\"\n",
        "  Pixelwise auc score. \n",
        "  \"\"\"\n",
        "\n",
        "  sensitivity = []\n",
        "  fpr = []\n",
        "\n",
        "  # compute graph points\n",
        "  for threshold in np.linspace(1, 0, num=num_thresholds):\n",
        "    sensitivity.append(pixelwise_sensitivity(y_true, y_pred, threshold=threshold))\n",
        "    fpr.append(pixelwise_fpr(y_true, y_pred, threshold=threshold))\n",
        "  \n",
        "  # compute trapeze area\n",
        "  trapeze = []\n",
        "  for i in range(len(sensitivity)- 1):\n",
        "    area = tf.multiply(sensitivity[i], fpr[i+1] - fpr[i])\n",
        "    trapeze.append(area)\n",
        "  \n",
        "  auc_score = tf.add_n(trapeze) \n",
        "\n",
        "  return auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNIBD7hmoL-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dec = get_decoder(sn, copy_encoder_weights=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uBo_DSMrXDZ",
        "colab_type": "text"
      },
      "source": [
        "## Decoder dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmNwACcSraxu",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title decoder dataset\n",
        "def get_decoder_dataset(siamese, encoder_dataset):\n",
        "  \"\"\"\n",
        "  Creates the decoder dataset from the encoder.\n",
        "  \n",
        "  Args:\n",
        "  siamese - siamese model to extract encoder from\n",
        "  encoder dataset - boards to encoder to feature vectors\n",
        "\n",
        "  Returns:\n",
        "  x - boards feature vectors, predicted by encoder\n",
        "  y - boards \n",
        "  \"\"\"\n",
        "  \n",
        "  enc = get_encoder_from_siamese(siamese) # fetch encoder\n",
        "\n",
        "  # build dataset\n",
        "  x = enc.predict(encoder_dataset) # get feature vectors\n",
        "  y = encoder_dataset # boards to recreate\n",
        "\n",
        "  return x,y\n",
        "\n",
        "# create encoder dataset from the old pair and label lists\n",
        "encoder_dataset = get_dataset_from_lists(pair_list, label_list, for_encoder=True) \n",
        "\n",
        "# create decoder dataset from encoder\n",
        "decoder_x, decoder_y = get_decoder_dataset(sn, encoder_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg9iv1vPtEEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist_decoder = dec.fit(x=decoder_x, y=decoder_y, batch_size=DECODER_BATCH_SIZE, shuffle=True, epochs=10, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pHSQ3pYcTII",
        "colab_type": "text"
      },
      "source": [
        "# Check regular encoder-decoder architecture\n",
        "## For checking some hyperparameters and deciding which adecoder architecture to use\n",
        "\n",
        "## Some takeaways:\n",
        "- adding two sets of randomly padded data helps alot. more doesnt help\n",
        "- small decoder batch size (4-16) really helps\n",
        "- adding more neurons to the compressed represantation (512 --> 1024) really helps\n",
        "- removing the last layers batch normalization really helps\n",
        " - removing bn before compressed representation isnt good\n",
        "- penelizing mistaking 1's for 0's more doesnt help at all\n",
        "- changing last layer activation to linear and clipping to [0,1] helps to some extent\n",
        "- chagning middle to relu - really bad!!!\n",
        "- addind another conv-deconv layer at the beggining helps a little\n",
        "- adding another conv-deconv layer at the end doesnt help at all\n",
        "- residual layers - really cool! used the architecture from this papare -- >https://github.com/omerhac/arc_challenge/blob/master/deep%20residual%20conv-deconv%20network.pdf\n",
        "- more types of data augmentation....\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzGZn1y0vt54",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRvqZ5OczS9J",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title encoder-decoder baseline\n",
        "def get_encoder_decoder():\n",
        "  \"\"\"\n",
        "  Create an encoder decoer \"normal\" architecture, with residual connections\n",
        "  \"\"\"\n",
        "  inp = tf.keras.layers.Input([*BOARD_SIZE, 1])\n",
        "  \n",
        "  # encoder\n",
        "  block1 = residual_encoder_block(16, kernel_size=(3,3), bn_moment=0.9)(inp)\n",
        "  mp1 = MaxPool2D(pool_size=(2,2))(block1)\n",
        "\n",
        "  block2 = residual_encoder_block(32, kernel_size=(3,3), bn_moment=0.9)(mp1)\n",
        "  mp2 = MaxPool2D(pool_size=(2,2))(block2)\n",
        "\n",
        "  block3 = residual_encoder_block(64, kernel_size=(3,3), bn_moment=(0.9))(mp2)\n",
        "\n",
        "  # flatten\n",
        "  flat = Flatten()(block3)\n",
        "\n",
        "  # dense representation\n",
        "  dense = Dense(1024, activation='sigmoid')(flat)\n",
        "  dense = BatchNormalization(momentum=0.9)(dense)\n",
        "\n",
        "\n",
        "  # decoder \n",
        "\n",
        "  # first_block\n",
        "  block1 = residual_decoder_block(64, kernel_size=(3,3), bn_moment=0.9, first=True)(dense)\n",
        "  us1 = UpSampling2D(size=(2,2))(block1)\n",
        "\n",
        "  block2 = residual_decoder_block(32, kernel_size=(3,3), bn_moment=0.9)(us1)\n",
        "  us2 = UpSampling2D(size=(2,2))(block2)\n",
        "\n",
        "  block3 = residual_decoder_block(16, kernel_size=(3,3), bn_moment=0.9)(us2)\n",
        "\n",
        "  output = Conv2D(1, kernel_size=(1,1), activation='sigmoid')(block3)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inp, outputs=output)\n",
        "  \n",
        "\n",
        "  # compile\n",
        "  model.compile(loss=pixelwise_error_loss, optimizer='adam', metrics=[pixelwise_auc])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mharVxoMMqJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ed = get_encoder_decoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb0_gTC8nIdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ed_hist = ed.fit(x=decoder_y, y=decoder_y, batch_size=8, shuffle=True, epochs=20, validation_split=0.2) # get image --> predict image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9ii2z4hh9de",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## check predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eytnCHw0iUIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boards = decoder_y[-10:]\n",
        "\n",
        "# predict\n",
        "predictions = ed.predict(boards)\n",
        "\n",
        "# reshape\n",
        "boards = [plotting_shape_board(board) for board in boards]\n",
        "predictions = [plotting_shape_board(board) for board in predictions]\n",
        "pairs = zip(boards, predictions)\n",
        "\n",
        "plot_decoder_boards(list(pairs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kujm22z3y0qa",
        "colab_type": "text"
      },
      "source": [
        "# Interleaved training (decoder/encoder/deocoder..)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5XjrxlkyZOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def copy_decoder_to_siamese(siamese, decoder):\n",
        "  \"\"\"\n",
        "  Copys the weights from the decoder to the siamese networks model\n",
        "  \"\"\"\n",
        "\n",
        "  decoder_layers = decoder.layers # get decoder layers\n",
        "  siamese_layers = siamese.layers[2].layers # siamese encoder layers\n",
        "\n",
        "  for i, layer in enumerate(decoder_layers[::-1]):\n",
        "      if(2 >= len(layer.weights) > 0): # trainable layer and not BN layer\n",
        "        if layer.weights[0].shape[0] < 100: # deconv layers\n",
        "          decoder_layer_weigths = layer.get_weights() # get weights from decoder\n",
        "          bias = np.zeros(siamese_layers[i].get_weights()[1].shape) # init new bias\n",
        "          w = decoder_layer_weigths[0].transpose([1,0,2,3]) # transpose weights\n",
        "          siamese_layers[i].set_weights([w, bias]) # set siamese weights\n",
        "\n",
        "        else:\n",
        "          # dense layer\n",
        "          decoder_layer_weigths = layer.get_weights() # get weights from decoder\n",
        "          bias = np.zeros(siamese_layers[i].get_weights()[1].shape) # init new bias\n",
        "          w = decoder_layer_weigths[0].transpose() # transpose weights\n",
        "          siamese_layers[i].set_weights([w, bias]) # set siamese weights\n",
        "\n",
        "\n",
        "def copy_siamese_to_decoder(siamese, decoder):\n",
        "  \"\"\"\n",
        "  Copys the weights from the siamese networks model to the decoder\n",
        "  \"\"\"\n",
        "\n",
        "  decoder_layers = decoder.layers # get decoder layers\n",
        "  siamese_layers = siamese.layers[2].layers # siamese encoder layers\n",
        "\n",
        "  for i, layer in enumerate(decoder_layers[::-1]):\n",
        "      if(2 >= len(layer.weights) > 0): # trainable layer and not BN layer\n",
        "        if layer.weights[0].shape[0] < 100: # deconv layers\n",
        "          siamese_layer_weigths = siamese_layers[i].get_weights() # get weights from siamese encoder\n",
        "          bias = np.zeros(layer.get_weights()[1].shape) # init new bias\n",
        "          w = siamese_layer_weigths[0].transpose([1,0,2,3]) # transpose weights\n",
        "          layer.set_weights([w, bias]) # set decoder weights\n",
        "\n",
        "        else:\n",
        "          # dense layer\n",
        "          siamese_layer_weigths = siamese_layers[i].get_weights() # get weights from siamese encoder\n",
        "          bias = np.zeros(layer.get_weights()[1].shape) # init new bias\n",
        "          w = siamese_layer_weigths[0].transpose() # transpose weights\n",
        "          layer.set_weights([w, bias]) # set decoder weights\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2ucFsFkzIkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create new models with same weights\n",
        "EPOCHS = 30\n",
        "sn = get_siamese_networks_model([*BOARD_SIZE, 1])\n",
        "decoder = get_decoder(sn, copy_encoder_weights=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pfDn78N4ecn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "SN_BATCH_SIZE = 64\n",
        "DEC_BATCH_SIZE = 4\n",
        "DATA_SPLIT = 5\n",
        "SN_STEPS_PER_EPOCH = len(x) // SN_BATCH_SIZE\n",
        "DEC_STEPS_PER_EPOCH = len(decoder_x) // DEC_BATCH_SIZE\n",
        "\n",
        "# even fancier training loop\n",
        "for i in range(EPOCHS):\n",
        "  print(\"training... epoch num: {}\".format(i))\n",
        "\n",
        "  for i in range(40):\n",
        "    # train siamese\n",
        "    _ = sn.fit(x=x, y=y, epochs=1, batch_size=SN_BATCH_SIZE, shuffle=True, steps_per_epoch=(780//DATA_SPLIT))\n",
        "    \n",
        "    # copy weights\n",
        "    copy_siamese_to_decoder(sn, decoder)\n",
        "\n",
        "    # get new decoder dataset\n",
        "    decoder_x, decoder_y = get_decoder_dataset(sn, encoder_dataset) # encoder dataset is the same as before\n",
        "\n",
        "    # train decoder\n",
        "    _ = decoder.fit(x=decoder_x, y=decoder_y, epochs=1, batch_size=DEC_BATCH_SIZE, shuffle=True, steps_per_epoch=(24000//DATA_SPLIT)) #### WOW!!! use smaller batch size, WOHOOO!!\n",
        "\n",
        "    # copy weights\n",
        "    copy_decoder_to_siamese(sn, decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BefQARZeJ6OK",
        "colab_type": "text"
      },
      "source": [
        "# Predicting from diffrance in boards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Api-_erL_rs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task_training_input, task_training_output, task_test_input, task_test_output = get_task_boards(training_tasks[0], pad=pad, divide_sets=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmT5I2c2KNS0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "39ffa1c3-27fe-4373-f344-ea18380d2159"
      },
      "source": [
        "# toy dataset (x is the dataset for the siamese networks)\n",
        "anchor = x[0][0]\n",
        "rotate_once_anchor = x[1][1]\n",
        "rotate_twice_anchor = x[1][2]\n",
        "\n",
        "toy = np.stack([anchor, rotate_once_anchor, rotate_twice_anchor])\n",
        "\n",
        "# plot\n",
        "fig, axs = plt.subplots(1,3)\n",
        "plot_board(plotting_shape_board(anchor), axs[0])\n",
        "plot_board(plotting_shape_board(rotate_once_anchor), axs[1])\n",
        "plot_board(plotting_shape_board(rotate_twice_anchor), axs[2])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVmhKDCOOEYJ",
        "colab_type": "text"
      },
      "source": [
        "## define predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uVadzm3Lw3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictor(decoder):\n",
        "  \"\"\"\n",
        "  Builds a detector which takes a board and a rules vector and predicts output board.\n",
        "\n",
        "  Args:\n",
        "  decoder --> trained decoder \n",
        "  \"\"\"\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}