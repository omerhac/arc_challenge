{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arc.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omerhac/arc_challenge/blob/master/arc_advanced_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YShrwCd2GUWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import json\n",
        "from google.cloud import storage\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import colors\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Conv2D, Lambda, Dense, Flatten, MaxPool2D, Input, BatchNormalization, Conv2DTranspose, UpSampling2D, Reshape\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uko3X-E_CrZr",
        "colab_type": "code",
        "outputId": "a1d0dbe9-8c76-4b1b-e199-96e3a751dbde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# get repository from github\n",
        "!git clone https://github.com/omerhac/arc_challenge.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'arc_challenge'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (134/134), done.\u001b[K\n",
            "remote: Compressing objects: 100% (132/132), done.\u001b[K\n",
            "Receiving objects: 100% (134/134), 10.49 MiB | 12.97 MiB/s, done.\n",
            "remote: Total 134 (delta 55), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcDHkw11CtSu",
        "colab_type": "code",
        "outputId": "1b6da9c1-2244-4f21-80b4-b73f30d4b901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# navigate to directory\n",
        "cd arc_challenge"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/arc_challenge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX5E2ZNODM5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load repository dependencies\n",
        "import preprocess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF4Ovm97vXEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## constants ##\n",
        "BOARD_SIZE = (16,16) # board upperbound size\n",
        "SN_BATCH_SIZE = 64\n",
        "DECODER_BATCH_SIZE = 8\n",
        "DENSE_REP_SIZE = 1024 # dense vector represantation size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqOKR-POOXki",
        "colab_type": "text"
      },
      "source": [
        "# Load data / util funcitons\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-96BlT_hHVkF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title load data\n",
        "def load_data():\n",
        "  \"\"\"\n",
        "  Loads all the data into training_tasks, eval_tasks and test_tasks\n",
        "  \"\"\"\n",
        "\n",
        "  ## get paths\n",
        "  GCS_PATH = \"gs://kds-d3cfb3d523ca35d2517017a78110126404d01fdea69417ce49950459\"\n",
        "  training_filenames = tf.io.gfile.glob(GCS_PATH + \"/training/*\")\n",
        "  test_filenames = tf.io.gfile.glob(GCS_PATH + \"/test/*\")\n",
        "  eval_filenames = tf.io.gfile.glob(GCS_PATH + \"/evaluation/*\")\n",
        "\n",
        "  # create datasets with filenames\n",
        "  training_dataset = tf.data.Dataset.list_files(training_filenames)\n",
        "  eval_dataset = tf.data.Dataset.list_files(eval_filenames)\n",
        "  test_dataset = tf.data.Dataset.list_files(test_filenames)\n",
        "\n",
        "  # load the jsons\n",
        "  def load_task(filename):\n",
        "    task_json = tf.io.read_file(filename)\n",
        "    return task_json\n",
        "\n",
        "  training_dataset = training_dataset.map(load_task)\n",
        "  eval_dataset = eval_dataset.map(load_task)\n",
        "  test_dataset = test_dataset.map(load_task)\n",
        "\n",
        "  training_dataset_numpy = tf.data.Dataset.as_numpy_iterator(training_dataset) # convert to numpy iterator\n",
        "  eval_dataset_numpy = tf.data.Dataset.as_numpy_iterator(eval_dataset)\n",
        "  test_dataset_numpy = tf.data.Dataset.as_numpy_iterator(test_dataset)\n",
        "\n",
        "  ## create a numpy array of tasks (n_tasks, )\n",
        "  def list_from_jsons(jsons_numpy_iterator):\n",
        "    \"\"\"\n",
        "      Create a list of task dictionaries from jsons numpy interator\n",
        "    \"\"\"\n",
        "\n",
        "    tasks = []\n",
        "    for task in jsons_numpy_iterator:\n",
        "      tasks.append(json.loads(task))\n",
        "\n",
        "    return tasks\n",
        "\n",
        "  ## get numpy arrays of datasets\n",
        "  training_tasks = list_from_jsons(training_dataset_numpy)\n",
        "  eval_tasks = list_from_jsons(eval_dataset_numpy)\n",
        "  test_tasks = list_from_jsons(test_dataset_numpy)\n",
        "\n",
        "  return training_tasks, eval_tasks, test_tasks\n",
        "\n",
        "\n",
        "def load_data_from_jsons():\n",
        "  \"\"\"\n",
        "  Load tasks from jsons to lists of tasks.\n",
        "\n",
        "  Returns:\n",
        "  training_tasks, eval_tasks, test_tasks --> lists of tasks\n",
        "  \"\"\"\n",
        "\n",
        "  with open(\"training_tasks.json\", 'r') as f:\n",
        "    training_tasks = json.load(f)\n",
        "\n",
        "  with open(\"eval_tasks.json\", 'r') as f:\n",
        "    eval_tasks = json.load(f)\n",
        "  \n",
        "  with open(\"test_tasks.json\", 'r') as f:\n",
        "    test_tasks = json.load(f)\n",
        "\n",
        "  return training_tasks, eval_tasks, test_tasks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oOyaYHAJr1A",
        "colab_type": "code",
        "outputId": "192390bb-685b-4380-86f4-d44f69be28f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%%time\n",
        "training_tasks, eval_tasks, test_tasks = load_data_from_jsons()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 232 ms, sys: 16.2 ms, total: 248 ms\n",
            "Wall time: 249 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYLEt0yrPXf9",
        "colab_type": "text"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO9FwJjFNAnO",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Utilities\n",
        "def plot_board(board, ax, title=\"\"):\n",
        "  \"\"\"\n",
        "  Plot a board on a given axis\n",
        "  \"\"\"\n",
        "  cmap = colors.ListedColormap(\n",
        "      ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
        "        '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
        "  norm = colors.Normalize(vmin=0, vmax=9)\n",
        "  \n",
        "  ax.imshow(board, cmap=cmap, norm=norm)\n",
        "  ax.grid(True,which='both',color='lightgrey', linewidth=0.5)    \n",
        "  ax.set_yticks([x-0.5 for x in range(1+board.shape[0])])\n",
        "  ax.set_xticks([x-0.5 for x in range(1+board.shape[1])])     \n",
        "  ax.set_xticklabels([])\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_title(title)\n",
        "\n",
        "def plot_one(task, ax, i,train_or_test,input_or_output):\n",
        "  \"\"\"\n",
        "  Plot one task on a given axis\n",
        "  \"\"\"\n",
        "  cmap = colors.ListedColormap(\n",
        "      ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
        "        '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
        "  norm = colors.Normalize(vmin=0, vmax=9)\n",
        "  \n",
        "  input_matrix = task[train_or_test][i][input_or_output]\n",
        "  ax.imshow(input_matrix, cmap=cmap, norm=norm)\n",
        "  ax.grid(True,which='both',color='lightgrey', linewidth=0.5)    \n",
        "  ax.set_yticks([x-0.5 for x in range(1+len(input_matrix))])\n",
        "  ax.set_xticks([x-0.5 for x in range(1+len(input_matrix[0]))])     \n",
        "  ax.set_xticklabels([])\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_title(train_or_test + ' '+input_or_output)\n",
        "    \n",
        "\n",
        "def plot_task(task):\n",
        "    \"\"\"\n",
        "    Plots the first train and test pairs of a specified task,\n",
        "    using same color scheme as the ARC app\n",
        "    \"\"\"    \n",
        "    num_train = len(task['train'])\n",
        "    fig, axs = plt.subplots(2, num_train, figsize=(3*num_train,3*2))\n",
        "    for i in range(num_train):     \n",
        "        plot_one(task, axs[0,i],i,'train','input')\n",
        "        plot_one(task, axs[1,i],i,'train','output')        \n",
        "    plt.tight_layout()\n",
        "    plt.show()        \n",
        "        \n",
        "    num_test = len(task['test'])\n",
        "    fig, axs = plt.subplots(2, num_test, figsize=(3*num_test,3*2))\n",
        "    if num_test==1: \n",
        "        plot_one(task, axs[0],0,'test','input')\n",
        "        plot_one(task, axs[1],0,'test','output')     \n",
        "    else:\n",
        "        for i in range(num_test):      \n",
        "            plot_one(task, axs[0,i],i,'test','input')\n",
        "            plot_one(task, axs[1,i],i,'test','output')  \n",
        "    plt.tight_layout()\n",
        "    plt.show() \n",
        "  \n",
        "\n",
        "def plot_board_pairs(board_pairs, labels):\n",
        "  \"\"\"\n",
        "  Plots the board pairs (for siamese networks) with their label as a title\n",
        "  \"\"\"\n",
        "\n",
        "  fig, axs = plt.subplots(len(board_pairs), 2, figsize=(8, 3 * len(board_pairs)))\n",
        "  \n",
        "  for i, pair in enumerate(board_pairs):\n",
        "    # plot a pair on a given axis\n",
        "    plot_board(pair[0], axs[i, 0], title=\"anchor\") \n",
        "    plot_board(pair[1], axs[i, 1], title=labels[i])\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_decoder_boards(board_pairs):\n",
        "  \"\"\"\n",
        "  Plots the board pairs (for siamese networks) with their label as a title\n",
        "  \"\"\"\n",
        "\n",
        "  fig, axs = plt.subplots(len(board_pairs), 2, figsize=(8, 3 * len(board_pairs)))\n",
        "\n",
        "  for i, pair in enumerate(board_pairs):\n",
        "    # plot a pair on a given axis\n",
        "    plot_board(pair[0], axs[i, 0]) \n",
        "    plot_board(pair[1], axs[i, 1])\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def display_training_curves(hist, metric='accuracy', with_val=False):\n",
        "  \"\"\"display learning curves for keras history dict, args: history dict, with val --> boolean with/without val\"\"\"\n",
        "  plt.figure(figsize=(18,6))\n",
        "\n",
        "  # accuracy plots\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(hist[metric])\n",
        "  \n",
        "  if with_val:\n",
        "    plt.plot(hist['val_' + metric])\n",
        "    plt.legend(['Train', 'Validation'])\n",
        "  \n",
        "  else:\n",
        "    plt.legend(['Train'])\n",
        "  \n",
        "  plt.title('Model accuracy')\n",
        "  plt.xlabel('EPOCH')\n",
        "  plt.ylabel('Accuracy')\n",
        "\n",
        "  # loss plots\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(hist['loss'])\n",
        "\n",
        "  if with_val:\n",
        "    plt.plot(hist['val_loss'])\n",
        "    plt.legend(['Train loss', 'Val loss'])\n",
        "  \n",
        "  else:\n",
        "    plt.legend(['Train loss'])\n",
        "  \n",
        "  plt.title('Model loss')\n",
        "  plt.xlabel('EPOCH')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9AnAL0VU2Tu",
        "colab_type": "text"
      },
      "source": [
        "# Data generating functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuTeaiVASRhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset_shapes(dataset):\n",
        "  \"\"\"\n",
        "  Returns dataset board shapes. \n",
        "  \"\"\"\n",
        "\n",
        "  shape_0 = []\n",
        "  shape_1 = []\n",
        "  \n",
        "  # check every task\n",
        "  for task in dataset:\n",
        "    boards = get_task_boards(task) # get all boards\n",
        "    shape_0 += [board.shape[0] for board in boards]\n",
        "    shape_1 += [board.shape[1] for board in boards]\n",
        "  \n",
        "  return shape_0, shape_1\n",
        "\n",
        "def get_task_boards(task, threshold_shape=(40, 40), pad=None, test=False, divide_sets=False):\n",
        "  \"\"\"\n",
        "  Get the training / testing boards of every example in a specific task. \n",
        "\n",
        "  Args: threshold_shape --> threshold shape for which \n",
        "  biggger samples won't be returned. \n",
        "  pad --> functional padding function to pad boards with (up tp threshold_size).\n",
        "  test --> bool, whether a test task or not. for output of task example.\n",
        "  divide_sets --> bool, whether to divide boards sets to training - input / output, test - input / output.\n",
        "  \"\"\"\n",
        "  \n",
        "  training_input_boards, training_output_boards, test_input_boards, test_output_boards = [], [], [], []\n",
        "\n",
        "  # train boards\n",
        "  for example in task['train']:\n",
        "    input_board = np.array(example['input'])\n",
        "    output_board = np.array(example['output'])\n",
        "\n",
        "    if((input_board.shape[0] < threshold_shape[0]) and (input_board.shape[1] < threshold_shape[1])):\n",
        "      training_input_boards.append(pad(input_board, output_shape=threshold_shape) if pad else input_board) # check for padding func\n",
        "\n",
        "    if((output_board.shape[0] < threshold_shape[0]) and (output_board.shape[1] < threshold_shape[1])):\n",
        "      training_output_boards.append(pad(output_board, output_shape=threshold_shape) if pad else output_board) # check for padding func\n",
        "\n",
        "  # test boards\n",
        "  for example in task['test']:\n",
        "    input_board = np.array(example['input'])\n",
        "\n",
        "    if not test: # check if test example\n",
        "      output_board = np.array(example['output'])\n",
        "    \n",
        "    # check whether the board is smaller then threshold shape\n",
        "    if((input_board.shape[0] < threshold_shape[0]) and (input_board.shape[1] < threshold_shape[1])):\n",
        "      test_input_boards.append(pad(input_board, output_shape=threshold_shape) if pad else input_board) # check for padding func\n",
        "\n",
        "    if((output_board.shape[0] < threshold_shape[0]) and (output_board.shape[1] < threshold_shape[1]) and (not test)):\n",
        "      test_output_boards.append(pad(output_board, output_shape=threshold_shape) if pad else output_board) # check for padding func\n",
        "  \n",
        "  # whether to divide boards:\n",
        "  if divide_sets:\n",
        "    return training_input_boards, training_output_boards, test_input_boards, test_output_boards\n",
        "  else:\n",
        "    return training_input_boards + training_output_boards + test_input_boards + test_output_boards\n",
        "\n",
        "def pad(mat, output_shape, padder=0):\n",
        "  \"\"\"\n",
        "  Pad a matrix with padder up to output_shape. Insert matrix at upper left corner.\n",
        "  \n",
        "  Args:\n",
        "  mat - np.array matrix of rank 2\n",
        "  output_shape - tuple \n",
        "  padder - int\n",
        "  \"\"\"\n",
        "\n",
        "  output_board = np.zeros(shape=output_shape) + padder # create output board and pad it\n",
        "\n",
        "  # get input shape\n",
        "  input_rows = mat.shape[0]\n",
        "  input_cols = mat.shape[1]\n",
        "\n",
        "  # if random=False, insert input matrix in upper left corner\n",
        "  output_board[:input_rows, :input_cols] = mat\n",
        "  return output_board\n",
        "\n",
        "def random_pad(mat, output_shape, padder=0):\n",
        "  \"\"\"\n",
        "  Pad a matrix with padder up to output_shape. Insert the matrix at a random location\n",
        "  \n",
        "  Args:\n",
        "  mat - np.array matrix of rank 2\n",
        "  output_shape - tuple \n",
        "  padder - int\n",
        "  seed - int\n",
        "  \"\"\"\n",
        "\n",
        "  output_board = np.zeros(shape=output_shape) + padder # create output board and pad it\n",
        "\n",
        "  # get input shape\n",
        "  input_rows = mat.shape[0]\n",
        "  input_cols = mat.shape[1]\n",
        "\n",
        "  # insert mat at a random loacation\n",
        "  # get random location\n",
        "  start_row = np.random.randint(output_shape[0] - input_rows)\n",
        "  start_col = np.random.randint(output_shape[1] - input_cols)\n",
        "  # insert\n",
        "  output_board[start_row:start_row+input_rows, start_col:start_col+input_cols] = mat\n",
        "\n",
        "  return output_board\n",
        "\n",
        "def get_all_boards(training_tasks, eval_tasks, test_tasks):\n",
        "  \"\"\"\n",
        "  Extracts all the boards from all training/eval/testing tasks. \n",
        "  \"\"\"\n",
        "\n",
        "  training_boards = []\n",
        "  for task in training_tasks:\n",
        "    training_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=pad)\n",
        "\n",
        "    # augment training set (random pad)\n",
        "    training_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "    training_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "\n",
        "  eval_boards = []\n",
        "  for task in eval_tasks:\n",
        "    eval_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=pad)\n",
        "\n",
        "    # augment eval set (random pad)\n",
        "    eval_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "    eval_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "    \n",
        "\n",
        "  test_boards = []\n",
        "  for task in test_tasks:\n",
        "    test_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=pad, test=True)\n",
        "\n",
        "    # augment test set (random pad)\n",
        "    test_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad, test=True)\n",
        "    test_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad, test=True)\n",
        "\n",
        "  all_boards = training_boards+eval_boards+test_boards\n",
        "  return all_boards\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLXZsdbYodKl",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oofkpiCmt8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_rotated_views(board):\n",
        "  \"\"\"\n",
        "  Turns a board 90 deg counter clockwise 3 times. Returns a list length 4 with all the rotated views including the original one.\n",
        "  \"\"\"\n",
        "\n",
        "  rotates = [board]\n",
        "  for i in range(3):\n",
        "    board = np.rot90(board)\n",
        "    rotates.append(board)\n",
        "  \n",
        "  return rotates\n",
        "\n",
        "def get_rotated_data_pairs(rotated_views):\n",
        "  \"\"\"\n",
        "  Creates all the possible pairs out of the rotated views tensor and labels them. If a1..a4 are rotated vies (A.C.W) of a1, returns labels according to:\n",
        "   - (a1, a1) = 1 --> same board\n",
        "   - (a1, a2) = 2 --> 90 a.c.w rotate\n",
        "   - (a1, a3) = 3 --> 180 rotate\n",
        "   - (a1, a4) = 4 --> 270 a.c.w rotate\n",
        "   - ...\n",
        "   - (a3, a4) = 2 --> 90 a.c.w rotate\n",
        "\n",
        "  Args:\n",
        "  rotated_views = a list of 4 rotated views of the board\n",
        "\n",
        "  Returns:\n",
        "  pairs: a list of tuples of boards\n",
        "  labels: a list of labels matching each pair\n",
        "  \"\"\"\n",
        "\n",
        "  # stopping rule\n",
        "  if rotated_views == []:\n",
        "    return [], []\n",
        "\n",
        "  anchor = rotated_views[0]  # select board to compare with \n",
        "  pairs, labels = [], []\n",
        "  label = 1 # init label\n",
        "\n",
        "  # iterate over all remaining examples\n",
        "  for view in rotated_views:\n",
        "    pairs.append((anchor, view))\n",
        "    labels.append(label)\n",
        "    label += 1 # update label, views rotate a.c.w\n",
        "  \n",
        "  next_pairs, next_labels = get_rotated_data_pairs(rotated_views[1:]) # recursive call\n",
        "\n",
        "  return pairs + next_pairs, labels + next_labels\n",
        "\n",
        "def get_binary_board(board):\n",
        "  \"\"\"\n",
        "  Returns a binary board. Every non 0 value becomes 1. 0 stays 0.\n",
        "  \"\"\"\n",
        "\n",
        "  return (board != 0).astype('int32')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnKVXvD8TmZu",
        "colab_type": "text"
      },
      "source": [
        "## Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibYTel0ORlJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_shape_board(board):\n",
        "  \"\"\"\n",
        "  Reshapes board for moedl digestion. shape [*BOARD_SIZE, 1]\n",
        "  \"\"\"\n",
        "  return board.reshape([*BOARD_SIZE, 1])\n",
        "\n",
        "def plotting_shape_board(board):\n",
        "  \"\"\"\n",
        "  Reshapes the board for plotting. shape [*BOARD_SIZE]\n",
        "  \"\"\"\n",
        "  return board.reshape([*BOARD_SIZE])\n",
        "\n",
        "def get_all_pairs_reshaped(board, all_boards):\n",
        "  \"\"\"\n",
        "  Creates a list of pairs of data for the board and reshapes the boards to shape [*BOARD_SHAPE, 1] for nn digestion. For all the possible rotates and a false match\n",
        "\n",
        "  Args:\n",
        "  board - np.array\n",
        "  all_boards - all the other boards on the dataset not including this one\n",
        "\n",
        "  Returns:\n",
        "  a list of 5 tuples, 4 for the rotation part and 1 for a false match\n",
        "  \"\"\"\n",
        "\n",
        "  rotated_views = get_rotated_views(board)\n",
        "  rotated_pairs, rotated_labels = get_rotated_data_pairs(rotated_views)\n",
        "  rotated_pairs = [(model_shape_board(t[0]), model_shape_board(t[1])) for t in rotated_pairs] # reshape boards\n",
        "\n",
        "  # create false match pair\n",
        "  different_board = all_boards[np.random.randint(len(all_boards))] # generate random example\n",
        "  false_label = 0\n",
        "\n",
        "  return rotated_pairs + [[board.reshape([*BOARD_SIZE, 1]), different_board.reshape([*BOARD_SIZE, 1])]], rotated_labels + [false_label]\n",
        "\n",
        "def get_dataset_from_lists(pair_list, label_list, for_encoder=False):\n",
        "  \"\"\"\n",
        "  Creates an x,y dataset from a list of pairs and a list of labels. One hot encode labels\n",
        "\n",
        "  Args:\n",
        "  pair list: a list of pairs of images\n",
        "  label list: a list of ints - labels\n",
        "  for_encoder: bool - determains if the dataset if for the encoder\n",
        "\n",
        "  Retrurns:\n",
        "  x - [x1, x2] a list of two numpy array, each of shape [n_samples, *BOARD_SHAPE, 1] containing stacked inputs of one of the siamese twins OR only x1 if for encoder\n",
        "  y - one hot labels, only for siamese networks\n",
        "  \"\"\"\n",
        "\n",
        "  # prepare x\n",
        "  pairs_stacked = np.stack(pair_list)\n",
        "  x1 = pairs_stacked[:,0,:,:] # get one column of inputs\n",
        "  x2 = pairs_stacked[:,1,:,:] # get the second one\n",
        "  x = [x1, x2]\n",
        "\n",
        "  # prepare y\n",
        "  ohe = OneHotEncoder(sparse=False) # sparse = false is super importent!!\n",
        "  stacked_labels = np.stack(label_list).reshape([-1,1]) # stack and reshape label\n",
        "  y = ohe.fit_transform(stacked_labels)\n",
        "\n",
        "  # if the dataset is for the decoder return only one column of images\n",
        "  if not for_encoder:\n",
        "    return x, y\n",
        "  \n",
        "  else:\n",
        "    return x1\n",
        "  \n",
        "\n",
        "def normalize_boards(boards):\n",
        "  \"\"\"\n",
        "  Normalize a list of boards. The boards already have unit variance so we only need to zero-mean them.\n",
        "  \"\"\"\n",
        "  \n",
        "  sum = 0\n",
        "\n",
        "  # check what is the avarage of the boards:\n",
        "  for board in boards:\n",
        "    sum += board.sum()\n",
        "  \n",
        "  avarage = sum / (len(boards) * BOARD_SIZE[0] * BOARD_SIZE[1])\n",
        "  \n",
        "  # normalize boards\n",
        "  norm_boards = [board - avarage for board in boards]\n",
        "\n",
        "  return norm_boards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZA3unNtUSBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_siamese_dataset(training_tasks, eval_tasks, test_tasks, get_lists=False):\n",
        "  \"\"\"\n",
        "  Creates a dataset for the siamese networks.\n",
        "\n",
        "  Args:\n",
        "  .._tasks: list of tasks.\n",
        "  get_lists: bool, whether to get just the lists of pairs or the processed dataset.\n",
        "  \"\"\"\n",
        "\n",
        "  # extract all_boards\n",
        "  all_boards = get_all_boards(training_tasks, eval_tasks, test_tasks)\n",
        "\n",
        "  # binirize all boards\n",
        "  all_boards_binary = [get_binary_board(board) for board in all_boards]\n",
        "\n",
        "  # normalize boards --> not sure if necessary\n",
        "  #all_boards_binary = normalize_boards(all_boards_binary)\n",
        "\n",
        "  # create a list of all boards augmentation data\n",
        "  pair_list = []\n",
        "  label_list = []\n",
        "\n",
        "  # iterate over all boards\n",
        "  for i, board in enumerate(all_boards_binary):\n",
        "    board_pairs, board_labels = get_all_pairs_reshaped(board, all_boards[i:]) # augment example. use only boards from here onward\n",
        "    pair_list += board_pairs\n",
        "    label_list += board_labels\n",
        "\n",
        "  ## create dataset\n",
        "  x, y = get_dataset_from_lists(pair_list, label_list)\n",
        "\n",
        "  if get_lists:\n",
        "    return pair_list, label_list\n",
        "\n",
        "  else:\n",
        "    return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV-DsYyV-W23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### just for this notebook\n",
        "x, y = get_siamese_dataset(training_tasks, eval_tasks, test_tasks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gHJfVQ69rDn",
        "colab_type": "text"
      },
      "source": [
        "# Siamese networks architecture\n",
        "- getting the right architecture\n",
        "- went for residual networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-kGomzhzlo7",
        "colab_type": "text"
      },
      "source": [
        "## Defining basic blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsL9Hpfazgxr",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title basic blocks\n",
        "def residual_encoder_block(filter_num, kernel_size, bn_moment):\n",
        "  \"\"\"\n",
        "  A functional style residual connection convolutional block.\n",
        "  \"\"\"\n",
        "\n",
        "  def block(x, filter_num, kernel_size=(3,3), bn_moment=0.9):\n",
        "    # first layer\n",
        "    c1 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(x)\n",
        "    c1 = BatchNormalization(momentum=bn_moment)(c1)\n",
        "\n",
        "    # second layer\n",
        "    c2 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(c1)\n",
        "    c2 = BatchNormalization(momentum=bn_moment)(c2)\n",
        "\n",
        "    # third layer\n",
        "    c3 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(c2)\n",
        "    c3 = BatchNormalization(momentum=bn_moment)(c3)\n",
        "\n",
        "    # residual connection\n",
        "    res = c1 + c3\n",
        "\n",
        "    return res\n",
        "  \n",
        "  return lambda x: block(x, filter_num, kernel_size, bn_moment)\n",
        "\n",
        "def residual_decoder_block(filter_num, kernel_size, bn_moment, first=False):\n",
        "  \"\"\"\n",
        "  A functional style residual connection deconvolutional block.\n",
        "\n",
        "  Args:\n",
        "  first - bool, determains if it is the first block of a network\n",
        "  \"\"\"\n",
        "  \n",
        "  def block(x, filter_num, kernel_size=(3,3), bn_moment=0.9):\n",
        "    \n",
        "    # first layer\n",
        "    if first:\n",
        "      dc1 = Dense(4*4*filter_num, activation='linear')(x) # num neurons is dependent upon the number of blocks\n",
        "      dc1 = BatchNormalization(momentum=bn_moment)(dc1)\n",
        "      dc1 = Reshape(target_shape=(4,4,filter_num))(dc1) # reshaping is dependent upon the number of blocks\n",
        "    \n",
        "    else:\n",
        "      dc1 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(x)\n",
        "      dc1 = BatchNormalization(momentum=bn_moment)(dc1)\n",
        "\n",
        "    # second layer\n",
        "    dc2 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(dc1)\n",
        "    dc2 = BatchNormalization(momentum=bn_moment)(dc2)\n",
        "\n",
        "    # third layer\n",
        "    dc3 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(dc2)\n",
        "    dc3 = BatchNormalization(momentum=bn_moment)(dc3)\n",
        "\n",
        "    # residual connection\n",
        "    res = dc1 + dc3\n",
        "\n",
        "    return res\n",
        "\n",
        "  return lambda x: block(x, filter_num, kernel_size, bn_moment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU9XOnUA99dD",
        "colab_type": "text"
      },
      "source": [
        "## Main architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISOAGouf9ym1",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def get_encoder(input_shape):\n",
        "    \"\"\"\n",
        "    Creates an encoder with residual connections\n",
        "    \"\"\"\n",
        "\n",
        "    x = Input(input_shape, name='x')  \n",
        "\n",
        "    # first block\n",
        "    block1 = residual_encoder_block(16, kernel_size=(3,3), bn_moment=0.9)(x)\n",
        "    mp1 = MaxPool2D(pool_size=(2,2), name='max_pool1')(block1)\n",
        "\n",
        "    # second block\n",
        "    block2 = residual_encoder_block(32, kernel_size=(3,3), bn_moment=0.9)(mp1)\n",
        "    mp2 = MaxPool2D(pool_size=(2,2), name='max_pool2')(block2)\n",
        "\n",
        "    # third block\n",
        "    block3 = residual_encoder_block(64, kernel_size=(3,3), bn_moment=0.9)(mp2)\n",
        "    mp3 = MaxPool2D(pool_size=(2,2), name='max_pool3')(block3)\n",
        "\n",
        "    # flatten\n",
        "    flat = Flatten()(mp3)\n",
        "\n",
        "    # dense\n",
        "    dense = Dense(DENSE_REP_SIZE, activation='sigmoid', name='dense_rep')(flat)\n",
        "    bn_dense = BatchNormalization(momentum=0.9)(dense)\n",
        "\n",
        "    return tf.keras.Model(inputs=x, outputs=bn_dense)\n",
        "\n",
        "def get_siamese_networks_model(input_shape):\n",
        "  \"\"\"\n",
        "  Creates siamese networks model. ref paper: https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf\n",
        "  \"\"\"\n",
        "\n",
        "  # define input vectors\n",
        "  input1 = Input(input_shape, name='input1')\n",
        "  input2 = Input(input_shape, name='input2')\n",
        "\n",
        "  # get shared encoder\n",
        "  encoder = get_encoder(input_shape)\n",
        "  \n",
        "  # get feature vectors\n",
        "  v1 = encoder(input1)\n",
        "  v2 = encoder(input2)\n",
        "\n",
        "  # compute L1 loss\n",
        "  L1_Layer = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))\n",
        "  L1_diff = L1_Layer([v1, v2])\n",
        "\n",
        "  # compute probs\n",
        "  probs = Dense(5, activation='softmax')(L1_diff)\n",
        "\n",
        "  siamese_net = tf.keras.Model(inputs=[input1, input2], outputs=probs)\n",
        "\n",
        "  # compile\n",
        "  siamese_net.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
        "  return siamese_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEKcGPtKgbM6",
        "colab_type": "text"
      },
      "source": [
        "## Training siamese networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0kKJwuy2t2R",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "sn = get_siamese_networks_model([*BOARD_SIZE, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfvNty6FbxNA",
        "colab_type": "code",
        "outputId": "f1457028-d866-48c4-a3df-b421dfe001fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "hist = sn.fit(x=x, y=y, epochs=30, batch_size=SN_BATCH_SIZE, shuffle=True, validation_split=0.2)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            " 299/2343 [==>...........................] - ETA: 37s - loss: 0.2887 - categorical_accuracy: 0.9015"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-3f7152cfe031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSN_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    850\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDuSUF8lRP0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_training_curves(hist.history, metric='categorical_accuracy', with_val=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkiOZvd7gOkP",
        "colab_type": "text"
      },
      "source": [
        "# Board generator (decoder) architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ir0OqLyUPa_",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def get_encoder_from_siamese(siamese):\n",
        "  \"\"\"\n",
        "  Creates a board encoder from the siamese networks model. Maps board --> feature vector of dim 512\n",
        "  \"\"\"\n",
        "\n",
        "  # create encoder from all layers up to dense\n",
        "  model = tf.keras.models.Sequential([\n",
        "                                      tf.keras.layers.InputLayer(input_shape=[*BOARD_SIZE, 1]),\n",
        "                                      siamese.layers[2] # all sequential layers from siamese networks model\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "def get_decoder():\n",
        "  \"\"\"\n",
        "  Creates a decoder. Maps feature vector of dim DENSE_REP_SIZE --> board. \n",
        "  Same architecture from encoder is preserved.\n",
        "  \"\"\"\n",
        "\n",
        "  # build model\n",
        "  inp = Input([DENSE_REP_SIZE])\n",
        "\n",
        "  # first_block\n",
        "  block1 = residual_decoder_block(64, kernel_size=(3,3), bn_moment=0.9, first=True)(inp)\n",
        "  us1 = UpSampling2D(size=(2,2))(block1)\n",
        "\n",
        "  block2 = residual_decoder_block(32, kernel_size=(3,3), bn_moment=0.9)(us1)\n",
        "  us2 = UpSampling2D(size=(2,2))(block2)\n",
        "\n",
        "  block3 = residual_decoder_block(16, kernel_size=(3,3), bn_moment=0.9)(us2)\n",
        "\n",
        "  output = Conv2D(1, kernel_size=(1,1), activation='sigmoid')(block3)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inp, outputs=output)\n",
        "\n",
        "  # compile\n",
        "  model.compile(loss=pixelwise_error_loss, optimizer='adam', metrics=[pixelwise_auc])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def pixelwise_error_loss(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Custom loss function. Pixelwise error from the true image. ||y_true - y_pred||\n",
        "  \"\"\"\n",
        "\n",
        "  return tf.keras.backend.sum((y_true - y_pred) ** 2) / y_pred.shape[0]\n",
        "\n",
        "\n",
        "def pixelwise_accuracy(y_true, y_pred, threshold=0.8):\n",
        "  \"\"\"\n",
        "  Custom accuracy function. Pixelwise accuracy of predicting 0 / 1\n",
        "  \"\"\"\n",
        "  \n",
        "  tp = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred >= threshold), 'float32'), y_true)) # cast boolean to binary\n",
        "  tn = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred < threshold), 'float32'), tf.cast((y_true == 0), 'float32')))\n",
        "  n_predictions = BOARD_SIZE[0] * BOARD_SIZE[1]\n",
        "\n",
        "  return ((tp + tn) / n_predictions) / y_pred.shape[0]\n",
        "\n",
        "\n",
        "def pixelwise_sensitivity(y_true, y_pred, threshold=0.8):\n",
        "  \"\"\"\n",
        "  Pixelwise sensitivity of correctly predicting 1s. true positives / positives\n",
        "  \"\"\"\n",
        "\n",
        "  tp = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred >= threshold), 'float32'), y_true)) \n",
        "  p = tf.cast(tf.math.reduce_sum(y_true), 'float32') \n",
        "  \n",
        "  return tf.cond(p > 0, lambda: tp / p, lambda: tf.constant(1, dtype='float32')) # don't divide by zero\n",
        "\n",
        "def pixelwise_fpr(y_true, y_pred, threshold=0.8):\n",
        "  \"\"\"\n",
        "  Pixelwise false positive rate of wrongly predicting 1s. false positives / negatives\n",
        "  \"\"\"\n",
        "\n",
        "  fp = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred >= threshold), 'float32'), tf.cast((y_true==0), 'float32'))) # cast boolean to binary\n",
        "  n = tf.cast(tf.math.reduce_sum(tf.cast((y_true == 0), 'float32')), 'float32')\n",
        "\n",
        "  return tf.cond(n > 0, lambda: fp / n, lambda: tf.constant(1, dtype='float32')) # don't divide by zero\n",
        "\n",
        "\n",
        "def pixelwise_auc(y_true, y_pred, num_thresholds=10):\n",
        "  \"\"\"\n",
        "  Pixelwise auc score. \n",
        "  \"\"\"\n",
        "\n",
        "  sensitivity = []\n",
        "  fpr = []\n",
        "\n",
        "  # compute graph points\n",
        "  for threshold in np.linspace(1, 0, num=num_thresholds):\n",
        "    sensitivity.append(pixelwise_sensitivity(y_true, y_pred, threshold=threshold))\n",
        "    fpr.append(pixelwise_fpr(y_true, y_pred, threshold=threshold))\n",
        "  \n",
        "  # compute trapeze area\n",
        "  trapeze = []\n",
        "  for i in range(len(sensitivity)- 1):\n",
        "    area = tf.multiply(sensitivity[i], fpr[i+1] - fpr[i])\n",
        "    trapeze.append(area)\n",
        "  \n",
        "  auc_score = tf.add_n(trapeze) \n",
        "\n",
        "  return auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNIBD7hmoL-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dec = get_decoder(sn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uBo_DSMrXDZ",
        "colab_type": "text"
      },
      "source": [
        "## Decoder dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmNwACcSraxu",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def get_decoder_dataset(siamese, encoder_dataset):\n",
        "  \"\"\"\n",
        "  Creates the decoder dataset from the encoder.\n",
        "  \n",
        "  Args:\n",
        "  siamese - siamese model to extract encoder from\n",
        "  encoder dataset - boards to encoder to feature vectors\n",
        "\n",
        "  Returns:\n",
        "  x - boards feature vectors, predicted by encoder\n",
        "  y - boards \n",
        "  \"\"\"\n",
        "  \n",
        "  enc = get_encoder_from_siamese(siamese) # fetch encoder\n",
        "\n",
        "  # build dataset\n",
        "  x = enc.predict(encoder_dataset) # get feature vectors\n",
        "  y = encoder_dataset # boards to recreate\n",
        "\n",
        "  return x,y\n",
        "\n",
        "# create encoder dataset from the old pair and label lists\n",
        "pair_list, label_list = get_siamese_dataset(training_tasks, eval_tasks, test_tasks, get_lists=True)\n",
        "encoder_dataset = get_dataset_from_lists(pair_list, label_list, for_encoder=True) \n",
        "# create decoder dataset from encoder\n",
        "decoder_x, decoder_y = get_decoder_dataset(sn, encoder_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg9iv1vPtEEe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "d12eabe2-9e7d-4bfb-d9bf-e603e533caa5"
      },
      "source": [
        "hist_decoder = dec.fit(x=decoder_x, y=decoder_y, batch_size=DECODER_BATCH_SIZE, shuffle=True, epochs=10, validation_split=0.2)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "18744/18744 [==============================] - 203s 11ms/step - loss: 10.9392 - pixelwise_auc: 0.9271 - val_loss: 12.4255 - val_pixelwise_auc: 0.9009\n",
            "Epoch 2/10\n",
            "18744/18744 [==============================] - 202s 11ms/step - loss: 10.3647 - pixelwise_auc: 0.9329 - val_loss: 12.1614 - val_pixelwise_auc: 0.8943\n",
            "Epoch 3/10\n",
            "  713/18744 [>.............................] - ETA: 2:58 - loss: 10.0382 - pixelwise_auc: 0.9386"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-01e640f77e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDECODER_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    850\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pHSQ3pYcTII",
        "colab_type": "text"
      },
      "source": [
        "# Check regular encoder-decoder architecture\n",
        "## For checking some hyperparameters and deciding which adecoder architecture to use\n",
        "\n",
        "## Some takeaways:\n",
        "- adding two sets of randomly padded data helps alot. more doesnt help\n",
        "- small decoder batch size (4-16) really helps\n",
        "- adding more neurons to the compressed represantation (512 --> 1024) really helps\n",
        "- removing the last layers batch normalization really helps\n",
        " - removing bn before compressed representation isnt good\n",
        "- penelizing mistaking 1's for 0's more doesnt help at all\n",
        "- changing last layer activation to linear and clipping to [0,1] helps to some extent\n",
        "- chagning middle to relu - really bad!!!\n",
        "- addind another conv-deconv layer at the beggining helps a little\n",
        "- adding another conv-deconv layer at the end doesnt help at all\n",
        "- residual layers - really cool! used the architecture from this paper -- >https://github.com/omerhac/arc_challenge/blob/master/deep%20residual%20conv-deconv%20network.pdf\n",
        "- more types of data augmentation....\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzGZn1y0vt54",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRvqZ5OczS9J",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def get_encoder_decoder(input_shape):\n",
        "  \"\"\"\n",
        "  Create an encoder decoer \"normal\" architecture, with residual connections\n",
        "  \"\"\"\n",
        "  inp = tf.keras.layers.Input([*BOARD_SIZE, 1])\n",
        "  \n",
        "  # encoder\n",
        "  encoder = get_encoder(input_shape)\n",
        "\n",
        "  # dense representation\n",
        "  dense_rep = encoder(inp)\n",
        "\n",
        "  # decoder\n",
        "  decoder = get_decoder()\n",
        "  pred_board = decoder(dense_rep)\n",
        "\n",
        "  model = tf.keras.Model(inp, pred_board)\n",
        "\n",
        "  # compile\n",
        "  model.compile(loss=pixelwise_error_loss, optimizer='adam', metrics=[pixelwise_auc])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mharVxoMMqJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ed = get_encoder_decoder([*BOARD_SIZE, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb0_gTC8nIdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "3f9bb846-d1f0-4932-a787-ae116a061fd3"
      },
      "source": [
        "ed_hist = ed.fit(x=decoder_y, y=decoder_y, batch_size=8, shuffle=True, epochs=20, validation_split=0.2) # get image --> predict image"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "18744/18744 [==============================] - 357s 19ms/step - loss: 7.7845 - pixelwise_auc: 0.9262 - val_loss: 5.5363 - val_pixelwise_auc: 0.9497\n",
            "Epoch 2/20\n",
            "18744/18744 [==============================] - 356s 19ms/step - loss: 3.6183 - pixelwise_auc: 0.9699 - val_loss: 3.8067 - val_pixelwise_auc: 0.9677\n",
            "Epoch 3/20\n",
            "18744/18744 [==============================] - 358s 19ms/step - loss: 2.6270 - pixelwise_auc: 0.9796 - val_loss: 3.0955 - val_pixelwise_auc: 0.9774\n",
            "Epoch 4/20\n",
            "18744/18744 [==============================] - 357s 19ms/step - loss: 2.1404 - pixelwise_auc: 0.9834 - val_loss: 2.9063 - val_pixelwise_auc: 0.9794\n",
            "Epoch 5/20\n",
            " 7597/18744 [===========>..................] - ETA: 3:16 - loss: 1.8678 - pixelwise_auc: 0.9857"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9ii2z4hh9de",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## check predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eytnCHw0iUIS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63dd6592-9a75-4dcd-9129-f29fdc05aeca"
      },
      "source": [
        "boards = decoder_y[-10:]\n",
        "\n",
        "# predict\n",
        "predictions = ed.predict(boards)\n",
        "\n",
        "# reshape\n",
        "boards = [plotting_shape_board(board) for board in boards]\n",
        "predictions = [plotting_shape_board(board) for board in predictions]\n",
        "pairs = zip(boards, predictions)\n",
        "\n",
        "plot_decoder_boards(list(pairs))\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x2160 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kujm22z3y0qa",
        "colab_type": "text"
      },
      "source": [
        "# Interleaved training (decoder/encoder/deocoder..)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5XjrxlkyZOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def copy_decoder_to_siamese(siamese, decoder):\n",
        "  \"\"\"\n",
        "  Copys the weights from the decoder to the siamese networks model\n",
        "  \"\"\"\n",
        "\n",
        "  decoder_layers = decoder.layers # get decoder layers\n",
        "  siamese_layers = siamese.layers[2].layers # siamese encoder layers\n",
        "\n",
        "  for i, layer in enumerate(decoder_layers[::-1]):\n",
        "      if(2 >= len(layer.weights) > 0): # trainable layer and not BN layer\n",
        "        if layer.weights[0].shape[0] < 100: # deconv layers\n",
        "          decoder_layer_weigths = layer.get_weights() # get weights from decoder\n",
        "          bias = np.zeros(siamese_layers[i].get_weights()[1].shape) # init new bias\n",
        "          w = decoder_layer_weigths[0].transpose([1,0,2,3]) # transpose weights\n",
        "          siamese_layers[i].set_weights([w, bias]) # set siamese weights\n",
        "\n",
        "        else:\n",
        "          # dense layer\n",
        "          decoder_layer_weigths = layer.get_weights() # get weights from decoder\n",
        "          bias = np.zeros(siamese_layers[i].get_weights()[1].shape) # init new bias\n",
        "          w = decoder_layer_weigths[0].transpose() # transpose weights\n",
        "          siamese_layers[i].set_weights([w, bias]) # set siamese weights\n",
        "\n",
        "\n",
        "def copy_siamese_to_decoder(siamese, decoder):\n",
        "  \"\"\"\n",
        "  Copys the weights from the siamese networks model to the decoder\n",
        "  \"\"\"\n",
        "\n",
        "  decoder_layers = decoder.layers # get decoder layers\n",
        "  siamese_layers = siamese.layers[2].layers # siamese encoder layers\n",
        "\n",
        "  for i, layer in enumerate(decoder_layers[::-1]):\n",
        "      if(2 >= len(layer.weights) > 0): # trainable layer and not BN layer\n",
        "        if layer.weights[0].shape[0] < 100: # deconv layers\n",
        "          siamese_layer_weigths = siamese_layers[i].get_weights() # get weights from siamese encoder\n",
        "          bias = np.zeros(layer.get_weights()[1].shape) # init new bias\n",
        "          w = siamese_layer_weigths[0].transpose([1,0,2,3]) # transpose weights\n",
        "          layer.set_weights([w, bias]) # set decoder weights\n",
        "\n",
        "        else:\n",
        "          # dense layer\n",
        "          siamese_layer_weigths = siamese_layers[i].get_weights() # get weights from siamese encoder\n",
        "          bias = np.zeros(layer.get_weights()[1].shape) # init new bias\n",
        "          w = siamese_layer_weigths[0].transpose() # transpose weights\n",
        "          layer.set_weights([w, bias]) # set decoder weights\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2ucFsFkzIkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create new models with same weights\n",
        "EPOCHS = 30\n",
        "sn = get_siamese_networks_model([*BOARD_SIZE, 1])\n",
        "decoder = get_decoder(sn, copy_encoder_weights=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pfDn78N4ecn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "SN_BATCH_SIZE = 64\n",
        "DEC_BATCH_SIZE = 4\n",
        "DATA_SPLIT = 5\n",
        "SN_STEPS_PER_EPOCH = len(x) // SN_BATCH_SIZE\n",
        "DEC_STEPS_PER_EPOCH = len(decoder_x) // DEC_BATCH_SIZE\n",
        "\n",
        "# even fancier training loop\n",
        "for i in range(EPOCHS):\n",
        "  print(\"training... epoch num: {}\".format(i))\n",
        "\n",
        "  for i in range(40):\n",
        "    # train siamese\n",
        "    _ = sn.fit(x=x, y=y, epochs=1, batch_size=SN_BATCH_SIZE, shuffle=True, steps_per_epoch=(780//DATA_SPLIT))\n",
        "    \n",
        "    # copy weights\n",
        "    copy_siamese_to_decoder(sn, decoder)\n",
        "\n",
        "    # get new decoder dataset\n",
        "    decoder_x, decoder_y = get_decoder_dataset(sn, encoder_dataset) # encoder dataset is the same as before\n",
        "\n",
        "    # train decoder\n",
        "    _ = decoder.fit(x=decoder_x, y=decoder_y, epochs=1, batch_size=DEC_BATCH_SIZE, shuffle=True, steps_per_epoch=(24000//DATA_SPLIT)) #### WOW!!! use smaller batch size, WOHOOO!!\n",
        "\n",
        "    # copy weights\n",
        "    copy_decoder_to_siamese(sn, decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BefQARZeJ6OK",
        "colab_type": "text"
      },
      "source": [
        "# Predicting from diffrance in boards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Api-_erL_rs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task_training_input, task_training_output, task_test_input, task_test_output = get_task_boards(training_tasks[0], pad=pad, divide_sets=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmT5I2c2KNS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# toy dataset (x is the dataset for the siamese networks)\n",
        "anchor = x[0][0]\n",
        "rotate_once_anchor = x[1][1]\n",
        "rotate_twice_anchor = x[1][2]\n",
        "\n",
        "toy = np.stack([anchor, rotate_once_anchor, rotate_twice_anchor])\n",
        "\n",
        "# plot\n",
        "fig, axs = plt.subplots(1,3)\n",
        "plot_board(plotting_shape_board(anchor), axs[0])\n",
        "plot_board(plotting_shape_board(rotate_once_anchor), axs[1])\n",
        "plot_board(plotting_shape_board(rotate_twice_anchor), axs[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVmhKDCOOEYJ",
        "colab_type": "text"
      },
      "source": [
        "## define predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uVadzm3Lw3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictor(decoder):\n",
        "  \"\"\"\n",
        "  Builds a detector which takes a board and a rules vector and predicts output board.\n",
        "\n",
        "  Args:\n",
        "  decoder --> trained decoder \n",
        "  \"\"\"\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}