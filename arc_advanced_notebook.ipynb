{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arc.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omerhac/arc_challenge/blob/master/arc_advanced_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YShrwCd2GUWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import json\n",
        "from google.cloud import storage\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import colors\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Conv2D, Lambda, Dense, Flatten, MaxPool2D, Input, BatchNormalization, Conv2DTranspose, UpSampling2D, Reshape\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uko3X-E_CrZr",
        "colab_type": "code",
        "outputId": "a1d0dbe9-8c76-4b1b-e199-96e3a751dbde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# get repository from github\n",
        "!git clone https://github.com/omerhac/arc_challenge.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'arc_challenge'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (134/134), done.\u001b[K\n",
            "remote: Compressing objects: 100% (132/132), done.\u001b[K\n",
            "Receiving objects: 100% (134/134), 10.49 MiB | 12.97 MiB/s, done.\n",
            "remote: Total 134 (delta 55), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcDHkw11CtSu",
        "colab_type": "code",
        "outputId": "1b6da9c1-2244-4f21-80b4-b73f30d4b901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# navigate to directory\n",
        "cd arc_challenge"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/arc_challenge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX5E2ZNODM5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load repository dependencies\n",
        "import preprocess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF4Ovm97vXEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## constants ##\n",
        "BOARD_SIZE = (16,16) # board upperbound size\n",
        "SN_BATCH_SIZE = 64\n",
        "DECODER_BATCH_SIZE = 8\n",
        "DENSE_REP_SIZE = 1024 # dense vector represantation size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqOKR-POOXki",
        "colab_type": "text"
      },
      "source": [
        "# Load data / util funcitons\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-96BlT_hHVkF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title load data\n",
        "def load_data():\n",
        "  \"\"\"\n",
        "  Loads all the data into training_tasks, eval_tasks and test_tasks\n",
        "  \"\"\"\n",
        "\n",
        "  ## get paths\n",
        "  GCS_PATH = \"gs://kds-d3cfb3d523ca35d2517017a78110126404d01fdea69417ce49950459\"\n",
        "  training_filenames = tf.io.gfile.glob(GCS_PATH + \"/training/*\")\n",
        "  test_filenames = tf.io.gfile.glob(GCS_PATH + \"/test/*\")\n",
        "  eval_filenames = tf.io.gfile.glob(GCS_PATH + \"/evaluation/*\")\n",
        "\n",
        "  # create datasets with filenames\n",
        "  training_dataset = tf.data.Dataset.list_files(training_filenames)\n",
        "  eval_dataset = tf.data.Dataset.list_files(eval_filenames)\n",
        "  test_dataset = tf.data.Dataset.list_files(test_filenames)\n",
        "\n",
        "  # load the jsons\n",
        "  def load_task(filename):\n",
        "    task_json = tf.io.read_file(filename)\n",
        "    return task_json\n",
        "\n",
        "  training_dataset = training_dataset.map(load_task)\n",
        "  eval_dataset = eval_dataset.map(load_task)\n",
        "  test_dataset = test_dataset.map(load_task)\n",
        "\n",
        "  training_dataset_numpy = tf.data.Dataset.as_numpy_iterator(training_dataset) # convert to numpy iterator\n",
        "  eval_dataset_numpy = tf.data.Dataset.as_numpy_iterator(eval_dataset)\n",
        "  test_dataset_numpy = tf.data.Dataset.as_numpy_iterator(test_dataset)\n",
        "\n",
        "  ## create a numpy array of tasks (n_tasks, )\n",
        "  def list_from_jsons(jsons_numpy_iterator):\n",
        "    \"\"\"\n",
        "      Create a list of task dictionaries from jsons numpy interator\n",
        "    \"\"\"\n",
        "\n",
        "    tasks = []\n",
        "    for task in jsons_numpy_iterator:\n",
        "      tasks.append(json.loads(task))\n",
        "\n",
        "    return tasks\n",
        "\n",
        "  ## get numpy arrays of datasets\n",
        "  training_tasks = list_from_jsons(training_dataset_numpy)\n",
        "  eval_tasks = list_from_jsons(eval_dataset_numpy)\n",
        "  test_tasks = list_from_jsons(test_dataset_numpy)\n",
        "\n",
        "  return training_tasks, eval_tasks, test_tasks\n",
        "\n",
        "\n",
        "def load_data_from_jsons():\n",
        "  \"\"\"\n",
        "  Load tasks from jsons to lists of tasks.\n",
        "\n",
        "  Returns:\n",
        "  training_tasks, eval_tasks, test_tasks --> lists of tasks\n",
        "  \"\"\"\n",
        "\n",
        "  with open(\"training_tasks.json\", 'r') as f:\n",
        "    training_tasks = json.load(f)\n",
        "\n",
        "  with open(\"eval_tasks.json\", 'r') as f:\n",
        "    eval_tasks = json.load(f)\n",
        "  \n",
        "  with open(\"test_tasks.json\", 'r') as f:\n",
        "    test_tasks = json.load(f)\n",
        "\n",
        "  return training_tasks, eval_tasks, test_tasks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oOyaYHAJr1A",
        "colab_type": "code",
        "outputId": "192390bb-685b-4380-86f4-d44f69be28f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%%time\n",
        "training_tasks, eval_tasks, test_tasks = load_data_from_jsons()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 232 ms, sys: 16.2 ms, total: 248 ms\n",
            "Wall time: 249 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYLEt0yrPXf9",
        "colab_type": "text"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO9FwJjFNAnO",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Utilities\n",
        "def plot_board(board, ax, title=\"\"):\n",
        "  \"\"\"\n",
        "  Plot a board on a given axis\n",
        "  \"\"\"\n",
        "  cmap = colors.ListedColormap(\n",
        "      ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
        "        '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
        "  norm = colors.Normalize(vmin=0, vmax=9)\n",
        "  \n",
        "  ax.imshow(board, cmap=cmap, norm=norm)\n",
        "  ax.grid(True,which='both',color='lightgrey', linewidth=0.5)    \n",
        "  ax.set_yticks([x-0.5 for x in range(1+board.shape[0])])\n",
        "  ax.set_xticks([x-0.5 for x in range(1+board.shape[1])])     \n",
        "  ax.set_xticklabels([])\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_title(title)\n",
        "\n",
        "def plot_one(task, ax, i,train_or_test,input_or_output):\n",
        "  \"\"\"\n",
        "  Plot one task on a given axis\n",
        "  \"\"\"\n",
        "  cmap = colors.ListedColormap(\n",
        "      ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
        "        '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
        "  norm = colors.Normalize(vmin=0, vmax=9)\n",
        "  \n",
        "  input_matrix = task[train_or_test][i][input_or_output]\n",
        "  ax.imshow(input_matrix, cmap=cmap, norm=norm)\n",
        "  ax.grid(True,which='both',color='lightgrey', linewidth=0.5)    \n",
        "  ax.set_yticks([x-0.5 for x in range(1+len(input_matrix))])\n",
        "  ax.set_xticks([x-0.5 for x in range(1+len(input_matrix[0]))])     \n",
        "  ax.set_xticklabels([])\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_title(train_or_test + ' '+input_or_output)\n",
        "    \n",
        "\n",
        "def plot_task(task):\n",
        "    \"\"\"\n",
        "    Plots the first train and test pairs of a specified task,\n",
        "    using same color scheme as the ARC app\n",
        "    \"\"\"    \n",
        "    num_train = len(task['train'])\n",
        "    fig, axs = plt.subplots(2, num_train, figsize=(3*num_train,3*2))\n",
        "    for i in range(num_train):     \n",
        "        plot_one(task, axs[0,i],i,'train','input')\n",
        "        plot_one(task, axs[1,i],i,'train','output')        \n",
        "    plt.tight_layout()\n",
        "    plt.show()        \n",
        "        \n",
        "    num_test = len(task['test'])\n",
        "    fig, axs = plt.subplots(2, num_test, figsize=(3*num_test,3*2))\n",
        "    if num_test==1: \n",
        "        plot_one(task, axs[0],0,'test','input')\n",
        "        plot_one(task, axs[1],0,'test','output')     \n",
        "    else:\n",
        "        for i in range(num_test):      \n",
        "            plot_one(task, axs[0,i],i,'test','input')\n",
        "            plot_one(task, axs[1,i],i,'test','output')  \n",
        "    plt.tight_layout()\n",
        "    plt.show() \n",
        "  \n",
        "\n",
        "def plot_board_pairs(board_pairs, labels):\n",
        "  \"\"\"\n",
        "  Plots the board pairs (for siamese networks) with their label as a title\n",
        "  \"\"\"\n",
        "\n",
        "  fig, axs = plt.subplots(len(board_pairs), 2, figsize=(8, 3 * len(board_pairs)))\n",
        "  \n",
        "  for i, pair in enumerate(board_pairs):\n",
        "    # plot a pair on a given axis\n",
        "    plot_board(pair[0], axs[i, 0], title=\"anchor\") \n",
        "    plot_board(pair[1], axs[i, 1], title=labels[i])\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_decoder_boards(board_pairs):\n",
        "  \"\"\"\n",
        "  Plots the board pairs (for siamese networks) with their label as a title\n",
        "  \"\"\"\n",
        "\n",
        "  fig, axs = plt.subplots(len(board_pairs), 2, figsize=(8, 3 * len(board_pairs)))\n",
        "\n",
        "  for i, pair in enumerate(board_pairs):\n",
        "    # plot a pair on a given axis\n",
        "    plot_board(pair[0], axs[i, 0]) \n",
        "    plot_board(pair[1], axs[i, 1])\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def display_training_curves(hist, metric='accuracy', with_val=False):\n",
        "  \"\"\"display learning curves for keras history dict, args: history dict, with val --> boolean with/without val\"\"\"\n",
        "  plt.figure(figsize=(18,6))\n",
        "\n",
        "  # accuracy plots\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(hist[metric])\n",
        "  \n",
        "  if with_val:\n",
        "    plt.plot(hist['val_' + metric])\n",
        "    plt.legend(['Train', 'Validation'])\n",
        "  \n",
        "  else:\n",
        "    plt.legend(['Train'])\n",
        "  \n",
        "  plt.title('Model accuracy')\n",
        "  plt.xlabel('EPOCH')\n",
        "  plt.ylabel('Accuracy')\n",
        "\n",
        "  # loss plots\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(hist['loss'])\n",
        "\n",
        "  if with_val:\n",
        "    plt.plot(hist['val_loss'])\n",
        "    plt.legend(['Train loss', 'Val loss'])\n",
        "  \n",
        "  else:\n",
        "    plt.legend(['Train loss'])\n",
        "  \n",
        "  plt.title('Model loss')\n",
        "  plt.xlabel('EPOCH')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9AnAL0VU2Tu",
        "colab_type": "text"
      },
      "source": [
        "# Data generating functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuTeaiVASRhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset_shapes(dataset):\n",
        "  \"\"\"\n",
        "  Returns dataset board shapes. \n",
        "  \"\"\"\n",
        "\n",
        "  shape_0 = []\n",
        "  shape_1 = []\n",
        "  \n",
        "  # check every task\n",
        "  for task in dataset:\n",
        "    boards = get_task_boards(task) # get all boards\n",
        "    shape_0 += [board.shape[0] for board in boards]\n",
        "    shape_1 += [board.shape[1] for board in boards]\n",
        "  \n",
        "  return shape_0, shape_1\n",
        "\n",
        "def get_task_boards(task, threshold_shape=(40, 40), pad=None, test=False, divide_sets=False):\n",
        "  \"\"\"\n",
        "  Get the training / testing boards of every example in a specific task. \n",
        "\n",
        "  Args: threshold_shape --> threshold shape for which \n",
        "  biggger samples won't be returned. \n",
        "  pad --> functional padding function to pad boards with (up tp threshold_size).\n",
        "  test --> bool, whether a test task or not. for output of task example.\n",
        "  divide_sets --> bool, whether to divide boards sets to training - input / output, test - input / output.\n",
        "  \"\"\"\n",
        "  \n",
        "  training_input_boards, training_output_boards, test_input_boards, test_output_boards = [], [], [], []\n",
        "\n",
        "  # train boards\n",
        "  for example in task['train']:\n",
        "    input_board = np.array(example['input'])\n",
        "    output_board = np.array(example['output'])\n",
        "\n",
        "    if((input_board.shape[0] < threshold_shape[0]) and (input_board.shape[1] < threshold_shape[1])):\n",
        "      training_input_boards.append(pad(input_board, output_shape=threshold_shape) if pad else input_board) # check for padding func\n",
        "\n",
        "    if((output_board.shape[0] < threshold_shape[0]) and (output_board.shape[1] < threshold_shape[1])):\n",
        "      training_output_boards.append(pad(output_board, output_shape=threshold_shape) if pad else output_board) # check for padding func\n",
        "\n",
        "  # test boards\n",
        "  for example in task['test']:\n",
        "    input_board = np.array(example['input'])\n",
        "\n",
        "    if not test: # check if test example\n",
        "      output_board = np.array(example['output'])\n",
        "    \n",
        "    # check whether the board is smaller then threshold shape\n",
        "    if((input_board.shape[0] < threshold_shape[0]) and (input_board.shape[1] < threshold_shape[1])):\n",
        "      test_input_boards.append(pad(input_board, output_shape=threshold_shape) if pad else input_board) # check for padding func\n",
        "\n",
        "    if((output_board.shape[0] < threshold_shape[0]) and (output_board.shape[1] < threshold_shape[1]) and (not test)):\n",
        "      test_output_boards.append(pad(output_board, output_shape=threshold_shape) if pad else output_board) # check for padding func\n",
        "  \n",
        "  # whether to divide boards:\n",
        "  if divide_sets:\n",
        "    return training_input_boards, training_output_boards, test_input_boards, test_output_boards\n",
        "  else:\n",
        "    return training_input_boards + training_output_boards + test_input_boards + test_output_boards\n",
        "\n",
        "def pad(mat, output_shape, padder=0):\n",
        "  \"\"\"\n",
        "  Pad a matrix with padder up to output_shape. Insert matrix at upper left corner.\n",
        "  \n",
        "  Args:\n",
        "  mat - np.array matrix of rank 2\n",
        "  output_shape - tuple \n",
        "  padder - int\n",
        "  \"\"\"\n",
        "\n",
        "  output_board = np.zeros(shape=output_shape) + padder # create output board and pad it\n",
        "\n",
        "  # get input shape\n",
        "  input_rows = mat.shape[0]\n",
        "  input_cols = mat.shape[1]\n",
        "\n",
        "  # if random=False, insert input matrix in upper left corner\n",
        "  output_board[:input_rows, :input_cols] = mat\n",
        "  return output_board\n",
        "\n",
        "def random_pad(mat, output_shape, padder=0):\n",
        "  \"\"\"\n",
        "  Pad a matrix with padder up to output_shape. Insert the matrix at a random location\n",
        "  \n",
        "  Args:\n",
        "  mat - np.array matrix of rank 2\n",
        "  output_shape - tuple \n",
        "  padder - int\n",
        "  seed - int\n",
        "  \"\"\"\n",
        "\n",
        "  output_board = np.zeros(shape=output_shape) + padder # create output board and pad it\n",
        "\n",
        "  # get input shape\n",
        "  input_rows = mat.shape[0]\n",
        "  input_cols = mat.shape[1]\n",
        "\n",
        "  # insert mat at a random loacation\n",
        "  # get random location\n",
        "  start_row = np.random.randint(output_shape[0] - input_rows)\n",
        "  start_col = np.random.randint(output_shape[1] - input_cols)\n",
        "  # insert\n",
        "  output_board[start_row:start_row+input_rows, start_col:start_col+input_cols] = mat\n",
        "\n",
        "  return output_board\n",
        "\n",
        "def get_all_boards(training_tasks, eval_tasks, test_tasks):\n",
        "  \"\"\"\n",
        "  Extracts all the boards from all training/eval/testing tasks. \n",
        "  \"\"\"\n",
        "\n",
        "  training_boards = []\n",
        "  for task in training_tasks:\n",
        "    training_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=pad)\n",
        "\n",
        "    # augment training set (random pad)\n",
        "    training_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "    training_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "\n",
        "  eval_boards = []\n",
        "  for task in eval_tasks:\n",
        "    eval_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=pad)\n",
        "\n",
        "    # augment eval set (random pad)\n",
        "    eval_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "    eval_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "    \n",
        "\n",
        "  test_boards = []\n",
        "  for task in test_tasks:\n",
        "    test_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=pad, test=True)\n",
        "\n",
        "    # augment test set (random pad)\n",
        "    test_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad, test=True)\n",
        "    test_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad, test=True)\n",
        "\n",
        "  all_boards = training_boards+eval_boards+test_boards\n",
        "  return all_boards\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLXZsdbYodKl",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oofkpiCmt8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_rotated_views(board):\n",
        "  \"\"\"\n",
        "  Turns a board 90 deg counter clockwise 3 times. Returns a list length 4 with all the rotated views including the original one.\n",
        "  \"\"\"\n",
        "\n",
        "  rotates = [board]\n",
        "  for i in range(3):\n",
        "    board = np.rot90(board)\n",
        "    rotates.append(board)\n",
        "  \n",
        "  return rotates\n",
        "\n",
        "def get_rotated_data_pairs(rotated_views):\n",
        "  \"\"\"\n",
        "  Creates all the possible pairs out of the rotated views tensor and labels them. If a1..a4 are rotated vies (A.C.W) of a1, returns labels according to:\n",
        "   - (a1, a1) = 1 --> same board\n",
        "   - (a1, a2) = 2 --> 90 a.c.w rotate\n",
        "   - (a1, a3) = 3 --> 180 rotate\n",
        "   - (a1, a4) = 4 --> 270 a.c.w rotate\n",
        "   - ...\n",
        "   - (a3, a4) = 2 --> 90 a.c.w rotate\n",
        "\n",
        "  Args:\n",
        "  rotated_views = a list of 4 rotated views of the board\n",
        "\n",
        "  Returns:\n",
        "  pairs: a list of tuples of boards\n",
        "  labels: a list of labels matching each pair\n",
        "  \"\"\"\n",
        "\n",
        "  # stopping rule\n",
        "  if rotated_views == []:\n",
        "    return [], []\n",
        "\n",
        "  anchor = rotated_views[0]  # select board to compare with \n",
        "  pairs, labels = [], []\n",
        "  label = 1 # init label\n",
        "\n",
        "  # iterate over all remaining examples\n",
        "  for view in rotated_views:\n",
        "    pairs.append((anchor, view))\n",
        "    labels.append(label)\n",
        "    label += 1 # update label, views rotate a.c.w\n",
        "  \n",
        "  next_pairs, next_labels = get_rotated_data_pairs(rotated_views[1:]) # recursive call\n",
        "\n",
        "  return pairs + next_pairs, labels + next_labels\n",
        "\n",
        "def get_binary_board(board):\n",
        "  \"\"\"\n",
        "  Returns a binary board. Every non 0 value becomes 1. 0 stays 0.\n",
        "  \"\"\"\n",
        "\n",
        "  return (board != 0).astype('int32')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnKVXvD8TmZu",
        "colab_type": "text"
      },
      "source": [
        "## Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibYTel0ORlJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_shape_board(board):\n",
        "  \"\"\"\n",
        "  Reshapes board for moedl digestion. shape [*BOARD_SIZE, 1]\n",
        "  \"\"\"\n",
        "  return board.reshape([*BOARD_SIZE, 1])\n",
        "\n",
        "def plotting_shape_board(board):\n",
        "  \"\"\"\n",
        "  Reshapes the board for plotting. shape [*BOARD_SIZE]\n",
        "  \"\"\"\n",
        "  return board.reshape([*BOARD_SIZE])\n",
        "\n",
        "def get_all_pairs_reshaped(board, all_boards):\n",
        "  \"\"\"\n",
        "  Creates a list of pairs of data for the board and reshapes the boards to shape [*BOARD_SHAPE, 1] for nn digestion. For all the possible rotates and a false match\n",
        "\n",
        "  Args:\n",
        "  board - np.array\n",
        "  all_boards - all the other boards on the dataset not including this one\n",
        "\n",
        "  Returns:\n",
        "  a list of 5 tuples, 4 for the rotation part and 1 for a false match\n",
        "  \"\"\"\n",
        "\n",
        "  rotated_views = get_rotated_views(board)\n",
        "  rotated_pairs, rotated_labels = get_rotated_data_pairs(rotated_views)\n",
        "  rotated_pairs = [(model_shape_board(t[0]), model_shape_board(t[1])) for t in rotated_pairs] # reshape boards\n",
        "\n",
        "  # create false match pair\n",
        "  different_board = all_boards[np.random.randint(len(all_boards))] # generate random example\n",
        "  false_label = 0\n",
        "\n",
        "  return rotated_pairs + [[board.reshape([*BOARD_SIZE, 1]), different_board.reshape([*BOARD_SIZE, 1])]], rotated_labels + [false_label]\n",
        "\n",
        "def get_dataset_from_lists(pair_list, label_list, for_encoder=False):\n",
        "  \"\"\"\n",
        "  Creates an x,y dataset from a list of pairs and a list of labels. One hot encode labels\n",
        "\n",
        "  Args:\n",
        "  pair list: a list of pairs of images\n",
        "  label list: a list of ints - labels\n",
        "  for_encoder: bool - determains if the dataset if for the encoder\n",
        "\n",
        "  Retrurns:\n",
        "  x - [x1, x2] a list of two numpy array, each of shape [n_samples, *BOARD_SHAPE, 1] containing stacked inputs of one of the siamese twins OR only x1 if for encoder\n",
        "  y - one hot labels, only for siamese networks\n",
        "  \"\"\"\n",
        "\n",
        "  # prepare x\n",
        "  pairs_stacked = np.stack(pair_list)\n",
        "  x1 = pairs_stacked[:,0,:,:] # get one column of inputs\n",
        "  x2 = pairs_stacked[:,1,:,:] # get the second one\n",
        "  x = [x1, x2]\n",
        "\n",
        "  # prepare y\n",
        "  ohe = OneHotEncoder(sparse=False) # sparse = false is super importent!!\n",
        "  stacked_labels = np.stack(label_list).reshape([-1,1]) # stack and reshape label\n",
        "  y = ohe.fit_transform(stacked_labels)\n",
        "\n",
        "  # if the dataset is for the decoder return only one column of images\n",
        "  if not for_encoder:\n",
        "    return x, y\n",
        "  \n",
        "  else:\n",
        "    return x1\n",
        "  \n",
        "\n",
        "def normalize_boards(boards):\n",
        "  \"\"\"\n",
        "  Normalize a list of boards. The boards already have unit variance so we only need to zero-mean them.\n",
        "  \"\"\"\n",
        "  \n",
        "  sum = 0\n",
        "\n",
        "  # check what is the avarage of the boards:\n",
        "  for board in boards:\n",
        "    sum += board.sum()\n",
        "  \n",
        "  avarage = sum / (len(boards) * BOARD_SIZE[0] * BOARD_SIZE[1])\n",
        "  \n",
        "  # normalize boards\n",
        "  norm_boards = [board - avarage for board in boards]\n",
        "\n",
        "  return norm_boards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZA3unNtUSBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_siamese_dataset(training_tasks, eval_tasks, test_tasks, get_lists=False):\n",
        "  \"\"\"\n",
        "  Creates a dataset for the siamese networks.\n",
        "\n",
        "  Args:\n",
        "  .._tasks: list of tasks.\n",
        "  get_lists: bool, whether to get just the lists of pairs or the processed dataset.\n",
        "  \"\"\"\n",
        "\n",
        "  # extract all_boards\n",
        "  all_boards = get_all_boards(training_tasks, eval_tasks, test_tasks)\n",
        "\n",
        "  # binirize all boards\n",
        "  all_boards_binary = [get_binary_board(board) for board in all_boards]\n",
        "\n",
        "  # normalize boards --> not sure if necessary\n",
        "  #all_boards_binary = normalize_boards(all_boards_binary)\n",
        "\n",
        "  # create a list of all boards augmentation data\n",
        "  pair_list = []\n",
        "  label_list = []\n",
        "\n",
        "  # iterate over all boards\n",
        "  for i, board in enumerate(all_boards_binary):\n",
        "    board_pairs, board_labels = get_all_pairs_reshaped(board, all_boards[i:]) # augment example. use only boards from here onward\n",
        "    pair_list += board_pairs\n",
        "    label_list += board_labels\n",
        "\n",
        "  ## create dataset\n",
        "  x, y = get_dataset_from_lists(pair_list, label_list)\n",
        "\n",
        "  if get_lists:\n",
        "    return pair_list, label_list\n",
        "\n",
        "  else:\n",
        "    return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV-DsYyV-W23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### just for this notebook\n",
        "x, y = get_siamese_dataset(training_tasks, eval_tasks, test_tasks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gHJfVQ69rDn",
        "colab_type": "text"
      },
      "source": [
        "# Siamese networks architecture\n",
        "- getting the right architecture\n",
        "- went for residual networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-kGomzhzlo7",
        "colab_type": "text"
      },
      "source": [
        "## Defining basic blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsL9Hpfazgxr",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title basic blocks\n",
        "def residual_encoder_block(filter_num, kernel_size, bn_moment):\n",
        "  \"\"\"\n",
        "  A functional style residual connection convolutional block.\n",
        "  \"\"\"\n",
        "\n",
        "  def block(x, filter_num, kernel_size=(3,3), bn_moment=0.9):\n",
        "    # first layer\n",
        "    c1 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(x)\n",
        "    c1 = BatchNormalization(momentum=bn_moment)(c1)\n",
        "\n",
        "    # second layer\n",
        "    c2 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(c1)\n",
        "    c2 = BatchNormalization(momentum=bn_moment)(c2)\n",
        "\n",
        "    # third layer\n",
        "    c3 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(c2)\n",
        "    c3 = BatchNormalization(momentum=bn_moment)(c3)\n",
        "\n",
        "    # residual connection\n",
        "    res = c1 + c3\n",
        "\n",
        "    return res\n",
        "  \n",
        "  return lambda x: block(x, filter_num, kernel_size, bn_moment)\n",
        "\n",
        "def residual_decoder_block(filter_num, kernel_size, bn_moment, first=False):\n",
        "  \"\"\"\n",
        "  A functional style residual connection deconvolutional block.\n",
        "\n",
        "  Args:\n",
        "  first - bool, determains if it is the first block of a network\n",
        "  \"\"\"\n",
        "  \n",
        "  def block(x, filter_num, kernel_size=(3,3), bn_moment=0.9):\n",
        "    \n",
        "    # first layer\n",
        "    if first:\n",
        "      dc1 = Dense(4*4*filter_num, activation='linear')(x) # num neurons is dependent upon the number of blocks\n",
        "      dc1 = BatchNormalization(momentum=bn_moment)(dc1)\n",
        "      dc1 = Reshape(target_shape=(4,4,filter_num))(dc1) # reshaping is dependent upon the number of blocks\n",
        "    \n",
        "    else:\n",
        "      dc1 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(x)\n",
        "      dc1 = BatchNormalization(momentum=bn_moment)(dc1)\n",
        "\n",
        "    # second layer\n",
        "    dc2 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(dc1)\n",
        "    dc2 = BatchNormalization(momentum=bn_moment)(dc2)\n",
        "\n",
        "    # third layer\n",
        "    dc3 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(dc2)\n",
        "    dc3 = BatchNormalization(momentum=bn_moment)(dc3)\n",
        "\n",
        "    # residual connection\n",
        "    res = dc1 + dc3\n",
        "\n",
        "    return res\n",
        "\n",
        "  return lambda x: block(x, filter_num, kernel_size, bn_moment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU9XOnUA99dD",
        "colab_type": "text"
      },
      "source": [
        "## Main architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISOAGouf9ym1",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def get_siamese_networks_model(input_shape):\n",
        "  \"\"\"\n",
        "  Creates siamese networks model. ref paper: https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf\n",
        "  \"\"\"\n",
        "\n",
        "  # define input vectors\n",
        "  input1 = Input(input_shape, name='input1')\n",
        "  input2 = Input(input_shape, name='input2')\n",
        "\n",
        "  def get_encoder():\n",
        "    \"\"\"\n",
        "    A helper function for sharing weights between inputs\n",
        "    \"\"\"\n",
        "\n",
        "    x = Input(input_shape, name='x')  \n",
        "\n",
        "    # first block\n",
        "    block1 = residual_encoder_block(16, kernel_size=(3,3), bn_moment=0.9)(x)\n",
        "    mp1 = MaxPool2D(pool_size=(2,2), name='max_pool1')(block1)\n",
        "\n",
        "    # second block\n",
        "    block2 = residual_encoder_block(32, kernel_size=(3,3), bn_moment=0.9)(mp1)\n",
        "    mp2 = MaxPool2D(pool_size=(2,2), name='max_pool2')(block2)\n",
        "\n",
        "    # third block\n",
        "    block3 = residual_encoder_block(64, kernel_size=(3,3), bn_moment=0.9)(mp2)\n",
        "    mp3 = MaxPool2D(pool_size=(2,2), name='max_pool3')(block3)\n",
        "\n",
        "    # flatten\n",
        "    flat = Flatten()(mp3)\n",
        "\n",
        "    # dense\n",
        "    dense = Dense(DENSE_REP_SIZE, activation='sigmoid', name='dense_rep')(flat)\n",
        "    bn_dense = BatchNormalization(momentum=0.9)(dense)\n",
        "\n",
        "    return tf.keras.Model(inputs=x, outputs=bn_dense)\n",
        "\n",
        "  # get shared encoder\n",
        "  encoder = get_encoder()\n",
        "  \n",
        "  # get feature vectors\n",
        "  v1 = encoder(input1)\n",
        "  v2 = encoder(input2)\n",
        "\n",
        "  # compute L1 loss\n",
        "  L1_Layer = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))\n",
        "  L1_diff = L1_Layer([v1, v2])\n",
        "\n",
        "  # compute probs\n",
        "  probs = Dense(5, activation='softmax')(L1_diff)\n",
        "\n",
        "  siamese_net = tf.keras.Model(inputs=[input1, input2], outputs=probs)\n",
        "\n",
        "  # compile\n",
        "  siamese_net.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
        "  return siamese_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEKcGPtKgbM6",
        "colab_type": "text"
      },
      "source": [
        "## Training siamese networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0kKJwuy2t2R",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "sn = get_siamese_networks_model([*BOARD_SIZE, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfvNty6FbxNA",
        "colab_type": "code",
        "outputId": "e11c5e79-e089-4ade-834b-a119941eafc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "source": [
        "hist = sn.fit(x=x, y=y, epochs=30, batch_size=SN_BATCH_SIZE, shuffle=True, validation_split=0.2)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "2343/2343 [==============================] - 47s 20ms/step - loss: 0.0512 - categorical_accuracy: 0.9843 - val_loss: 0.6027 - val_categorical_accuracy: 0.8662\n",
            "Epoch 2/30\n",
            "2343/2343 [==============================] - 47s 20ms/step - loss: 0.0372 - categorical_accuracy: 0.9885 - val_loss: 0.8198 - val_categorical_accuracy: 0.8888\n",
            "Epoch 3/30\n",
            "2343/2343 [==============================] - 47s 20ms/step - loss: 0.0361 - categorical_accuracy: 0.9889 - val_loss: 0.8168 - val_categorical_accuracy: 0.8938\n",
            "Epoch 4/30\n",
            "2343/2343 [==============================] - 47s 20ms/step - loss: 0.0311 - categorical_accuracy: 0.9901 - val_loss: 0.5342 - val_categorical_accuracy: 0.8938\n",
            "Epoch 5/30\n",
            "2343/2343 [==============================] - 48s 21ms/step - loss: 0.0271 - categorical_accuracy: 0.9911 - val_loss: 0.4717 - val_categorical_accuracy: 0.8829\n",
            "Epoch 6/30\n",
            "2343/2343 [==============================] - 47s 20ms/step - loss: 0.0241 - categorical_accuracy: 0.9923 - val_loss: 0.4898 - val_categorical_accuracy: 0.9126\n",
            "Epoch 7/30\n",
            "2343/2343 [==============================] - 46s 20ms/step - loss: 0.0220 - categorical_accuracy: 0.9926 - val_loss: 0.4632 - val_categorical_accuracy: 0.8897\n",
            "Epoch 8/30\n",
            " 445/2343 [====>.........................] - ETA: 34s - loss: 0.0159 - categorical_accuracy: 0.9943"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-3f7152cfe031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSN_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    850\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDuSUF8lRP0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_training_curves(hist.history, metric='categorical_accuracy', with_val=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkiOZvd7gOkP",
        "colab_type": "text"
      },
      "source": [
        "# Board generator (decoder) architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ir0OqLyUPa_",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def get_encoder_from_siamese(siamese):\n",
        "  \"\"\"\n",
        "  Creates a board encoder from the siamese networks model. Maps board --> feature vector of dim 512\n",
        "  \"\"\"\n",
        "\n",
        "  # create encoder from all layers up to dense\n",
        "  model = tf.keras.models.Sequential([\n",
        "                                      tf.keras.layers.InputLayer(input_shape=[*BOARD_SIZE, 1]),\n",
        "                                      siamese.layers[2] # all sequential layers from siamese networks model\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "def get_decoder(siamese):\n",
        "  \"\"\"\n",
        "  Creates a decoder from siamese netwok model. Maps feature vector of dim 512 --> board. \n",
        "  Same architecture from encoder is preserved\n",
        "  \"\"\"\n",
        "\n",
        "  # build model\n",
        "  inp = Input([DENSE_REP_SIZE])\n",
        "\n",
        "  # first_block\n",
        "  block1 = residual_decoder_block(64, kernel_size=(3,3), bn_moment=0.9, first=True)(inp)\n",
        "  us1 = UpSampling2D(size=(2,2))(block1)\n",
        "\n",
        "  block2 = residual_decoder_block(32, kernel_size=(3,3), bn_moment=0.9)(us1)\n",
        "  us2 = UpSampling2D(size=(2,2))(block2)\n",
        "\n",
        "  block3 = residual_decoder_block(16, kernel_size=(3,3), bn_moment=0.9)(us2)\n",
        "\n",
        "  output = Conv2D(1, kernel_size=(1,1), activation='sigmoid')(block3)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inp, outputs=output)\n",
        "\n",
        "  # compile\n",
        "  model.compile(loss=pixelwise_error_loss, optimizer='adam', metrics=[pixelwise_auc])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def pixelwise_error_loss(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Custom loss function. Pixelwise error from the true image. ||y_true - y_pred||\n",
        "  \"\"\"\n",
        "\n",
        "  return tf.keras.backend.sum((y_true - y_pred) ** 2) / y_pred.shape[0]\n",
        "\n",
        "\n",
        "def pixelwise_accuracy(y_true, y_pred, threshold=0.8):\n",
        "  \"\"\"\n",
        "  Custom accuracy function. Pixelwise accuracy of predicting 0 / 1\n",
        "  \"\"\"\n",
        "  \n",
        "  tp = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred >= threshold), 'float32'), y_true)) # cast boolean to binary\n",
        "  tn = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred < threshold), 'float32'), tf.cast((y_true == 0), 'float32')))\n",
        "  n_predictions = BOARD_SIZE[0] * BOARD_SIZE[1]\n",
        "\n",
        "  return ((tp + tn) / n_predictions) / y_pred.shape[0]\n",
        "\n",
        "\n",
        "def pixelwise_sensitivity(y_true, y_pred, threshold=0.8):\n",
        "  \"\"\"\n",
        "  Pixelwise sensitivity of correctly predicting 1s. true positives / positives\n",
        "  \"\"\"\n",
        "\n",
        "  tp = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred >= threshold), 'float32'), y_true)) \n",
        "  p = tf.cast(tf.math.reduce_sum(y_true), 'float32') \n",
        "  \n",
        "  return tf.cond(p > 0, lambda: tp / p, lambda: tf.constant(1, dtype='float32')) # don't divide by zero\n",
        "\n",
        "def pixelwise_fpr(y_true, y_pred, threshold=0.8):\n",
        "  \"\"\"\n",
        "  Pixelwise false positive rate of wrongly predicting 1s. false positives / negatives\n",
        "  \"\"\"\n",
        "\n",
        "  fp = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred >= threshold), 'float32'), tf.cast((y_true==0), 'float32'))) # cast boolean to binary\n",
        "  n = tf.cast(tf.math.reduce_sum(tf.cast((y_true == 0), 'float32')), 'float32')\n",
        "\n",
        "  return tf.cond(n > 0, lambda: fp / n, lambda: tf.constant(1, dtype='float32')) # don't divide by zero\n",
        "\n",
        "\n",
        "def pixelwise_auc(y_true, y_pred, num_thresholds=10):\n",
        "  \"\"\"\n",
        "  Pixelwise auc score. \n",
        "  \"\"\"\n",
        "\n",
        "  sensitivity = []\n",
        "  fpr = []\n",
        "\n",
        "  # compute graph points\n",
        "  for threshold in np.linspace(1, 0, num=num_thresholds):\n",
        "    sensitivity.append(pixelwise_sensitivity(y_true, y_pred, threshold=threshold))\n",
        "    fpr.append(pixelwise_fpr(y_true, y_pred, threshold=threshold))\n",
        "  \n",
        "  # compute trapeze area\n",
        "  trapeze = []\n",
        "  for i in range(len(sensitivity)- 1):\n",
        "    area = tf.multiply(sensitivity[i], fpr[i+1] - fpr[i])\n",
        "    trapeze.append(area)\n",
        "  \n",
        "  auc_score = tf.add_n(trapeze) \n",
        "\n",
        "  return auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNIBD7hmoL-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dec = get_decoder(sn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uBo_DSMrXDZ",
        "colab_type": "text"
      },
      "source": [
        "## Decoder dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmNwACcSraxu",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def get_decoder_dataset(siamese, encoder_dataset):\n",
        "  \"\"\"\n",
        "  Creates the decoder dataset from the encoder.\n",
        "  \n",
        "  Args:\n",
        "  siamese - siamese model to extract encoder from\n",
        "  encoder dataset - boards to encoder to feature vectors\n",
        "\n",
        "  Returns:\n",
        "  x - boards feature vectors, predicted by encoder\n",
        "  y - boards \n",
        "  \"\"\"\n",
        "  \n",
        "  enc = get_encoder_from_siamese(siamese) # fetch encoder\n",
        "\n",
        "  # build dataset\n",
        "  x = enc.predict(encoder_dataset) # get feature vectors\n",
        "  y = encoder_dataset # boards to recreate\n",
        "\n",
        "  return x,y\n",
        "\n",
        "# create encoder dataset from the old pair and label lists\n",
        "pair_list, label_list = get_siamese_dataset(training_tasks, eval_tasks, test_tasks, get_lists=True)\n",
        "encoder_dataset = get_dataset_from_lists(pair_list, label_list, for_encoder=True) \n",
        "# create decoder dataset from encoder\n",
        "decoder_x, decoder_y = get_decoder_dataset(sn, encoder_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg9iv1vPtEEe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "88979f6c-86fc-46b6-8dc7-2ba178e36823"
      },
      "source": [
        "hist_decoder = dec.fit(x=decoder_x, y=decoder_y, batch_size=DECODER_BATCH_SIZE, shuffle=True, epochs=1, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1918/18744 [==>...........................] - ETA: 2:30 - loss: 17.4123 - pixelwise_auc: 0.8597"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pHSQ3pYcTII",
        "colab_type": "text"
      },
      "source": [
        "# Check regular encoder-decoder architecture\n",
        "## For checking some hyperparameters and deciding which adecoder architecture to use\n",
        "\n",
        "## Some takeaways:\n",
        "- adding two sets of randomly padded data helps alot. more doesnt help\n",
        "- small decoder batch size (4-16) really helps\n",
        "- adding more neurons to the compressed represantation (512 --> 1024) really helps\n",
        "- removing the last layers batch normalization really helps\n",
        " - removing bn before compressed representation isnt good\n",
        "- penelizing mistaking 1's for 0's more doesnt help at all\n",
        "- changing last layer activation to linear and clipping to [0,1] helps to some extent\n",
        "- chagning middle to relu - really bad!!!\n",
        "- addind another conv-deconv layer at the beggining helps a little\n",
        "- adding another conv-deconv layer at the end doesnt help at all\n",
        "- residual layers - really cool! used the architecture from this papare -- >https://github.com/omerhac/arc_challenge/blob/master/deep%20residual%20conv-deconv%20network.pdf\n",
        "- more types of data augmentation....\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzGZn1y0vt54",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRvqZ5OczS9J",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title encoder-decoder baseline\n",
        "def get_encoder_decoder():\n",
        "  \"\"\"\n",
        "  Create an encoder decoer \"normal\" architecture, with residual connections\n",
        "  \"\"\"\n",
        "  inp = tf.keras.layers.Input([*BOARD_SIZE, 1])\n",
        "  \n",
        "  # encoder\n",
        "  block1 = residual_encoder_block(16, kernel_size=(3,3), bn_moment=0.9)(inp)\n",
        "  mp1 = MaxPool2D(pool_size=(2,2))(block1)\n",
        "\n",
        "  block2 = residual_encoder_block(32, kernel_size=(3,3), bn_moment=0.9)(mp1)\n",
        "  mp2 = MaxPool2D(pool_size=(2,2))(block2)\n",
        "\n",
        "  block3 = residual_encoder_block(64, kernel_size=(3,3), bn_moment=(0.9))(mp2)\n",
        "\n",
        "  # flatten\n",
        "  flat = Flatten()(block3)\n",
        "\n",
        "  # dense representation\n",
        "  dense = Dense(1024, activation='sigmoid')(flat)\n",
        "  dense = BatchNormalization(momentum=0.9)(dense)\n",
        "\n",
        "\n",
        "  # decoder \n",
        "\n",
        "  # first_block\n",
        "  block1 = residual_decoder_block(64, kernel_size=(3,3), bn_moment=0.9, first=True)(dense)\n",
        "  us1 = UpSampling2D(size=(2,2))(block1)\n",
        "\n",
        "  block2 = residual_decoder_block(32, kernel_size=(3,3), bn_moment=0.9)(us1)\n",
        "  us2 = UpSampling2D(size=(2,2))(block2)\n",
        "\n",
        "  block3 = residual_decoder_block(16, kernel_size=(3,3), bn_moment=0.9)(us2)\n",
        "\n",
        "  output = Conv2D(1, kernel_size=(1,1), activation='sigmoid')(block3)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inp, outputs=output)\n",
        "  \n",
        "\n",
        "  # compile\n",
        "  model.compile(loss=pixelwise_error_loss, optimizer='adam', metrics=[pixelwise_auc])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mharVxoMMqJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ed = get_encoder_decoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb0_gTC8nIdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ed_hist = ed.fit(x=decoder_y, y=decoder_y, batch_size=8, shuffle=True, epochs=20, validation_split=0.2) # get image --> predict image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9ii2z4hh9de",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## check predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eytnCHw0iUIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boards = decoder_y[-10:]\n",
        "\n",
        "# predict\n",
        "predictions = ed.predict(boards)\n",
        "\n",
        "# reshape\n",
        "boards = [plotting_shape_board(board) for board in boards]\n",
        "predictions = [plotting_shape_board(board) for board in predictions]\n",
        "pairs = zip(boards, predictions)\n",
        "\n",
        "plot_decoder_boards(list(pairs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kujm22z3y0qa",
        "colab_type": "text"
      },
      "source": [
        "# Interleaved training (decoder/encoder/deocoder..)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5XjrxlkyZOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def copy_decoder_to_siamese(siamese, decoder):\n",
        "  \"\"\"\n",
        "  Copys the weights from the decoder to the siamese networks model\n",
        "  \"\"\"\n",
        "\n",
        "  decoder_layers = decoder.layers # get decoder layers\n",
        "  siamese_layers = siamese.layers[2].layers # siamese encoder layers\n",
        "\n",
        "  for i, layer in enumerate(decoder_layers[::-1]):\n",
        "      if(2 >= len(layer.weights) > 0): # trainable layer and not BN layer\n",
        "        if layer.weights[0].shape[0] < 100: # deconv layers\n",
        "          decoder_layer_weigths = layer.get_weights() # get weights from decoder\n",
        "          bias = np.zeros(siamese_layers[i].get_weights()[1].shape) # init new bias\n",
        "          w = decoder_layer_weigths[0].transpose([1,0,2,3]) # transpose weights\n",
        "          siamese_layers[i].set_weights([w, bias]) # set siamese weights\n",
        "\n",
        "        else:\n",
        "          # dense layer\n",
        "          decoder_layer_weigths = layer.get_weights() # get weights from decoder\n",
        "          bias = np.zeros(siamese_layers[i].get_weights()[1].shape) # init new bias\n",
        "          w = decoder_layer_weigths[0].transpose() # transpose weights\n",
        "          siamese_layers[i].set_weights([w, bias]) # set siamese weights\n",
        "\n",
        "\n",
        "def copy_siamese_to_decoder(siamese, decoder):\n",
        "  \"\"\"\n",
        "  Copys the weights from the siamese networks model to the decoder\n",
        "  \"\"\"\n",
        "\n",
        "  decoder_layers = decoder.layers # get decoder layers\n",
        "  siamese_layers = siamese.layers[2].layers # siamese encoder layers\n",
        "\n",
        "  for i, layer in enumerate(decoder_layers[::-1]):\n",
        "      if(2 >= len(layer.weights) > 0): # trainable layer and not BN layer\n",
        "        if layer.weights[0].shape[0] < 100: # deconv layers\n",
        "          siamese_layer_weigths = siamese_layers[i].get_weights() # get weights from siamese encoder\n",
        "          bias = np.zeros(layer.get_weights()[1].shape) # init new bias\n",
        "          w = siamese_layer_weigths[0].transpose([1,0,2,3]) # transpose weights\n",
        "          layer.set_weights([w, bias]) # set decoder weights\n",
        "\n",
        "        else:\n",
        "          # dense layer\n",
        "          siamese_layer_weigths = siamese_layers[i].get_weights() # get weights from siamese encoder\n",
        "          bias = np.zeros(layer.get_weights()[1].shape) # init new bias\n",
        "          w = siamese_layer_weigths[0].transpose() # transpose weights\n",
        "          layer.set_weights([w, bias]) # set decoder weights\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2ucFsFkzIkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create new models with same weights\n",
        "EPOCHS = 30\n",
        "sn = get_siamese_networks_model([*BOARD_SIZE, 1])\n",
        "decoder = get_decoder(sn, copy_encoder_weights=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pfDn78N4ecn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "SN_BATCH_SIZE = 64\n",
        "DEC_BATCH_SIZE = 4\n",
        "DATA_SPLIT = 5\n",
        "SN_STEPS_PER_EPOCH = len(x) // SN_BATCH_SIZE\n",
        "DEC_STEPS_PER_EPOCH = len(decoder_x) // DEC_BATCH_SIZE\n",
        "\n",
        "# even fancier training loop\n",
        "for i in range(EPOCHS):\n",
        "  print(\"training... epoch num: {}\".format(i))\n",
        "\n",
        "  for i in range(40):\n",
        "    # train siamese\n",
        "    _ = sn.fit(x=x, y=y, epochs=1, batch_size=SN_BATCH_SIZE, shuffle=True, steps_per_epoch=(780//DATA_SPLIT))\n",
        "    \n",
        "    # copy weights\n",
        "    copy_siamese_to_decoder(sn, decoder)\n",
        "\n",
        "    # get new decoder dataset\n",
        "    decoder_x, decoder_y = get_decoder_dataset(sn, encoder_dataset) # encoder dataset is the same as before\n",
        "\n",
        "    # train decoder\n",
        "    _ = decoder.fit(x=decoder_x, y=decoder_y, epochs=1, batch_size=DEC_BATCH_SIZE, shuffle=True, steps_per_epoch=(24000//DATA_SPLIT)) #### WOW!!! use smaller batch size, WOHOOO!!\n",
        "\n",
        "    # copy weights\n",
        "    copy_decoder_to_siamese(sn, decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BefQARZeJ6OK",
        "colab_type": "text"
      },
      "source": [
        "# Predicting from diffrance in boards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Api-_erL_rs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task_training_input, task_training_output, task_test_input, task_test_output = get_task_boards(training_tasks[0], pad=pad, divide_sets=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmT5I2c2KNS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# toy dataset (x is the dataset for the siamese networks)\n",
        "anchor = x[0][0]\n",
        "rotate_once_anchor = x[1][1]\n",
        "rotate_twice_anchor = x[1][2]\n",
        "\n",
        "toy = np.stack([anchor, rotate_once_anchor, rotate_twice_anchor])\n",
        "\n",
        "# plot\n",
        "fig, axs = plt.subplots(1,3)\n",
        "plot_board(plotting_shape_board(anchor), axs[0])\n",
        "plot_board(plotting_shape_board(rotate_once_anchor), axs[1])\n",
        "plot_board(plotting_shape_board(rotate_twice_anchor), axs[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVmhKDCOOEYJ",
        "colab_type": "text"
      },
      "source": [
        "## define predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uVadzm3Lw3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictor(decoder):\n",
        "  \"\"\"\n",
        "  Builds a detector which takes a board and a rules vector and predicts output board.\n",
        "\n",
        "  Args:\n",
        "  decoder --> trained decoder \n",
        "  \"\"\"\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}