{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arc.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omerhac/arc_challenge/blob/master/arc_advanced_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YShrwCd2GUWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import json\n",
        "from google.cloud import storage\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import colors\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Conv2D, Lambda, Dense, Flatten, MaxPool2D, Input, BatchNormalization, Conv2DTranspose, UpSampling2D, Reshape\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uko3X-E_CrZr",
        "colab_type": "code",
        "outputId": "2ac6512b-6fd9-4a09-cdf3-2e4b9e32abf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# get repository from github\n",
        "!git clone https://github.com/omerhac/arc_challenge.git"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'arc_challenge'...\n",
            "remote: Enumerating objects: 140, done.\u001b[K\n",
            "remote: Counting objects: 100% (140/140), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 140 (delta 59), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (140/140), 10.51 MiB | 12.79 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcDHkw11CtSu",
        "colab_type": "code",
        "outputId": "02b27d7f-f02c-465e-c07c-b02c9e6947a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# navigate to directory\n",
        "cd arc_challenge"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/arc_challenge/arc_challenge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX5E2ZNODM5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load repository dependencies\n",
        "import preprocess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF4Ovm97vXEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## constants ##\n",
        "BOARD_SIZE = (16,16) # board upperbound size\n",
        "SN_BATCH_SIZE = 64\n",
        "DECODER_BATCH_SIZE = 8\n",
        "DENSE_REP_SIZE = 1024 # dense vector represantation size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqOKR-POOXki",
        "colab_type": "text"
      },
      "source": [
        "# Load data / util funcitons\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-96BlT_hHVkF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title load data\n",
        "def load_data():\n",
        "  \"\"\"\n",
        "  Loads all the data into training_tasks, eval_tasks and test_tasks\n",
        "  \"\"\"\n",
        "\n",
        "  ## get paths\n",
        "  GCS_PATH = \"gs://kds-d3cfb3d523ca35d2517017a78110126404d01fdea69417ce49950459\"\n",
        "  training_filenames = tf.io.gfile.glob(GCS_PATH + \"/training/*\")\n",
        "  test_filenames = tf.io.gfile.glob(GCS_PATH + \"/test/*\")\n",
        "  eval_filenames = tf.io.gfile.glob(GCS_PATH + \"/evaluation/*\")\n",
        "\n",
        "  # create datasets with filenames\n",
        "  training_dataset = tf.data.Dataset.list_files(training_filenames)\n",
        "  eval_dataset = tf.data.Dataset.list_files(eval_filenames)\n",
        "  test_dataset = tf.data.Dataset.list_files(test_filenames)\n",
        "\n",
        "  # load the jsons\n",
        "  def load_task(filename):\n",
        "    task_json = tf.io.read_file(filename)\n",
        "    return task_json\n",
        "\n",
        "  training_dataset = training_dataset.map(load_task)\n",
        "  eval_dataset = eval_dataset.map(load_task)\n",
        "  test_dataset = test_dataset.map(load_task)\n",
        "\n",
        "  training_dataset_numpy = tf.data.Dataset.as_numpy_iterator(training_dataset) # convert to numpy iterator\n",
        "  eval_dataset_numpy = tf.data.Dataset.as_numpy_iterator(eval_dataset)\n",
        "  test_dataset_numpy = tf.data.Dataset.as_numpy_iterator(test_dataset)\n",
        "\n",
        "  ## create a numpy array of tasks (n_tasks, )\n",
        "  def list_from_jsons(jsons_numpy_iterator):\n",
        "    \"\"\"\n",
        "      Create a list of task dictionaries from jsons numpy interator\n",
        "    \"\"\"\n",
        "\n",
        "    tasks = []\n",
        "    for task in jsons_numpy_iterator:\n",
        "      tasks.append(json.loads(task))\n",
        "\n",
        "    return tasks\n",
        "\n",
        "  ## get numpy arrays of datasets\n",
        "  training_tasks = list_from_jsons(training_dataset_numpy)\n",
        "  eval_tasks = list_from_jsons(eval_dataset_numpy)\n",
        "  test_tasks = list_from_jsons(test_dataset_numpy)\n",
        "\n",
        "  return training_tasks, eval_tasks, test_tasks\n",
        "\n",
        "\n",
        "def load_data_from_jsons():\n",
        "  \"\"\"\n",
        "  Load tasks from jsons to lists of tasks.\n",
        "\n",
        "  Returns:\n",
        "  training_tasks, eval_tasks, test_tasks --> lists of tasks\n",
        "  \"\"\"\n",
        "\n",
        "  with open(\"training_tasks.json\", 'r') as f:\n",
        "    training_tasks = json.load(f)\n",
        "\n",
        "  with open(\"eval_tasks.json\", 'r') as f:\n",
        "    eval_tasks = json.load(f)\n",
        "  \n",
        "  with open(\"test_tasks.json\", 'r') as f:\n",
        "    test_tasks = json.load(f)\n",
        "\n",
        "  return training_tasks, eval_tasks, test_tasks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oOyaYHAJr1A",
        "colab_type": "code",
        "outputId": "571e31f8-7ad6-4657-d3b5-ae1d74736c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "%%time\n",
        "training_tasks, eval_tasks, test_tasks = load_data_from_jsons()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 529 ms, sys: 39.3 ms, total: 569 ms\n",
            "Wall time: 571 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYLEt0yrPXf9",
        "colab_type": "text"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO9FwJjFNAnO",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Utilities\n",
        "def plot_board(board, ax, title=\"\"):\n",
        "  \"\"\"\n",
        "  Plot a board on a given axis\n",
        "  \"\"\"\n",
        "  cmap = colors.ListedColormap(\n",
        "      ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
        "        '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
        "  norm = colors.Normalize(vmin=0, vmax=9)\n",
        "  \n",
        "  ax.imshow(board, cmap=cmap, norm=norm)\n",
        "  ax.grid(True,which='both',color='lightgrey', linewidth=0.5)    \n",
        "  ax.set_yticks([x-0.5 for x in range(1+board.shape[0])])\n",
        "  ax.set_xticks([x-0.5 for x in range(1+board.shape[1])])     \n",
        "  ax.set_xticklabels([])\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_title(title)\n",
        "\n",
        "def plot_one(task, ax, i,train_or_test,input_or_output):\n",
        "  \"\"\"\n",
        "  Plot one task on a given axis\n",
        "  \"\"\"\n",
        "  cmap = colors.ListedColormap(\n",
        "      ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
        "        '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
        "  norm = colors.Normalize(vmin=0, vmax=9)\n",
        "  \n",
        "  input_matrix = task[train_or_test][i][input_or_output]\n",
        "  ax.imshow(input_matrix, cmap=cmap, norm=norm)\n",
        "  ax.grid(True,which='both',color='lightgrey', linewidth=0.5)    \n",
        "  ax.set_yticks([x-0.5 for x in range(1+len(input_matrix))])\n",
        "  ax.set_xticks([x-0.5 for x in range(1+len(input_matrix[0]))])     \n",
        "  ax.set_xticklabels([])\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_title(train_or_test + ' '+input_or_output)\n",
        "    \n",
        "\n",
        "def plot_task(task):\n",
        "    \"\"\"\n",
        "    Plots the first train and test pairs of a specified task,\n",
        "    using same color scheme as the ARC app\n",
        "    \"\"\"    \n",
        "    num_train = len(task['train'])\n",
        "    fig, axs = plt.subplots(2, num_train, figsize=(3*num_train,3*2))\n",
        "    for i in range(num_train):     \n",
        "        plot_one(task, axs[0,i],i,'train','input')\n",
        "        plot_one(task, axs[1,i],i,'train','output')        \n",
        "    plt.tight_layout()\n",
        "    plt.show()        \n",
        "        \n",
        "    num_test = len(task['test'])\n",
        "    fig, axs = plt.subplots(2, num_test, figsize=(3*num_test,3*2))\n",
        "    if num_test==1: \n",
        "        plot_one(task, axs[0],0,'test','input')\n",
        "        plot_one(task, axs[1],0,'test','output')     \n",
        "    else:\n",
        "        for i in range(num_test):      \n",
        "            plot_one(task, axs[0,i],i,'test','input')\n",
        "            plot_one(task, axs[1,i],i,'test','output')  \n",
        "    plt.tight_layout()\n",
        "    plt.show() \n",
        "  \n",
        "\n",
        "def plot_board_pairs(board_pairs, labels):\n",
        "  \"\"\"\n",
        "  Plots the board pairs (for siamese networks) with their label as a title\n",
        "  \"\"\"\n",
        "\n",
        "  fig, axs = plt.subplots(len(board_pairs), 2, figsize=(8, 3 * len(board_pairs)))\n",
        "  \n",
        "  for i, pair in enumerate(board_pairs):\n",
        "    # plot a pair on a given axis\n",
        "    plot_board(pair[0], axs[i, 0], title=\"anchor\") \n",
        "    plot_board(pair[1], axs[i, 1], title=labels[i])\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_decoder_boards(board_pairs):\n",
        "  \"\"\"\n",
        "  Plots the board pairs (for siamese networks) with their label as a title\n",
        "  \"\"\"\n",
        "\n",
        "  fig, axs = plt.subplots(len(board_pairs), 2, figsize=(8, 3 * len(board_pairs)))\n",
        "\n",
        "  for i, pair in enumerate(board_pairs):\n",
        "    # plot a pair on a given axis\n",
        "    plot_board(pair[0], axs[i, 0]) \n",
        "    plot_board(pair[1], axs[i, 1])\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def display_training_curves(hist, metric='accuracy', with_val=False):\n",
        "  \"\"\"display learning curves for keras history dict, args: history dict, with val --> boolean with/without val\"\"\"\n",
        "  plt.figure(figsize=(18,6))\n",
        "\n",
        "  # accuracy plots\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(hist[metric])\n",
        "  \n",
        "  if with_val:\n",
        "    plt.plot(hist['val_' + metric])\n",
        "    plt.legend(['Train', 'Validation'])\n",
        "  \n",
        "  else:\n",
        "    plt.legend(['Train'])\n",
        "  \n",
        "  plt.title('Model accuracy')\n",
        "  plt.xlabel('EPOCH')\n",
        "  plt.ylabel('Accuracy')\n",
        "\n",
        "  # loss plots\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(hist['loss'])\n",
        "\n",
        "  if with_val:\n",
        "    plt.plot(hist['val_loss'])\n",
        "    plt.legend(['Train loss', 'Val loss'])\n",
        "  \n",
        "  else:\n",
        "    plt.legend(['Train loss'])\n",
        "  \n",
        "  plt.title('Model loss')\n",
        "  plt.xlabel('EPOCH')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9AnAL0VU2Tu",
        "colab_type": "text"
      },
      "source": [
        "# Data generating functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuTeaiVASRhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset_shapes(dataset):\n",
        "  \"\"\"\n",
        "  Returns dataset board shapes. \n",
        "  \"\"\"\n",
        "\n",
        "  shape_0 = []\n",
        "  shape_1 = []\n",
        "  \n",
        "  # check every task\n",
        "  for task in dataset:\n",
        "    boards = get_task_boards(task) # get all boards\n",
        "    shape_0 += [board.shape[0] for board in boards]\n",
        "    shape_1 += [board.shape[1] for board in boards]\n",
        "  \n",
        "  return shape_0, shape_1\n",
        "\n",
        "def get_task_boards(task, threshold_shape=(40, 40), pad=None, test=False, divide_sets=False):\n",
        "  \"\"\"\n",
        "  Get the training / testing boards of every example in a specific task. \n",
        "\n",
        "  Args: threshold_shape --> threshold shape for which \n",
        "  biggger samples won't be returned. \n",
        "  pad --> functional padding function to pad boards with (up tp threshold_size).\n",
        "  test --> bool, whether a test task or not. for output of task example.\n",
        "  divide_sets --> bool, whether to divide boards sets to training - input / output, test - input / output.\n",
        "  \"\"\"\n",
        "  \n",
        "  training_input_boards, training_output_boards, test_input_boards, test_output_boards = [], [], [], []\n",
        "\n",
        "  # train boards\n",
        "  for example in task['train']:\n",
        "    input_board = np.array(example['input'])\n",
        "    output_board = np.array(example['output'])\n",
        "\n",
        "    if((input_board.shape[0] < threshold_shape[0]) and (input_board.shape[1] < threshold_shape[1])):\n",
        "      training_input_boards.append(pad(input_board, output_shape=threshold_shape) if pad else input_board) # check for padding func\n",
        "\n",
        "    if((output_board.shape[0] < threshold_shape[0]) and (output_board.shape[1] < threshold_shape[1])):\n",
        "      training_output_boards.append(pad(output_board, output_shape=threshold_shape) if pad else output_board) # check for padding func\n",
        "\n",
        "  # test boards\n",
        "  for example in task['test']:\n",
        "    input_board = np.array(example['input'])\n",
        "\n",
        "    if not test: # check if test example\n",
        "      output_board = np.array(example['output'])\n",
        "    \n",
        "    # check whether the board is smaller then threshold shape\n",
        "    if((input_board.shape[0] < threshold_shape[0]) and (input_board.shape[1] < threshold_shape[1])):\n",
        "      test_input_boards.append(pad(input_board, output_shape=threshold_shape) if pad else input_board) # check for padding func\n",
        "\n",
        "    if((output_board.shape[0] < threshold_shape[0]) and (output_board.shape[1] < threshold_shape[1]) and (not test)):\n",
        "      test_output_boards.append(pad(output_board, output_shape=threshold_shape) if pad else output_board) # check for padding func\n",
        "  \n",
        "  # whether to divide boards:\n",
        "  if divide_sets:\n",
        "    return training_input_boards, training_output_boards, test_input_boards, test_output_boards\n",
        "  else:\n",
        "    return training_input_boards + training_output_boards + test_input_boards + test_output_boards\n",
        "\n",
        "def pad(mat, output_shape, padder=0):\n",
        "  \"\"\"\n",
        "  Pad a matrix with padder up to output_shape. Insert matrix at upper left corner.\n",
        "  \n",
        "  Args:\n",
        "  mat - np.array matrix of rank 2\n",
        "  output_shape - tuple \n",
        "  padder - int\n",
        "  \"\"\"\n",
        "\n",
        "  output_board = np.zeros(shape=output_shape) + padder # create output board and pad it\n",
        "\n",
        "  # get input shape\n",
        "  input_rows = mat.shape[0]\n",
        "  input_cols = mat.shape[1]\n",
        "\n",
        "  # if random=False, insert input matrix in upper left corner\n",
        "  output_board[:input_rows, :input_cols] = mat\n",
        "  return output_board\n",
        "\n",
        "def random_pad(mat, output_shape, padder=0):\n",
        "  \"\"\"\n",
        "  Pad a matrix with padder up to output_shape. Insert the matrix at a random location\n",
        "  \n",
        "  Args:\n",
        "  mat - np.array matrix of rank 2\n",
        "  output_shape - tuple \n",
        "  padder - int\n",
        "  seed - int\n",
        "  \"\"\"\n",
        "\n",
        "  output_board = np.zeros(shape=output_shape) + padder # create output board and pad it\n",
        "\n",
        "  # get input shape\n",
        "  input_rows = mat.shape[0]\n",
        "  input_cols = mat.shape[1]\n",
        "\n",
        "  # insert mat at a random loacation\n",
        "  # get random location\n",
        "  start_row = np.random.randint(output_shape[0] - input_rows)\n",
        "  start_col = np.random.randint(output_shape[1] - input_cols)\n",
        "  # insert\n",
        "  output_board[start_row:start_row+input_rows, start_col:start_col+input_cols] = mat\n",
        "\n",
        "  return output_board\n",
        "\n",
        "def get_all_boards(training_tasks, eval_tasks, test_tasks):\n",
        "  \"\"\"\n",
        "  Extracts all the boards from all training/eval/testing tasks. \n",
        "  \"\"\"\n",
        "\n",
        "  training_boards = []\n",
        "  for task in training_tasks:\n",
        "    training_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=pad)\n",
        "\n",
        "    # augment training set (random pad)\n",
        "    training_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "    training_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "\n",
        "  eval_boards = []\n",
        "  for task in eval_tasks:\n",
        "    eval_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=pad)\n",
        "\n",
        "    # augment eval set (random pad)\n",
        "    eval_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "    eval_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "    \n",
        "\n",
        "  test_boards = []\n",
        "  for task in test_tasks:\n",
        "    test_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=pad, test=True)\n",
        "\n",
        "    # augment test set (random pad)\n",
        "    test_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad, test=True)\n",
        "    test_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad, test=True)\n",
        "\n",
        "  all_boards = training_boards+eval_boards+test_boards\n",
        "  return all_boards\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLXZsdbYodKl",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oofkpiCmt8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_rotated_views(board):\n",
        "  \"\"\"\n",
        "  Turns a board 90 deg counter clockwise 3 times. Returns a list length 4 with all the rotated views including the original one.\n",
        "  \"\"\"\n",
        "\n",
        "  rotates = [board]\n",
        "  for i in range(3):\n",
        "    board = np.rot90(board)\n",
        "    rotates.append(board)\n",
        "  \n",
        "  return rotates\n",
        "\n",
        "def get_rotated_data_pairs(rotated_views):\n",
        "  \"\"\"\n",
        "  Creates all the possible pairs out of the rotated views tensor and labels them. If a1..a4 are rotated vies (A.C.W) of a1, returns labels according to:\n",
        "   - (a1, a1) = 1 --> same board\n",
        "   - (a1, a2) = 2 --> 90 a.c.w rotate\n",
        "   - (a1, a3) = 3 --> 180 rotate\n",
        "   - (a1, a4) = 4 --> 270 a.c.w rotate\n",
        "   - ...\n",
        "   - (a3, a4) = 2 --> 90 a.c.w rotate\n",
        "\n",
        "  Args:\n",
        "  rotated_views = a list of 4 rotated views of the board\n",
        "\n",
        "  Returns:\n",
        "  pairs: a list of tuples of boards\n",
        "  labels: a list of labels matching each pair\n",
        "  \"\"\"\n",
        "\n",
        "  # stopping rule\n",
        "  if rotated_views == []:\n",
        "    return [], []\n",
        "\n",
        "  anchor = rotated_views[0]  # select board to compare with \n",
        "  pairs, labels = [], []\n",
        "  label = 1 # init label\n",
        "\n",
        "  # iterate over all remaining examples\n",
        "  for view in rotated_views:\n",
        "    pairs.append((anchor, view))\n",
        "    labels.append(label)\n",
        "    label += 1 # update label, views rotate a.c.w\n",
        "  \n",
        "  next_pairs, next_labels = get_rotated_data_pairs(rotated_views[1:]) # recursive call\n",
        "\n",
        "  return pairs + next_pairs, labels + next_labels\n",
        "\n",
        "def get_binary_board(board):\n",
        "  \"\"\"\n",
        "  Returns a binary board. Every non 0 value becomes 1. 0 stays 0.\n",
        "  \"\"\"\n",
        "\n",
        "  return (board != 0).astype('int32')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnKVXvD8TmZu",
        "colab_type": "text"
      },
      "source": [
        "## Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibYTel0ORlJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_shape_board(board):\n",
        "  \"\"\"\n",
        "  Reshapes board for moedl digestion. shape [*BOARD_SIZE, 1]\n",
        "  \"\"\"\n",
        "  return board.reshape([*BOARD_SIZE, 1])\n",
        "\n",
        "def plotting_shape_board(board):\n",
        "  \"\"\"\n",
        "  Reshapes the board for plotting. shape [*BOARD_SIZE]\n",
        "  \"\"\"\n",
        "  return board.reshape([*BOARD_SIZE])\n",
        "\n",
        "def get_all_pairs_reshaped(board, all_boards):\n",
        "  \"\"\"\n",
        "  Creates a list of pairs of data for the board and reshapes the boards to shape [*BOARD_SHAPE, 1] for nn digestion. For all the possible rotates and a false match\n",
        "\n",
        "  Args:\n",
        "  board - np.array\n",
        "  all_boards - all the other boards on the dataset not including this one\n",
        "\n",
        "  Returns:\n",
        "  a list of 5 tuples, 4 for the rotation part and 1 for a false match\n",
        "  \"\"\"\n",
        "\n",
        "  rotated_views = get_rotated_views(board)\n",
        "  rotated_pairs, rotated_labels = get_rotated_data_pairs(rotated_views)\n",
        "  rotated_pairs = [(model_shape_board(t[0]), model_shape_board(t[1])) for t in rotated_pairs] # reshape boards\n",
        "\n",
        "  # create false match pair\n",
        "  different_board = all_boards[np.random.randint(len(all_boards))] # generate random example\n",
        "  false_label = 0\n",
        "\n",
        "  return rotated_pairs + [[board.reshape([*BOARD_SIZE, 1]), different_board.reshape([*BOARD_SIZE, 1])]], rotated_labels + [false_label]\n",
        "\n",
        "def get_dataset_from_lists(pair_list, label_list, for_encoder=False):\n",
        "  \"\"\"\n",
        "  Creates an x,y dataset from a list of pairs and a list of labels. One hot encode labels\n",
        "\n",
        "  Args:\n",
        "  pair list: a list of pairs of images\n",
        "  label list: a list of ints - labels\n",
        "  for_encoder: bool - determains if the dataset if for the encoder\n",
        "\n",
        "  Retrurns:\n",
        "  x - [x1, x2] a list of two numpy array, each of shape [n_samples, *BOARD_SHAPE, 1] containing stacked inputs of one of the siamese twins OR only x1 if for encoder\n",
        "  y - one hot labels, only for siamese networks\n",
        "  \"\"\"\n",
        "\n",
        "  # prepare x\n",
        "  pairs_stacked = np.stack(pair_list)\n",
        "  x1 = pairs_stacked[:,0,:,:] # get one column of inputs\n",
        "  x2 = pairs_stacked[:,1,:,:] # get the second one\n",
        "  x = [x1, x2]\n",
        "\n",
        "  # prepare y\n",
        "  ohe = OneHotEncoder(sparse=False) # sparse = false is super importent!!\n",
        "  stacked_labels = np.stack(label_list).reshape([-1,1]) # stack and reshape label\n",
        "  y = ohe.fit_transform(stacked_labels)\n",
        "\n",
        "  # if the dataset is for the decoder return only one column of images\n",
        "  if not for_encoder:\n",
        "    return x, y\n",
        "  \n",
        "  else:\n",
        "    return x1\n",
        "  \n",
        "\n",
        "def normalize_boards(boards):\n",
        "  \"\"\"\n",
        "  Normalize a list of boards. The boards already have unit variance so we only need to zero-mean them.\n",
        "  \"\"\"\n",
        "  \n",
        "  sum = 0\n",
        "\n",
        "  # check what is the avarage of the boards:\n",
        "  for board in boards:\n",
        "    sum += board.sum()\n",
        "  \n",
        "  avarage = sum / (len(boards) * BOARD_SIZE[0] * BOARD_SIZE[1])\n",
        "  \n",
        "  # normalize boards\n",
        "  norm_boards = [board - avarage for board in boards]\n",
        "\n",
        "  return norm_boards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZA3unNtUSBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_siamese_dataset(training_tasks, eval_tasks, test_tasks, get_lists=False):\n",
        "  \"\"\"\n",
        "  Creates a dataset for the siamese networks.\n",
        "\n",
        "  Args:\n",
        "  .._tasks: list of tasks.\n",
        "  get_lists: bool, whether to get just the lists of pairs or the processed dataset.\n",
        "  \"\"\"\n",
        "\n",
        "  # extract all_boards\n",
        "  all_boards = get_all_boards(training_tasks, eval_tasks, test_tasks)\n",
        "\n",
        "  # binirize all boards\n",
        "  all_boards_binary = [get_binary_board(board) for board in all_boards]\n",
        "\n",
        "  # normalize boards --> not sure if necessary\n",
        "  #all_boards_binary = normalize_boards(all_boards_binary)\n",
        "\n",
        "  # create a list of all boards augmentation data\n",
        "  pair_list = []\n",
        "  label_list = []\n",
        "\n",
        "  # iterate over all boards\n",
        "  for i, board in enumerate(all_boards_binary):\n",
        "    board_pairs, board_labels = get_all_pairs_reshaped(board, all_boards[i:]) # augment example. use only boards from here onward\n",
        "    pair_list += board_pairs\n",
        "    label_list += board_labels\n",
        "\n",
        "  ## create dataset\n",
        "  x, y = get_dataset_from_lists(pair_list, label_list)\n",
        "\n",
        "  if get_lists:\n",
        "    return pair_list, label_list\n",
        "\n",
        "  else:\n",
        "    return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV-DsYyV-W23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### just for this notebook\n",
        "x, y = get_siamese_dataset(training_tasks, eval_tasks, test_tasks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gHJfVQ69rDn",
        "colab_type": "text"
      },
      "source": [
        "# Siamese networks architecture\n",
        "- getting the right architecture\n",
        "- went for residual networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-kGomzhzlo7",
        "colab_type": "text"
      },
      "source": [
        "## Defining basic blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsL9Hpfazgxr",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title basic blocks\n",
        "def residual_encoder_block(filter_num, kernel_size, bn_moment):\n",
        "  \"\"\"\n",
        "  A functional style residual connection convolutional block.\n",
        "  \"\"\"\n",
        "\n",
        "  def block(x, filter_num, kernel_size=(3,3), bn_moment=0.9):\n",
        "    # first layer\n",
        "    c1 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(x)\n",
        "    c1 = BatchNormalization(momentum=bn_moment)(c1)\n",
        "\n",
        "    # second layer\n",
        "    c2 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(c1)\n",
        "    c2 = BatchNormalization(momentum=bn_moment)(c2)\n",
        "\n",
        "    # third layer\n",
        "    c3 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(c2)\n",
        "    c3 = BatchNormalization(momentum=bn_moment)(c3)\n",
        "\n",
        "    # residual connection\n",
        "    res = c1 + c3\n",
        "\n",
        "    return res\n",
        "  \n",
        "  return lambda x: block(x, filter_num, kernel_size, bn_moment)\n",
        "\n",
        "def residual_decoder_block(filter_num, kernel_size, bn_moment, first=False):\n",
        "  \"\"\"\n",
        "  A functional style residual connection deconvolutional block.\n",
        "\n",
        "  Args:\n",
        "  first - bool, determains if it is the first block of a network\n",
        "  \"\"\"\n",
        "  \n",
        "  def block(x, filter_num, kernel_size=(3,3), bn_moment=0.9):\n",
        "    \n",
        "    # first layer\n",
        "    if first:\n",
        "      dc1 = Dense(4*4*filter_num, activation='linear')(x) # num neurons is dependent upon the number of blocks\n",
        "      dc1 = BatchNormalization(momentum=bn_moment)(dc1)\n",
        "      dc1 = Reshape(target_shape=(4,4,filter_num))(dc1) # reshaping is dependent upon the number of blocks\n",
        "    \n",
        "    else:\n",
        "      dc1 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(x)\n",
        "      dc1 = BatchNormalization(momentum=bn_moment)(dc1)\n",
        "\n",
        "    # second layer\n",
        "    dc2 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(dc1)\n",
        "    dc2 = BatchNormalization(momentum=bn_moment)(dc2)\n",
        "\n",
        "    # third layer\n",
        "    dc3 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(dc2)\n",
        "    dc3 = BatchNormalization(momentum=bn_moment)(dc3)\n",
        "\n",
        "    # residual connection\n",
        "    res = dc1 + dc3\n",
        "\n",
        "    return res\n",
        "\n",
        "  return lambda x: block(x, filter_num, kernel_size, bn_moment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU9XOnUA99dD",
        "colab_type": "text"
      },
      "source": [
        "## Main architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISOAGouf9ym1",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def get_encoder(input_shape):\n",
        "    \"\"\"\n",
        "    Creates an encoder with residual connections\n",
        "    \"\"\"\n",
        "\n",
        "    x = Input(input_shape, name='x')  \n",
        "\n",
        "    # first block\n",
        "    block1 = residual_encoder_block(16, kernel_size=(3,3), bn_moment=0.9)(x)\n",
        "    mp1 = MaxPool2D(pool_size=(2,2), name='max_pool1')(block1)\n",
        "\n",
        "    # second block\n",
        "    block2 = residual_encoder_block(32, kernel_size=(3,3), bn_moment=0.9)(mp1)\n",
        "    mp2 = MaxPool2D(pool_size=(2,2), name='max_pool2')(block2)\n",
        "\n",
        "    # third block\n",
        "    block3 = residual_encoder_block(64, kernel_size=(3,3), bn_moment=0.9)(mp2)\n",
        "    mp3 = MaxPool2D(pool_size=(2,2), name='max_pool3')(block3)\n",
        "\n",
        "    # flatten\n",
        "    flat = Flatten()(mp3)\n",
        "\n",
        "    # dense\n",
        "    dense = Dense(DENSE_REP_SIZE, activation='sigmoid', name='dense_rep')(flat)\n",
        "    bn_dense = BatchNormalization(momentum=0.9)(dense)\n",
        "\n",
        "    return tf.keras.Model(inputs=x, outputs=bn_dense)\n",
        "\n",
        "def get_siamese_networks_model(input_shape):\n",
        "  \"\"\"\n",
        "  Creates siamese networks model. ref paper: https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf\n",
        "  \"\"\"\n",
        "\n",
        "  # define input vectors\n",
        "  input1 = Input(input_shape, name='input1')\n",
        "  input2 = Input(input_shape, name='input2')\n",
        "\n",
        "  # get shared encoder\n",
        "  encoder = get_encoder(input_shape)\n",
        "  \n",
        "  # get feature vectors\n",
        "  v1 = encoder(input1)\n",
        "  v2 = encoder(input2)\n",
        "\n",
        "  # compute L1 loss\n",
        "  L1_Layer = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))\n",
        "  L1_diff = L1_Layer([v1, v2])\n",
        "\n",
        "  # compute probs\n",
        "  probs = Dense(5, activation='softmax')(L1_diff)\n",
        "\n",
        "  siamese_net = tf.keras.Model(inputs=[input1, input2], outputs=probs)\n",
        "\n",
        "  # compile\n",
        "  siamese_net.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
        "  return siamese_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEKcGPtKgbM6",
        "colab_type": "text"
      },
      "source": [
        "## Training siamese networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0kKJwuy2t2R",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "sn = get_siamese_networks_model([*BOARD_SIZE, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfvNty6FbxNA",
        "colab_type": "code",
        "outputId": "87018032-75cb-4403-e478-ef175c9d80f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "hist = sn.fit(x=x, y=y, epochs=30, batch_size=SN_BATCH_SIZE, shuffle=True, validation_split=0.2)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            " 791/2343 [=========>....................] - ETA: 28s - loss: 0.1798 - categorical_accuracy: 0.9411"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-3f7152cfe031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSN_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \"\"\"\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    517\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \"\"\"\n\u001b[1;32m    960\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDuSUF8lRP0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_training_curves(hist.history, metric='categorical_accuracy', with_val=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkiOZvd7gOkP",
        "colab_type": "text"
      },
      "source": [
        "# Board generator (decoder) architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ir0OqLyUPa_",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def get_encoder_from_siamese(siamese):\n",
        "  \"\"\"\n",
        "  Creates a board encoder from the siamese networks model. Maps board --> feature vector of dim DENSE_REP_SIZE\n",
        "  \"\"\"\n",
        "\n",
        "  # create encoder from all layers up to dense\n",
        "  model = tf.keras.models.Sequential([\n",
        "                                      tf.keras.layers.InputLayer(input_shape=[*BOARD_SIZE, 1]),\n",
        "                                      siamese.layers[2] # all sequential layers from siamese networks model\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "def get_decoder():\n",
        "  \"\"\"\n",
        "  Creates a decoder. Maps feature vector of dim DENSE_REP_SIZE --> board. \n",
        "  Same architecture from encoder is preserved.\n",
        "  \"\"\"\n",
        "\n",
        "  # build model\n",
        "  inp = Input([DENSE_REP_SIZE])\n",
        "\n",
        "  # first_block\n",
        "  block1 = residual_decoder_block(64, kernel_size=(3,3), bn_moment=0.9, first=True)(inp)\n",
        "  us1 = UpSampling2D(size=(2,2))(block1)\n",
        "\n",
        "  block2 = residual_decoder_block(32, kernel_size=(3,3), bn_moment=0.9)(us1)\n",
        "  us2 = UpSampling2D(size=(2,2))(block2)\n",
        "\n",
        "  block3 = residual_decoder_block(16, kernel_size=(3,3), bn_moment=0.9)(us2)\n",
        "\n",
        "  output = Conv2D(1, kernel_size=(1,1), activation='sigmoid')(block3)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inp, outputs=output)\n",
        "\n",
        "  # compile\n",
        "  model.compile(loss=pixelwise_error_loss, optimizer='adam', metrics=[pixelwise_auc])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def pixelwise_error_loss(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Custom loss function. Pixelwise error from the true image. ||y_true - y_pred||\n",
        "  \"\"\"\n",
        "\n",
        "  return tf.keras.backend.sum((y_true - y_pred) ** 2) / y_pred.shape[0]\n",
        "\n",
        "\n",
        "def pixelwise_accuracy(y_true, y_pred, threshold=0.8):\n",
        "  \"\"\"\n",
        "  Custom accuracy function. Pixelwise accuracy of predicting 0 / 1\n",
        "  \"\"\"\n",
        "  \n",
        "  tp = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred >= threshold), 'float32'), y_true)) # cast boolean to binary\n",
        "  tn = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred < threshold), 'float32'), tf.cast((y_true == 0), 'float32')))\n",
        "  n_predictions = BOARD_SIZE[0] * BOARD_SIZE[1]\n",
        "\n",
        "  return ((tp + tn) / n_predictions) / y_pred.shape[0]\n",
        "\n",
        "\n",
        "def pixelwise_sensitivity(y_true, y_pred, threshold=0.8):\n",
        "  \"\"\"\n",
        "  Pixelwise sensitivity of correctly predicting 1s. true positives / positives\n",
        "  \"\"\"\n",
        "\n",
        "  tp = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred >= threshold), 'float32'), y_true)) \n",
        "  p = tf.cast(tf.math.reduce_sum(y_true), 'float32') \n",
        "  \n",
        "  return tf.cond(p > 0, lambda: tp / p, lambda: tf.constant(1, dtype='float32')) # don't divide by zero\n",
        "\n",
        "def pixelwise_fpr(y_true, y_pred, threshold=0.8):\n",
        "  \"\"\"\n",
        "  Pixelwise false positive rate of wrongly predicting 1s. false positives / negatives\n",
        "  \"\"\"\n",
        "\n",
        "  fp = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred >= threshold), 'float32'), tf.cast((y_true==0), 'float32'))) # cast boolean to binary\n",
        "  n = tf.cast(tf.math.reduce_sum(tf.cast((y_true == 0), 'float32')), 'float32')\n",
        "\n",
        "  return tf.cond(n > 0, lambda: fp / n, lambda: tf.constant(1, dtype='float32')) # don't divide by zero\n",
        "\n",
        "\n",
        "def pixelwise_auc(y_true, y_pred, num_thresholds=10):\n",
        "  \"\"\"\n",
        "  Pixelwise auc score. \n",
        "  \"\"\"\n",
        "\n",
        "  sensitivity = []\n",
        "  fpr = []\n",
        "\n",
        "  # compute graph points\n",
        "  for threshold in np.linspace(1, 0, num=num_thresholds):\n",
        "    sensitivity.append(pixelwise_sensitivity(y_true, y_pred, threshold=threshold))\n",
        "    fpr.append(pixelwise_fpr(y_true, y_pred, threshold=threshold))\n",
        "  \n",
        "  # compute trapeze area\n",
        "  trapeze = []\n",
        "  for i in range(len(sensitivity)- 1):\n",
        "    area = tf.multiply(sensitivity[i], fpr[i+1] - fpr[i])\n",
        "    trapeze.append(area)\n",
        "  \n",
        "  auc_score = tf.add_n(trapeze) \n",
        "\n",
        "  return auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNIBD7hmoL-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dec = get_decoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uBo_DSMrXDZ",
        "colab_type": "text"
      },
      "source": [
        "## Decoder dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmNwACcSraxu",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def get_decoder_dataset(siamese, encoder_dataset):\n",
        "  \"\"\"\n",
        "  Creates the decoder dataset from the encoder.\n",
        "  \n",
        "  Args:\n",
        "  siamese - siamese model to extract encoder from\n",
        "  encoder dataset - boards to encoder to feature vectors\n",
        "\n",
        "  Returns:\n",
        "  x - boards feature vectors, predicted by encoder\n",
        "  y - boards \n",
        "  \"\"\"\n",
        "  \n",
        "  enc = get_encoder_from_siamese(siamese) # fetch encoder\n",
        "\n",
        "  # build dataset\n",
        "  x = enc.predict(encoder_dataset) # get feature vectors\n",
        "  y = encoder_dataset # boards to recreate\n",
        "\n",
        "  return x,y\n",
        "\n",
        "# create encoder dataset from the old pair and label lists\n",
        "pair_list, label_list = get_siamese_dataset(training_tasks, eval_tasks, test_tasks, get_lists=True)\n",
        "encoder_dataset = get_dataset_from_lists(pair_list, label_list, for_encoder=True) \n",
        "# create decoder dataset from encoder\n",
        "decoder_x, decoder_y = get_decoder_dataset(sn, encoder_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg9iv1vPtEEe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "73324223-b327-4aee-ed35-154939094d15"
      },
      "source": [
        "hist_decoder = dec.fit(x=decoder_x, y=decoder_y, batch_size=DECODER_BATCH_SIZE, shuffle=True, epochs=10, validation_split=0.2)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 3667/18744 [====>.........................] - ETA: 2:30 - loss: 15.4399 - pixelwise_auc: 0.8705"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-01e640f77e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDECODER_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    850\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pHSQ3pYcTII",
        "colab_type": "text"
      },
      "source": [
        "# Check regular encoder-decoder architecture\n",
        "## For checking some hyperparameters and deciding which adecoder architecture to use\n",
        "\n",
        "## Some takeaways:\n",
        "- adding two sets of randomly padded data helps alot. more doesnt help\n",
        "- small decoder batch size (4-16) really helps\n",
        "- adding more neurons to the compressed represantation (512 --> 1024) really helps\n",
        "- removing the last layers batch normalization really helps\n",
        " - removing bn before compressed representation isnt good\n",
        "- penelizing mistaking 1's for 0's more doesnt help at all\n",
        "- changing last layer activation to linear and clipping to [0,1] helps to some extent\n",
        "- chagning middle to relu - really bad!!!\n",
        "- addind another conv-deconv layer at the beggining helps a little\n",
        "- adding another conv-deconv layer at the end doesnt help at all\n",
        "- residual layers - really cool! used the architecture from this paper -- >https://github.com/omerhac/arc_challenge/blob/master/deep%20residual%20conv-deconv%20network.pdf\n",
        "- more types of data augmentation....\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzGZn1y0vt54",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRvqZ5OczS9J",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def get_encoder_decoder(input_shape):\n",
        "  \"\"\"\n",
        "  Create an encoder decoer \"normal\" architecture, with residual connections\n",
        "  \"\"\"\n",
        "  inp = tf.keras.layers.Input([*BOARD_SIZE, 1])\n",
        "  \n",
        "  # encoder\n",
        "  encoder = get_encoder(input_shape)\n",
        "\n",
        "  # dense representation\n",
        "  dense_rep = encoder(inp)\n",
        "\n",
        "  # decoder\n",
        "  decoder = get_decoder()\n",
        "  pred_board = decoder(dense_rep)\n",
        "\n",
        "  model = tf.keras.Model(inp, pred_board)\n",
        "\n",
        "  # compile\n",
        "  model.compile(loss=pixelwise_error_loss, optimizer='adam', metrics=[pixelwise_auc])\n",
        "  return model\n",
        "\n",
        "def get_encoder_from_autoencoder(auto_encoder):\n",
        "  \"\"\"\n",
        "  Get the trained decoder from the autoencoder.\n",
        "  \"\"\"\n",
        "\n",
        "  return tf.keras.models.Sequential([\n",
        "                                     auto_encoder.layers[0],\n",
        "                                     auto_encoder.layers[1]\n",
        "  ])\n",
        "\n",
        "def get_decoder_from_autoencoder(auto_encoder):\n",
        "  \"\"\"\n",
        "  Get the decoder from the autoencoder.\n",
        "  \"\"\"\n",
        "\n",
        "  return tf.keras.models.Sequential([\n",
        "                                     tf.keras.layers.Input(shape=[DENSE_REP_SIZE]), # cut some corners here.. should inherit input shape from autoencoder\n",
        "                                     auto_encoder.layers[2]\n",
        "  ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mharVxoMMqJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ed = get_encoder_decoder([*BOARD_SIZE, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb0_gTC8nIdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "c60985a8-e3fc-41cd-cee1-3cc9cdf96869"
      },
      "source": [
        "ed_hist = ed.fit(x=decoder_y, y=decoder_y, batch_size=8, shuffle=True, epochs=20, validation_split=0.2) # get image --> predict image"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "18744/18744 [==============================] - 357s 19ms/step - loss: 3.3351 - pixelwise_auc: 0.9724 - val_loss: 3.6203 - val_pixelwise_auc: 0.9664\n",
            "Epoch 2/20\n",
            "  448/18744 [..............................] - ETA: 5:24 - loss: 2.6711 - pixelwise_auc: 0.9800"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-136-4b04debe872d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0med_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0med\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get image --> predict image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    850\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9ii2z4hh9de",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## check predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eytnCHw0iUIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boards = decoder_y[-10:]\n",
        "\n",
        "# predict\n",
        "predictions = ed.predict(boards)\n",
        "\n",
        "# reshape\n",
        "boards = [plotting_shape_board(board) for board in boards]\n",
        "predictions = [plotting_shape_board(board) for board in predictions]\n",
        "pairs = zip(boards, predictions)\n",
        "\n",
        "plot_decoder_boards(list(pairs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kujm22z3y0qa",
        "colab_type": "text"
      },
      "source": [
        "# Interleaved training (decoder/encoder/deocoder..)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5XjrxlkyZOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def copy_decoder_to_siamese(siamese, decoder):\n",
        "  \"\"\"\n",
        "  Copys the weights from the decoder to the siamese networks model\n",
        "  \"\"\"\n",
        "\n",
        "  decoder_layers = decoder.layers # get decoder layers\n",
        "  siamese_layers = siamese.layers[2].layers # siamese encoder layers\n",
        "\n",
        "  for i, layer in enumerate(decoder_layers[::-1]):\n",
        "      if(2 >= len(layer.weights) > 0): # trainable layer and not BN layer\n",
        "        if layer.weights[0].shape[0] < 100: # deconv layers\n",
        "          decoder_layer_weigths = layer.get_weights() # get weights from decoder\n",
        "          bias = np.zeros(siamese_layers[i].get_weights()[1].shape) # init new bias\n",
        "          w = decoder_layer_weigths[0].transpose([1,0,2,3]) # transpose weights\n",
        "          siamese_layers[i].set_weights([w, bias]) # set siamese weights\n",
        "\n",
        "        else:\n",
        "          # dense layer\n",
        "          decoder_layer_weigths = layer.get_weights() # get weights from decoder\n",
        "          bias = np.zeros(siamese_layers[i].get_weights()[1].shape) # init new bias\n",
        "          w = decoder_layer_weigths[0].transpose() # transpose weights\n",
        "          siamese_layers[i].set_weights([w, bias]) # set siamese weights\n",
        "\n",
        "\n",
        "def copy_siamese_to_decoder(siamese, decoder):\n",
        "  \"\"\"\n",
        "  Copys the weights from the siamese networks model to the decoder\n",
        "  \"\"\"\n",
        "\n",
        "  decoder_layers = decoder.layers # get decoder layers\n",
        "  siamese_layers = siamese.layers[2].layers # siamese encoder layers\n",
        "\n",
        "  for i, layer in enumerate(decoder_layers[::-1]):\n",
        "      if(2 >= len(layer.weights) > 0): # trainable layer and not BN layer\n",
        "        if layer.weights[0].shape[0] < 100: # deconv layers\n",
        "          siamese_layer_weigths = siamese_layers[i].get_weights() # get weights from siamese encoder\n",
        "          bias = np.zeros(layer.get_weights()[1].shape) # init new bias\n",
        "          w = siamese_layer_weigths[0].transpose([1,0,2,3]) # transpose weights\n",
        "          layer.set_weights([w, bias]) # set decoder weights\n",
        "\n",
        "        else:\n",
        "          # dense layer\n",
        "          siamese_layer_weigths = siamese_layers[i].get_weights() # get weights from siamese encoder\n",
        "          bias = np.zeros(layer.get_weights()[1].shape) # init new bias\n",
        "          w = siamese_layer_weigths[0].transpose() # transpose weights\n",
        "          layer.set_weights([w, bias]) # set decoder weights\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2ucFsFkzIkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create new models with same weights\n",
        "EPOCHS = 30\n",
        "sn = get_siamese_networks_model([*BOARD_SIZE, 1])\n",
        "decoder = get_decoder(sn, copy_encoder_weights=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pfDn78N4ecn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "SN_BATCH_SIZE = 64\n",
        "DEC_BATCH_SIZE = 4\n",
        "DATA_SPLIT = 5\n",
        "SN_STEPS_PER_EPOCH = len(x) // SN_BATCH_SIZE\n",
        "DEC_STEPS_PER_EPOCH = len(decoder_x) // DEC_BATCH_SIZE\n",
        "\n",
        "# even fancier training loop\n",
        "for i in range(EPOCHS):\n",
        "  print(\"training... epoch num: {}\".format(i))\n",
        "\n",
        "  for i in range(40):\n",
        "    # train siamese\n",
        "    _ = sn.fit(x=x, y=y, epochs=1, batch_size=SN_BATCH_SIZE, shuffle=True, steps_per_epoch=(780//DATA_SPLIT))\n",
        "    \n",
        "    # copy weights\n",
        "    copy_siamese_to_decoder(sn, decoder)\n",
        "\n",
        "    # get new decoder dataset\n",
        "    decoder_x, decoder_y = get_decoder_dataset(sn, encoder_dataset) # encoder dataset is the same as before\n",
        "\n",
        "    # train decoder\n",
        "    _ = decoder.fit(x=decoder_x, y=decoder_y, epochs=1, batch_size=DEC_BATCH_SIZE, shuffle=True, steps_per_epoch=(24000//DATA_SPLIT)) #### WOW!!! use smaller batch size, WOHOOO!!\n",
        "\n",
        "    # copy weights\n",
        "    copy_decoder_to_siamese(sn, decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BefQARZeJ6OK",
        "colab_type": "text"
      },
      "source": [
        "# Predicting from diffrance in boards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Api-_erL_rs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task_training_input, task_training_output, task_test_input, task_test_output = get_task_boards(training_tasks[0], pad=pad, divide_sets=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmT5I2c2KNS0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "889ff828-0754-4336-828a-2cd5bccfba31"
      },
      "source": [
        "EXAMPLE = 99\n",
        "# toy dataset (x is the dataset for the siamese networks)\n",
        "anchor = x[0][EXAMPLE]\n",
        "rotate_once_anchor = x[1][EXAMPLE + 1]\n",
        "rotate_twice_anchor = x[1][EXAMPLE + 2]\n",
        "rotate_three_anchor = x[1][EXAMPLE + 3]\n",
        "\n",
        "toy = np.stack([anchor, rotate_once_anchor, rotate_twice_anchor, rotate_three_anchor])\n",
        "\n",
        "# plot\n",
        "fig, axs = plt.subplots(1,4)\n",
        "plot_board(plotting_shape_board(anchor), axs[0], title='0')\n",
        "plot_board(plotting_shape_board(rotate_once_anchor), axs[1], title='1')\n",
        "plot_board(plotting_shape_board(rotate_twice_anchor), axs[2], title='2')\n",
        "plot_board(plotting_shape_board(rotate_three_anchor), axs[3], title='3')"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVmhKDCOOEYJ",
        "colab_type": "text"
      },
      "source": [
        "## define predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uVadzm3Lw3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictor(decoder):\n",
        "  \"\"\"\n",
        "  Builds a detector which takes a board and a rules vector and predicts output board.\n",
        "\n",
        "  Args:\n",
        "  decoder --> trained decoder \n",
        "  \"\"\"\n",
        "\n",
        "  # inputs\n",
        "  board_vec = Input([DENSE_REP_SIZE])\n",
        "  rules_vec = Input([DENSE_REP_SIZE])\n",
        "\n",
        "  # predict\n",
        "  transformed_board = board_vec + rules_vec\n",
        "  prediction = decoder(transformed_board)\n",
        "\n",
        "  model = tf.keras.Model(inputs=[board_vec, rules_vec], outputs=prediction)\n",
        "\n",
        "  model.compile(loss=pixelwise_error_loss, optimizer='adam', metrics=[pixelwise_auc])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AD1eYdE_p0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## encode all toy datset\n",
        "encoder = get_encoder_from_autoencoder(ed)\n",
        "encoded_boards = encoder.predict(toy)\n",
        "\n",
        "# get diffrance\n",
        "diff = encoded_boards[1:] - encoded_boards[:-1]\n",
        "\n",
        "#diff = np.stack([diff[2], diff[0], diff[1]])\n",
        "\n",
        "# predict difference\n",
        "predictor = get_predictor(get_decoder_from_autoencoder(ed))\n",
        "predictions = predictor.predict([encoded_boards[:-1], diff])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MYz7l_mANIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "29726ea7-cea8-459c-d9df-7081fcb6755c"
      },
      "source": [
        "fig,axs = plt.subplots(1,3)\n",
        "plot_board(plotting_shape_board(predictions[0]), axs[0])\n",
        "plot_board(plotting_shape_board(predictions[1]), axs[1])\n",
        "plot_board(plotting_shape_board(predictions[2]), axs[2])"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD4fKdqJAPEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = dec.predict(encoded_boards)\n",
        "fig,axs = plt.subplots(1,4)\n",
        "plot_board(plotting_shape_board(p[0]), axs[0])\n",
        "plot_board(plotting_shape_board(p[1]), axs[1])\n",
        "plot_board(plotting_shape_board(p[2]), axs[2])\n",
        "plot_board(plotting_shape_board(p[3]), axs[3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JciQ9znGAQLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# permutate\n",
        "diffs = np.stack([diff[0], diff[1], diff[2],diff[0], diff[1], diff[2]])\n",
        "eb = np.stack([encoded_boards[0], encoded_boards[0], encoded_boards[0], encoded_boards[1], encoded_boards[1], encoded_boards[1]])\n",
        "t = np.stack([toys[1], toys[1], toys[1], toys[2], toys[2], toys[2]])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}