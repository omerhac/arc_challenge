{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arc.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omerhac/arc_challenge/blob/master/arc_modulerized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juC_cDVsM0yr",
        "colab_type": "text"
      },
      "source": [
        "# Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YShrwCd2GUWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import json\n",
        "from google.cloud import storage\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import colors\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Conv2D, Lambda, Dense, Flatten, MaxPool2D, Input, BatchNormalization, Conv2DTranspose, UpSampling2D, Reshape\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import math\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uko3X-E_CrZr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "204c6095-b1e1-48ec-cfc6-d62b037e2db2"
      },
      "source": [
        "# get repository from github\n",
        "!git clone https://github.com/omerhac/arc_challenge.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'arc_challenge'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcDHkw11CtSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# navigate to directory\n",
        "%cd arc_challenge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX5E2ZNODM5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load repository dependencies\n",
        "!pip install import_ipynb\n",
        "import import_ipynb\n",
        "import preprocess\n",
        "from importlib import reload\n",
        "from metrics import * # metrics module"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF4Ovm97vXEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## constants ##\n",
        "BOARD_SIZE = (16,16) # board upperbound size\n",
        "SN_BATCH_SIZE = 64\n",
        "DECODER_BATCH_SIZE = 8\n",
        "DENSE_REP_SIZE = 64 # dense vector represantation size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqOKR-POOXki",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oOyaYHAJr1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "training_tasks, eval_tasks, test_tasks = preprocess.load_data_from_jsons()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZA3unNtUSBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_siamese_dataset(training_tasks, eval_tasks, test_tasks, get_lists=False):\n",
        "  \"\"\"\n",
        "  Creates a dataset for the siamese networks.\n",
        "\n",
        "  Args:\n",
        "  .._tasks: list of tasks.\n",
        "  get_lists: bool, whether to get just the lists of pairs or the processed dataset.\n",
        "  \"\"\"\n",
        "\n",
        "  # extract all_boards\n",
        "  all_boards = preprocess.get_all_boards(training_tasks, eval_tasks, test_tasks)\n",
        "\n",
        "  # binirize all boards\n",
        "  all_boards_binary = [preprocess.get_binary_board(board) for board in all_boards]\n",
        "\n",
        "  # normalize boards --> not sure if necessary\n",
        "  #all_boards_binary = normalize_boards(all_boards_binary)\n",
        "\n",
        "  # create a list of all boards augmentation data\n",
        "  pair_list = []\n",
        "  label_list = []\n",
        "\n",
        "  # iterate over all boards\n",
        "  for i, board in enumerate(all_boards_binary):\n",
        "    board_pairs, board_labels = preprocess.get_all_pairs_reshaped(board, all_boards[i:]) # augment example. use only boards from here onward\n",
        "    pair_list += board_pairs\n",
        "    label_list += board_labels\n",
        "\n",
        "  ## create dataset\n",
        "  x, y = preprocess.get_dataset_from_lists(pair_list, label_list)\n",
        "\n",
        "  if get_lists:\n",
        "    return pair_list, label_list\n",
        "\n",
        "  else:\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV-DsYyV-W23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### just for this notebook\n",
        "x, y = get_siamese_dataset(training_tasks, eval_tasks, test_tasks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-kGomzhzlo7",
        "colab_type": "text"
      },
      "source": [
        "# Basic residual blocks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsL9Hpfazgxr",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def residual_encoder_block(filter_num, kernel_size, bn_moment):\n",
        "  \"\"\"\n",
        "  A functional style residual connection convolutional block.\n",
        "  \"\"\"\n",
        "\n",
        "  def block(x, filter_num, kernel_size=(3,3), bn_moment=0.9):\n",
        "    # first layer\n",
        "    c1 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(x)\n",
        "    c1 = BatchNormalization(momentum=bn_moment)(c1)\n",
        "\n",
        "    # second layer\n",
        "    c2 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(c1)\n",
        "    c2 = BatchNormalization(momentum=bn_moment)(c2)\n",
        "\n",
        "    # third layer\n",
        "    c3 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(c2)\n",
        "    c3 = BatchNormalization(momentum=bn_moment)(c3)\n",
        "\n",
        "    # residual connection\n",
        "    res = c1 + c3\n",
        "\n",
        "    return res\n",
        "  \n",
        "  return lambda x: block(x, filter_num, kernel_size, bn_moment)\n",
        "\n",
        "def residual_decoder_block(filter_num, kernel_size, bn_moment):\n",
        "  \"\"\"\n",
        "  A functional style residual connection deconvolutional block.\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  def block(x, filter_num, kernel_size=(3,3), bn_moment=0.9):\n",
        "    \n",
        "    # first layer\n",
        "    dc1 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(x)\n",
        "    dc1 = BatchNormalization(momentum=bn_moment)(dc1)\n",
        "\n",
        "    # second layer\n",
        "    dc2 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(dc1)\n",
        "    dc2 = BatchNormalization(momentum=bn_moment)(dc2)\n",
        "\n",
        "    # third layer\n",
        "    dc3 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(dc2)\n",
        "    dc3 = BatchNormalization(momentum=bn_moment)(dc3)\n",
        "\n",
        "    # residual connection\n",
        "    res = dc1 + dc3\n",
        "\n",
        "    return res\n",
        "\n",
        "  return lambda x: block(x, filter_num, kernel_size, bn_moment)\n",
        "\n",
        "def reshape_dense(filter_num, shape):\n",
        "  \"\"\"\n",
        "  Reshapes and projects the dense rep size for the first decoder layer\n",
        "  \"\"\"\n",
        "  def reshaper(x, filter_num, shape):\n",
        "\n",
        "    project = Dense(shape[0]*shape[1]*filter_num, activation='linear')(x) # num neurons is dependent upon the number of blocks\n",
        "    reshape = Reshape(target_shape=(shape[0],shape[1],filter_num))(project) # reshaping is dependent upon the number of blocks\n",
        "\n",
        "    return reshape\n",
        "  \n",
        "  return lambda x: reshaper(x, filter_num, shape)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZXgxsaYBDxW",
        "colab_type": "text"
      },
      "source": [
        "# Defining encoder and decoder architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HSjzttTBCvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_encoder(input_shape, b1_filters, b2_filters, b3_filters):\n",
        "    \"\"\"\n",
        "    Creates an encoder with residual connections\n",
        "\n",
        "    args:\n",
        "    b-_filters: num of filters of the - block\n",
        "    \"\"\"\n",
        "\n",
        "    x = Input(input_shape, name='x')  \n",
        "\n",
        "    # first block\n",
        "    block1 = residual_encoder_block(b1_filters, kernel_size=(3,3), bn_moment=0.9)(x)\n",
        "    de_res1 = Conv2D(b1_filters, kernel_size=(3,3), activation='relu', padding='valid', name='deres1')(block1)\n",
        "\n",
        "    # second block\n",
        "    block2 = residual_encoder_block(b2_filters, kernel_size=(3,3), bn_moment=0.9)(de_res1)\n",
        "    de_res2 = Conv2D(b2_filters, kernel_size=(3,3), activation='relu', padding='valid', name='deres2')(block2)\n",
        "\n",
        "    # third block\n",
        "    block3 = residual_encoder_block(b3_filters, kernel_size=(3,3), bn_moment=0.9)(de_res2)\n",
        "    de_res3 = Conv2D(b3_filters, kernel_size=(3,3), activation='relu', padding='valid', name='deres3')(block3)\n",
        "\n",
        "    # flatten\n",
        "    flat = Flatten()(de_res3)\n",
        "\n",
        "    # dense\n",
        "    dense = Dense(DENSE_REP_SIZE, activation='sigmoid', name='dense_rep')(flat)\n",
        "    bn_dense = BatchNormalization(momentum=0.9)(dense)\n",
        "\n",
        "    return tf.keras.Model(inputs=x, outputs=bn_dense)\n",
        "\n",
        "\n",
        "def get_decoder(b1_filters, b2_filters, b3_filters):\n",
        "  \"\"\"\n",
        "  Creates a decoder. Maps feature vector of dim DENSE_REP_SIZE --> board. \n",
        "  Same architecture from encoder is preserved.\n",
        "\n",
        "  args:\n",
        "  b-_filters: num of fiters of the - block\n",
        "  \"\"\"\n",
        "\n",
        "  # build model\n",
        "  inp = Input([DENSE_REP_SIZE])\n",
        "\n",
        "  # reshaping\n",
        "  reshaped_dense = reshape_dense(b1_filters, (10, 10))(inp)\n",
        "\n",
        "  # first_block\n",
        "  up_res1 = Conv2DTranspose(b1_filters, kernel_size=(3,3), padding='valid', activation='relu')(reshaped_dense)\n",
        "  block1 = residual_decoder_block(3, kernel_size=(3,3), bn_moment=0.9)(up_res1)\n",
        "  \n",
        "  up_res2 = Conv2DTranspose(b2_filters, kernel_size=(3,3), padding='valid', activation='relu')(block1)\n",
        "  block2 = residual_decoder_block(2, kernel_size=(3,3), bn_moment=0.9)(up_res2)\n",
        "  \n",
        "  up_res3 = Conv2DTranspose(b3_filters, kernel_size=(3,3), padding='valid', activation='relu')(block2)\n",
        "  block3 = residual_decoder_block(1, kernel_size=(3,3), bn_moment=0.9)(up_res3)\n",
        "\n",
        "  output = Conv2D(1, kernel_size=(1,1), activation='sigmoid')(block3)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inp, outputs=output)\n",
        "\n",
        "  # compile\n",
        "  model.compile(loss=pixelwise_error_loss, optimizer='adam', metrics=[pixelwise_auc])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkiOZvd7gOkP",
        "colab_type": "text"
      },
      "source": [
        "# def enc dec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNIBD7hmoL-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dec = get_decoder(3,2,1)\n",
        "enc = get_encoder([*BOARD_SIZE, 1], 1, 2, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62a17RiXO9RX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifOjvgszO3lw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dec.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uBo_DSMrXDZ",
        "colab_type": "text"
      },
      "source": [
        "## Decoder dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmNwACcSraxu",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# create encoder dataset from the old pair and label lists\n",
        "pair_list, label_list = get_siamese_dataset(training_tasks, eval_tasks, test_tasks, get_lists=True)\n",
        "encoder_dataset = preprocess.get_dataset_from_lists(pair_list, label_list, for_encoder=True) \n",
        "\n",
        "# create decoder dataset from encoder\n",
        "decoder_x, decoder_y = preprocess.get_decoder_dataset(enc, encoder_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRvqZ5OczS9J",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def get_encoder_decoder(input_shape, b1_filters, b2_filters, b3_filters):\n",
        "  \"\"\"\n",
        "  Create an encoder decoer \"normal\" architecture, with residual connections\n",
        "  \"\"\"\n",
        "  inp = tf.keras.layers.Input([*BOARD_SIZE, 1])\n",
        "  \n",
        "  # encoder\n",
        "  encoder = get_encoder(input_shape, b1_filters, b2_filters, b3_filters)\n",
        "\n",
        "  # dense representation\n",
        "  dense_rep = encoder(inp)\n",
        "\n",
        "  # decoder\n",
        "  decoder = get_decoder(b3_filters, b2_filters, b1_filters)\n",
        "  pred_board = decoder(dense_rep)\n",
        "\n",
        "  model = tf.keras.Model(inp, pred_board)\n",
        "\n",
        "  # compile\n",
        "  model.compile(loss=pixelwise_error_loss, optimizer='adam', metrics=[pixelwise_auc])\n",
        "  return model\n",
        "\n",
        "def get_encoder_from_autoencoder(auto_encoder):\n",
        "  \"\"\"\n",
        "  Get the trained decoder from the autoencoder.\n",
        "  \"\"\"\n",
        "\n",
        "  return tf.keras.models.Sequential([\n",
        "                                     auto_encoder.layers[0],\n",
        "                                     auto_encoder.layers[1]\n",
        "  ])\n",
        "\n",
        "def get_decoder_from_autoencoder(auto_encoder):\n",
        "  \"\"\"\n",
        "  Get the decoder from the autoencoder.\n",
        "  \"\"\"\n",
        "\n",
        "  return tf.keras.models.Sequential([\n",
        "                                     tf.keras.layers.Input(shape=[DENSE_REP_SIZE]), # cut some corners here.. should inherit input shape from autoencoder\n",
        "                                     auto_encoder.layers[2]\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mharVxoMMqJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ed = get_encoder_decoder([*BOARD_SIZE, 1], 1 ,2 ,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb0_gTC8nIdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ed_hist = ed.fit(x=decoder_y, y=decoder_y, batch_size=8, shuffle=True, epochs=20, validation_split=0.1) # get image --> predict image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9ii2z4hh9de",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## check predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eytnCHw0iUIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boards = decoder_y[-10:]\n",
        "\n",
        "# predict\n",
        "predictions = ed.predict(boards)\n",
        "\n",
        "# reshape\n",
        "boards = [plotting_shape_board(board) for board in boards]\n",
        "predictions = [plotting_shape_board(board) for board in predictions]\n",
        "pairs = zip(boards, predictions)\n",
        "\n",
        "plot_decoder_boards(list(pairs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kujm22z3y0qa",
        "colab_type": "text"
      },
      "source": [
        "# Interleaved training (decoder/encoder/deocoder..)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5XjrxlkyZOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def copy_decoder_to_siamese(siamese, decoder):\n",
        "  \"\"\"\n",
        "  Copys the weights from the decoder to the siamese networks model\n",
        "  \"\"\"\n",
        "\n",
        "  decoder_layers = decoder.layers # get decoder layers\n",
        "  siamese_layers = siamese.layers[2].layers # siamese encoder layers\n",
        "\n",
        "  for i, layer in enumerate(decoder_layers[::-1]):\n",
        "      if(2 >= len(layer.weights) > 0): # trainable layer and not BN layer\n",
        "        if layer.weights[0].shape[0] < 100: # deconv layers\n",
        "          decoder_layer_weigths = layer.get_weights() # get weights from decoder\n",
        "          bias = np.zeros(siamese_layers[i].get_weights()[1].shape) # init new bias\n",
        "          w = decoder_layer_weigths[0].transpose([1,0,2,3]) # transpose weights\n",
        "          siamese_layers[i].set_weights([w, bias]) # set siamese weights\n",
        "\n",
        "        else:\n",
        "          # dense layer\n",
        "          decoder_layer_weigths = layer.get_weights() # get weights from decoder\n",
        "          bias = np.zeros(siamese_layers[i].get_weights()[1].shape) # init new bias\n",
        "          w = decoder_layer_weigths[0].transpose() # transpose weights\n",
        "          siamese_layers[i].set_weights([w, bias]) # set siamese weights\n",
        "\n",
        "\n",
        "def copy_siamese_to_decoder(siamese, decoder):\n",
        "  \"\"\"\n",
        "  Copys the weights from the siamese networks model to the decoder\n",
        "  \"\"\"\n",
        "\n",
        "  decoder_layers = decoder.layers # get decoder layers\n",
        "  siamese_layers = siamese.layers[2].layers # siamese encoder layers\n",
        "\n",
        "  for i, layer in enumerate(decoder_layers[::-1]):\n",
        "      if(2 >= len(layer.weights) > 0): # trainable layer and not BN layer\n",
        "        if layer.weights[0].shape[0] < 100: # deconv layers\n",
        "          siamese_layer_weigths = siamese_layers[i].get_weights() # get weights from siamese encoder\n",
        "          bias = np.zeros(layer.get_weights()[1].shape) # init new bias\n",
        "          w = siamese_layer_weigths[0].transpose([1,0,2,3]) # transpose weights\n",
        "          layer.set_weights([w, bias]) # set decoder weights\n",
        "\n",
        "        else:\n",
        "          # dense layer\n",
        "          siamese_layer_weigths = siamese_layers[i].get_weights() # get weights from siamese encoder\n",
        "          bias = np.zeros(layer.get_weights()[1].shape) # init new bias\n",
        "          w = siamese_layer_weigths[0].transpose() # transpose weights\n",
        "          layer.set_weights([w, bias]) # set decoder weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2ucFsFkzIkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create new models with same weights\n",
        "EPOCHS = 30\n",
        "sn = get_siamese_networks_model([*BOARD_SIZE, 1])\n",
        "decoder = get_decoder(sn, copy_encoder_weights=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pfDn78N4ecn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "SN_BATCH_SIZE = 64\n",
        "DEC_BATCH_SIZE = 4\n",
        "DATA_SPLIT = 5\n",
        "SN_STEPS_PER_EPOCH = len(x) // SN_BATCH_SIZE\n",
        "DEC_STEPS_PER_EPOCH = len(decoder_x) // DEC_BATCH_SIZE\n",
        "\n",
        "# even fancier training loop\n",
        "for i in range(EPOCHS):\n",
        "  print(\"training... epoch num: {}\".format(i))\n",
        "\n",
        "  for i in range(40):\n",
        "    # train siamese\n",
        "    _ = sn.fit(x=x, y=y, epochs=1, batch_size=SN_BATCH_SIZE, shuffle=True, steps_per_epoch=(780//DATA_SPLIT))\n",
        "    \n",
        "    # copy weights\n",
        "    copy_siamese_to_decoder(sn, decoder)\n",
        "\n",
        "    # get new decoder dataset\n",
        "    decoder_x, decoder_y = get_decoder_dataset(sn, encoder_dataset) # encoder dataset is the same as before\n",
        "\n",
        "    # train decoder\n",
        "    _ = decoder.fit(x=decoder_x, y=decoder_y, epochs=1, batch_size=DEC_BATCH_SIZE, shuffle=True, steps_per_epoch=(24000//DATA_SPLIT)) #### WOW!!! use smaller batch size, WOHOOO!!\n",
        "\n",
        "    # copy weights\n",
        "    copy_decoder_to_siamese(sn, decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BefQARZeJ6OK",
        "colab_type": "text"
      },
      "source": [
        "# Predicting from diffrance in boards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Api-_erL_rs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task_training_input, task_training_output, task_test_input, task_test_output = get_task_boards(training_tasks[0], pad=pad, divide_sets=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmT5I2c2KNS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EXAMPLE = 99\n",
        "# toy dataset (x is the dataset for the siamese networks)\n",
        "anchor = x[0][EXAMPLE]\n",
        "rotate_once_anchor = x[1][EXAMPLE + 1]\n",
        "rotate_twice_anchor = x[1][EXAMPLE + 2]\n",
        "rotate_three_anchor = x[1][EXAMPLE + 3]\n",
        "\n",
        "toy = np.stack([anchor, rotate_once_anchor, rotate_twice_anchor, rotate_three_anchor])\n",
        "\n",
        "# plot\n",
        "fig, axs = plt.subplots(1,4)\n",
        "plot_board(plotting_shape_board(anchor), axs[0], title='0')\n",
        "plot_board(plotting_shape_board(rotate_once_anchor), axs[1], title='1')\n",
        "plot_board(plotting_shape_board(rotate_twice_anchor), axs[2], title='2')\n",
        "plot_board(plotting_shape_board(rotate_three_anchor), axs[3], title='3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVmhKDCOOEYJ",
        "colab_type": "text"
      },
      "source": [
        "## define predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uVadzm3Lw3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictor(decoder):\n",
        "  \"\"\"\n",
        "  Builds a detector which takes a board and a rules vector and predicts output board.\n",
        "\n",
        "  Args:\n",
        "  decoder --> trained decoder \n",
        "  \"\"\"\n",
        "\n",
        "  # inputs\n",
        "  board_vec = Input([DENSE_REP_SIZE])\n",
        "  rules_vec = Input([DENSE_REP_SIZE])\n",
        "\n",
        "  # predict\n",
        "  transformed_board = board_vec + rules_vec\n",
        "  prediction = decoder(transformed_board)\n",
        "\n",
        "  model = tf.keras.Model(inputs=[board_vec, rules_vec], outputs=prediction)\n",
        "\n",
        "  model.compile(loss=pixelwise_error_loss, optimizer='adam', metrics=[pixelwise_auc])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AD1eYdE_p0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## encode all toy datset\n",
        "encoder = get_encoder_from_autoencoder(ed)\n",
        "encoded_boards = encoder.predict(toy)\n",
        "\n",
        "# get diffrance\n",
        "diff = encoded_boards[1:] - encoded_boards[:-1]\n",
        "\n",
        "#diff = np.stack([diff[2], diff[0], diff[1]])\n",
        "\n",
        "# predict difference\n",
        "predictor = get_predictor(get_decoder_from_autoencoder(ed))\n",
        "predictions = predictor.predict([encoded_boards[:-1], diff])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MYz7l_mANIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig,axs = plt.subplots(1,3)\n",
        "plot_board(plotting_shape_board(predictions[0]), axs[0])\n",
        "plot_board(plotting_shape_board(predictions[1]), axs[1])\n",
        "plot_board(plotting_shape_board(predictions[2]), axs[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD4fKdqJAPEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = dec.predict(encoded_boards)\n",
        "fig,axs = plt.subplots(1,4)\n",
        "plot_board(plotting_shape_board(p[0]), axs[0])\n",
        "plot_board(plotting_shape_board(p[1]), axs[1])\n",
        "plot_board(plotting_shape_board(p[2]), axs[2])\n",
        "plot_board(plotting_shape_board(p[3]), axs[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JciQ9znGAQLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# permutate\n",
        "diffs = np.stack([diff[0], diff[1], diff[2],diff[0], diff[1], diff[2]])\n",
        "eb = np.stack([encoded_boards[0], encoded_boards[0], encoded_boards[0], encoded_boards[1], encoded_boards[1], encoded_boards[1]])\n",
        "t = np.stack([toys[1], toys[1], toys[1], toys[2], toys[2], toys[2]])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}