{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "board_gan.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOAaNTK+m0Fd1URgpb6pa1p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omerhac/arc_challenge/blob/master/board_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wlv1GB4-Nyzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbftZ2NoObBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "7c9498ae-32cc-4461-ae3c-344df00298a2"
      },
      "source": [
        "!pip install import_ipynb\n",
        "import import_ipynb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import_ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=973332d2acf7d8b3a5980d14ccb1ada2b8d97ef91598193a2109eba79fda5dd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHJh_xVxOpYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "e6177810-8e83-459c-db98-4534fb5b2fbc"
      },
      "source": [
        "# get repository from github\n",
        "!git clone https://github.com/omerhac/arc_challenge.git\n",
        "\n",
        "# navigate to dir\n",
        "%cd arc_challenge"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'arc_challenge'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 197 (delta 8), reused 0 (delta 0), pack-reused 179\u001b[K\n",
            "Receiving objects: 100% (197/197), 10.54 MiB | 11.25 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n",
            "/content/arc_challenge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXkWlMCKO5QY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0126f4c5-c571-4f42-c454-59a05ee799f9"
      },
      "source": [
        "import preprocess"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from preprocess.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-kSWbocP3xG",
        "colab_type": "text"
      },
      "source": [
        "# Data loading and gathering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7Je068QPnsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the lists of tasks\n",
        "training_tasks, eval_tasks, test_tasks = preprocess.load_data_from_jsons()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFw86YhoP688",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# augment and arrange the data as an array\n",
        "all_boards = preprocess.get_all_boards(training_tasks, eval_tasks, test_tasks)\n",
        "all_boards += preprocess.get_all_boards(training_tasks, eval_tasks, test_tasks) # add another batch of randomly padd boards\n",
        "\n",
        "# make all boards binary\n",
        "all_boards = [preprocess.model_shape_board(preprocess.get_binary_board(board)) for board in all_boards]\n",
        "\n",
        "# get rotated views of all the boards\n",
        "rotated_boards = []\n",
        "\n",
        "for board in all_boards:\n",
        "  rotated_views = preprocess.get_rotated_views(board)\n",
        "  rotated_boards += rotated_views\n",
        "\n",
        "# stack\n",
        "rotated_boards = np.stack(rotated_boards)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7PyfjX7TRB8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fa4725ab-57a7-4b19-f829-1c4635cf91f5"
      },
      "source": [
        "print(rotated_boards.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(136320, 16, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tCJHyZYW0Hh",
        "colab_type": "text"
      },
      "source": [
        "# GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELqCBMY3FulS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## constants ##\n",
        "GENERATOR_DENSE_REP = 64\n",
        "N_CHANNELS = 1\n",
        "BOARD_SIZE = [16,16]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J33SGSwCDYWb",
        "colab_type": "text"
      },
      "source": [
        "# Defining the generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz1IdwGxWtfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Conv2DTranspose, BatchNormalization, Input\n",
        "def generator():\n",
        "  \"\"\"\n",
        "  Creates a board generator\n",
        "  \"\"\"\n",
        "\n",
        "  def util_gen(z):\n",
        "    \"\"\"\n",
        "    A utility generator function to be used in a lambda later on\n",
        "    \"\"\"\n",
        "\n",
        "    # project and reshape z\n",
        "    linear = Dense(256*4*4, activation='linear')(z)\n",
        "    reshaped_z = tf.keras.layers.Reshape([4, 4, 256])(linear)\n",
        "\n",
        "    # first deconv layer\n",
        "    g_conv1 = Conv2DTranspose(128, kernel_size=(3,3), activation='linear')(reshaped_z)\n",
        "    g_normalized_1 = BatchNormalization(momentum=0.9)(g_conv1)\n",
        "\n",
        "    # second deconv layer\n",
        "    g_conv2 = Conv2DTranspose(64, kernel_size=(3,3), activation='linear')(g_normalized_1)\n",
        "    g_normalized_2 = BatchNormalization(momentum=0.9)(g_conv2)\n",
        "\n",
        "    # third deconv layer\n",
        "    g_conv3 = Conv2DTranspose(32, kernel_size=(3,3), activation='linear')(g_normalized_2)\n",
        "    g_normalized_3 = BatchNormalization(momentum=0.9)(g_conv3)\n",
        "\n",
        "    # fourth deconv layer\n",
        "    g_conv4 = Conv2DTranspose(16, kernel_size=(3,3), activation='linear')(g_normalized_3)\n",
        "    g_normalized_4 = BatchNormalization(momentum=0.9)(g_conv4)\n",
        "\n",
        "    # fifth deconv layer\n",
        "    g_conv5 = Conv2DTranspose(8, kernel_size=(3,3), activation='linear')(g_normalized_4)\n",
        "    g_normalized_5 = BatchNormalization(momentum=0.9)(g_conv5)\n",
        "\n",
        "    # final layer\n",
        "    output_board = Conv2DTranspose(1, kernel_size=(3,3), activation='tanh')(g_normalized_5)\n",
        "\n",
        "    return output_board\n",
        "\n",
        "  return lambda z: util_gen(z)\n"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkqNcrgYLXux",
        "colab_type": "text"
      },
      "source": [
        "## Defining discriminator "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQIghRGOLfcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, GlobalAveragePooling2D\n",
        "def discriminator():\n",
        "  \"\"\"\n",
        "  Creates a discriminator\n",
        "  \"\"\"\n",
        "\n",
        "  def util_discriminator(board):\n",
        "    \"\"\"\n",
        "    A utility discriminator to be used in a lambda later on\n",
        "    \"\"\"\n",
        "\n",
        "    # first layer\n",
        "    d_conv1 = Conv2D(8, kernel_size=(3,3), activation='relu', name=\"d_conv1\")(board)\n",
        "    d_normalized_1 = BatchNormalization(momentum=0.9)(d_conv1)\n",
        "\n",
        "    # second layer\n",
        "    d_conv2 = Conv2D(16, kernel_size=(3,3), activation='relu', name=\"d_conv2\")(d_normalized_1)\n",
        "    d_normalized_2 = BatchNormalization(momentum=0.9)(d_conv2)\n",
        "\n",
        "    # third layer\n",
        "    d_conv3 = Conv2D(32, kernel_size=(3,3), activation='relu', name=\"d_conv3\")(d_normalized_2)\n",
        "    d_normalized_3 = BatchNormalization(momentum=0.9)(d_conv3)\n",
        "\n",
        "    # fourth layer\n",
        "    d_conv4 = Conv2D(64, kernel_size=(3,3), activation='relu', name=\"d_conv4\")(d_normalized_3)\n",
        "    d_normalized_4 = BatchNormalization(momentum=0.9)(d_conv4)\n",
        "\n",
        "    # dense 1\n",
        "    dense1 = Dense(512, activation='relu')(d_normalized_4)\n",
        "\n",
        "    # dense 2\n",
        "    dense2 = Dense(128, activation='relu')(dense1)\n",
        "\n",
        "    # output\n",
        "    d_out = Dense(1, activation='sigmoid')(dense2)\n",
        "\n",
        "    return d_out\n",
        "  \n",
        "  return lambda boards: util_discriminator(boards)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEzqeG4WGHJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_generator():\n",
        "  z = Input([GENERATOR_DENSE_REP], name='z')\n",
        "  output = generator()(z)\n",
        "  \n",
        "  model = tf.keras.Model(inputs=z, outputs=output)\n",
        "  return model\n",
        "\n",
        "def get_discriminator():\n",
        "  boards = Input([*BOARD_SIZE, 1], name='boards')\n",
        "  output = discriminator()(boards)\n",
        "  \n",
        "  model = tf.keras.Model(inputs=boards, outputs=output)\n",
        "  return model"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD1ynmbIiMlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g = get_generator()\n",
        "d = get_discriminator()"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvicNAkfGbxt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "outputId": "c2507d09-29dc-46f0-cc24-4b1d0cb5c328"
      },
      "source": [
        "g.summary()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "z (InputLayer)               [(None, 64)]              0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 4096)              266240    \n",
            "_________________________________________________________________\n",
            "reshape_9 (Reshape)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_54 (Conv2DT (None, 6, 6, 128)         295040    \n",
            "_________________________________________________________________\n",
            "batch_normalization_83 (Batc (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_55 (Conv2DT (None, 8, 8, 64)          73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_84 (Batc (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_56 (Conv2DT (None, 10, 10, 32)        18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_85 (Batc (None, 10, 10, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_57 (Conv2DT (None, 12, 12, 16)        4624      \n",
            "_________________________________________________________________\n",
            "batch_normalization_86 (Batc (None, 12, 12, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_58 (Conv2DT (None, 14, 14, 8)         1160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_87 (Batc (None, 14, 14, 8)         32        \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_59 (Conv2DT (None, 16, 16, 1)         73        \n",
            "=================================================================\n",
            "Total params: 660,385\n",
            "Trainable params: 659,889\n",
            "Non-trainable params: 496\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTa-z_ybftFc",
        "colab_type": "text"
      },
      "source": [
        "# Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV46BI-HJ0KT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras.backend as kb\n",
        "\n",
        "def discriminator_loss(d_real, d_fake):\n",
        "  \"\"\" \n",
        "  discrimintor loss function\n",
        "\n",
        "  args:\n",
        "  d_real - decisions on real boards\n",
        "  d_fake - decisions on fake boards\n",
        "  \"\"\"\n",
        "\n",
        "  real_part = kb.log(d_real)\n",
        "  fake_part = kb.log(1-d_fake)\n",
        " \n",
        "  # compute loss\n",
        "  loss = (-1/BATCH_SIZE) * kb.sum(real_part + fake_part)\n",
        "\n",
        "  return loss\n",
        "\n",
        "def generator_loss(d_fake):\n",
        "  \"\"\"\n",
        "  generator loss\n",
        "\n",
        "  args:\n",
        "  d_fake - discriminator decisions on fake images\n",
        "  \"\"\"\n",
        "\n",
        "  loss = (-1/BATCH_SIZE) * kb.sum(kb.log(d_fake))\n",
        "  \n",
        "  return loss"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMj1obu2hF_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "5202e88a-b2c4-49da-ad3e-4d2a858f256f"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# get bum of batches\n",
        "num_of_batches = rotated_boards.shape[0] // BATCH_SIZE \n",
        "\n",
        "# set lists of losses\n",
        "g_losses, d_losess = [], []\n",
        "\n",
        "# initate optimizers\n",
        "g_optimizer = tf.keras.optimizers.Adam()\n",
        "d_optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  for batch_num in range(num_of_batches):\n",
        "    \n",
        "    # open gradient tape\n",
        "    with tf.GradientTape(persistent=True) as tape: # persistent is needed for drawing gradients twice\n",
        "      # GENERATE BOARDS\n",
        "      # generate random dense representations\n",
        "      rand_dense_reps = np.random.uniform(-1, 1, [BATCH_SIZE, GENERATOR_DENSE_REP]).astype(np.float32)\n",
        "\n",
        "      # apply generator\n",
        "      fake_boards = g(rand_dense_reps, training=True)\n",
        "      \n",
        "      # DISCRIMINATE\n",
        "      # batch some real examples\n",
        "      real_boards = rotated_boards[batch_num*BATCH_SIZE:(batch_num+1)*BATCH_SIZE]\n",
        "\n",
        "      d_fake = d(fake_boards, training=True) # decesions on fake boards\n",
        "      d_real = d(real_boards, training=True) # decisions on real boards\n",
        "\n",
        "      # compute losses\n",
        "      g_loss = generator_loss(d_fake) # generator loss\n",
        "      d_loss = discriminator_loss(d_real, d_fake) # discriminator loss \n",
        "\n",
        "    # get gradients\n",
        "    g_grad = tape.gradient(g_loss, g.trainable_weights) # gradient of the generator loss function wrt generator weights\n",
        "    d_grad = tape.gradient(d_loss, d.trainable_weights) # gradient of discriminator loss function wrt discriminator weights\n",
        "    \n",
        "    # TRAIN\n",
        "    g_optimizer.apply_gradients(zip(g_grad, g.trainable_weights))\n",
        "    d_optimizer.apply_gradients(zip(d_grad, d.trainable_weights))\n",
        "\n",
        "    # do some prints\n",
        "    if batch_num % (num_of_batches // 100) == 1:\n",
        "      clear_output(wait=True)\n",
        "      print(\"{} % done\".format(int((batch_num / num_of_batches) * 100))) # batch progress status\n",
        "\n",
        "      # print losses\n",
        "      print(\"generator loss: {}\".format(g_loss.numpy())) \n",
        "      print(\"discriminator loss: {}\".format(d_loss.numpy())) \n",
        "\n",
        "      # append losses\n",
        "      g_losses.append(g_loss.numpy())\n",
        "      d_losses.append(d_loss.numpy())\n",
        "\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "84 % done\n",
            "generator loss: nan\n",
            "discriminator loss: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-c548c330f0fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# TRAIN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mg_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0md_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_of_batches\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    506\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m           kwargs={\n\u001b[0;32m--> 508\u001b[0;31m               \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m           })\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     merge_fn = autograph.tf_convert(\n\u001b[1;32m   2419\u001b[0m         merge_fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2425\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[1;32m   2426\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2427\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2428\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[0;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[1;32m    590\u001b[0m                               \"update_\" + var.op.name, skip_on_eager=True):\n\u001b[1;32m    591\u001b[0m             update_ops.extend(distribution.extended.update(\n\u001b[0;32m--> 592\u001b[0;31m                 var, apply_grad_to_update_var, args=(grad,), group=False))\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m       any_symbolic = any(isinstance(i, ops.Operation) or\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2011\u001b[0m         fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m   2012\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2013\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2657\u001b[0m     \u001b[0;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m     \u001b[0;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   2663\u001b[0m     \u001b[0;31m# once that value is used for something.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2664\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2665\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2666\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2667\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m    565\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m\"apply_state\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_apply_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mapply_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"apply_state\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mapply_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/adam.py\u001b[0m in \u001b[0;36m_resource_apply_dense\u001b[0;34m(self, grad, var, apply_state)\u001b[0m\n\u001b[1;32m    212\u001b[0m           \u001b[0mcoefficients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epsilon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m           use_locking=self._use_locking)\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m       \u001b[0mvhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vhat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/gen_training_ops.py\u001b[0m in \u001b[0;36mresource_apply_adam\u001b[0;34m(var, m, v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, use_locking, use_nesterov, name)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m         \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_locking\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_nesterov\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m         use_nesterov)\n\u001b[0m\u001b[1;32m   1413\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_DX91KiE5vG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(d_fake)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx3vqYKsi16g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "752978ff-f0b9-418d-872a-3b37541ea22c"
      },
      "source": [
        "b = preprocess.plotting_shape_board(g(np.random.uniform(-1, 1, [2000, GENERATOR_DENSE_REP]).astype(np.float32)).numpy()[400])\n",
        "fig, ax = plt.subplots(1,1)\n",
        "preprocess.plot_board(b, ax)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXxYDaUoslzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}