{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arc.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omerhac/arc_challenge/blob/master/decoder_not_working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YShrwCd2GUWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import json\n",
        "from google.cloud import storage\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import colors\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Conv2D, Lambda, Dense, Flatten, MaxPool2D, Input, BatchNormalization, Conv2DTranspose, UpSampling2D, Reshape\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uko3X-E_CrZr",
        "colab_type": "code",
        "outputId": "1caf99f2-bec9-40a1-9f79-79f98234b293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# get repository from github\n",
        "!git clone https://github.com/omerhac/arc_challenge.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'arc_challenge' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcDHkw11CtSu",
        "colab_type": "code",
        "outputId": "8bccc7a1-62a1-4b6e-9ef6-f7c032276688",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# navigate to directory\n",
        "cd arc_challenge"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/arc_challenge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX5E2ZNODM5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load repository dependencies\n",
        "import preprocess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF4Ovm97vXEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## constants ##\n",
        "BOARD_SIZE = (16,16) # board upperbound size\n",
        "SN_BATCH_SIZE = 64\n",
        "DECODER_BATCH_SIZE = 8\n",
        "DENSE_REP_SIZE = 1024 # dense vector represantation size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqOKR-POOXki",
        "colab_type": "text"
      },
      "source": [
        "# Load data / util funcitons\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-96BlT_hHVkF",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title load data\n",
        "def load_data():\n",
        "  \"\"\"\n",
        "  Loads all the data into training_tasks, eval_tasks and test_tasks\n",
        "  \"\"\"\n",
        "\n",
        "  ## get paths\n",
        "  GCS_PATH = \"gs://kds-d3cfb3d523ca35d2517017a78110126404d01fdea69417ce49950459\"\n",
        "  training_filenames = tf.io.gfile.glob(GCS_PATH + \"/training/*\")\n",
        "  test_filenames = tf.io.gfile.glob(GCS_PATH + \"/test/*\")\n",
        "  eval_filenames = tf.io.gfile.glob(GCS_PATH + \"/evaluation/*\")\n",
        "\n",
        "  # create datasets with filenames\n",
        "  training_dataset = tf.data.Dataset.list_files(training_filenames)\n",
        "  eval_dataset = tf.data.Dataset.list_files(eval_filenames)\n",
        "  test_dataset = tf.data.Dataset.list_files(test_filenames)\n",
        "\n",
        "  # load the jsons\n",
        "  def load_task(filename):\n",
        "    task_json = tf.io.read_file(filename)\n",
        "    return task_json\n",
        "\n",
        "  training_dataset = training_dataset.map(load_task)\n",
        "  eval_dataset = eval_dataset.map(load_task)\n",
        "  test_dataset = test_dataset.map(load_task)\n",
        "\n",
        "  training_dataset_numpy = tf.data.Dataset.as_numpy_iterator(training_dataset) # convert to numpy iterator\n",
        "  eval_dataset_numpy = tf.data.Dataset.as_numpy_iterator(eval_dataset)\n",
        "  test_dataset_numpy = tf.data.Dataset.as_numpy_iterator(test_dataset)\n",
        "\n",
        "  ## create a numpy array of tasks (n_tasks, )\n",
        "  def array_from_jsons(jsons_numpy_iterator):\n",
        "    \"\"\"\n",
        "      Create an array of task dictionaries from jsons numpy interator\n",
        "    \"\"\"\n",
        "\n",
        "    tasks = []\n",
        "    for task in jsons_numpy_iterator:\n",
        "      tasks.append(json.loads(task))\n",
        "\n",
        "    return np.stack(tasks)\n",
        "\n",
        "  ## get numpy arrays of datasets\n",
        "  training_tasks = array_from_jsons(training_dataset_numpy)\n",
        "  eval_tasks = array_from_jsons(eval_dataset_numpy)\n",
        "  test_tasks = array_from_jsons(test_dataset_numpy)\n",
        "\n",
        "  return training_tasks, eval_tasks, test_tasks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oOyaYHAJr1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f958f81f-7296-4b26-9127-96fe39558cba"
      },
      "source": [
        "%%time\n",
        "training_tasks, eval_tasks, test_tasks = load_data()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 18.9 s, sys: 1.77 s, total: 20.7 s\n",
            "Wall time: 6min 53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYLEt0yrPXf9",
        "colab_type": "text"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO9FwJjFNAnO",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Utilities\n",
        "def plot_board(board, ax, title=\"\"):\n",
        "  \"\"\"\n",
        "  Plot a board on a given axis\n",
        "  \"\"\"\n",
        "  cmap = colors.ListedColormap(\n",
        "      ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
        "        '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
        "  norm = colors.Normalize(vmin=0, vmax=9)\n",
        "  \n",
        "  ax.imshow(board, cmap=cmap, norm=norm)\n",
        "  ax.grid(True,which='both',color='lightgrey', linewidth=0.5)    \n",
        "  ax.set_yticks([x-0.5 for x in range(1+board.shape[0])])\n",
        "  ax.set_xticks([x-0.5 for x in range(1+board.shape[1])])     \n",
        "  ax.set_xticklabels([])\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_title(title)\n",
        "\n",
        "def plot_one(task, ax, i,train_or_test,input_or_output):\n",
        "  \"\"\"\n",
        "  Plot one task on a given axis\n",
        "  \"\"\"\n",
        "  cmap = colors.ListedColormap(\n",
        "      ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
        "        '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
        "  norm = colors.Normalize(vmin=0, vmax=9)\n",
        "  \n",
        "  input_matrix = task[train_or_test][i][input_or_output]\n",
        "  ax.imshow(input_matrix, cmap=cmap, norm=norm)\n",
        "  ax.grid(True,which='both',color='lightgrey', linewidth=0.5)    \n",
        "  ax.set_yticks([x-0.5 for x in range(1+len(input_matrix))])\n",
        "  ax.set_xticks([x-0.5 for x in range(1+len(input_matrix[0]))])     \n",
        "  ax.set_xticklabels([])\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_title(train_or_test + ' '+input_or_output)\n",
        "    \n",
        "\n",
        "def plot_task(task):\n",
        "    \"\"\"\n",
        "    Plots the first train and test pairs of a specified task,\n",
        "    using same color scheme as the ARC app\n",
        "    \"\"\"    \n",
        "    num_train = len(task['train'])\n",
        "    fig, axs = plt.subplots(2, num_train, figsize=(3*num_train,3*2))\n",
        "    for i in range(num_train):     \n",
        "        plot_one(task, axs[0,i],i,'train','input')\n",
        "        plot_one(task, axs[1,i],i,'train','output')        \n",
        "    plt.tight_layout()\n",
        "    plt.show()        \n",
        "        \n",
        "    num_test = len(task['test'])\n",
        "    fig, axs = plt.subplots(2, num_test, figsize=(3*num_test,3*2))\n",
        "    if num_test==1: \n",
        "        plot_one(task, axs[0],0,'test','input')\n",
        "        plot_one(task, axs[1],0,'test','output')     \n",
        "    else:\n",
        "        for i in range(num_test):      \n",
        "            plot_one(task, axs[0,i],i,'test','input')\n",
        "            plot_one(task, axs[1,i],i,'test','output')  \n",
        "    plt.tight_layout()\n",
        "    plt.show() \n",
        "  \n",
        "\n",
        "def plot_board_pairs(board_pairs, labels):\n",
        "  \"\"\"\n",
        "  Plots the board pairs (for siamese networks) with their label as a title\n",
        "  \"\"\"\n",
        "\n",
        "  fig, axs = plt.subplots(len(board_pairs), 2, figsize=(8, 3 * len(board_pairs)))\n",
        "  \n",
        "  for i, pair in enumerate(board_pairs):\n",
        "    # plot a pair on a given axis\n",
        "    plot_board(pair[0], axs[i, 0], title=\"anchor\") \n",
        "    plot_board(pair[1], axs[i, 1], title=labels[i])\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_decoder_boards(board_pairs):\n",
        "  \"\"\"\n",
        "  Plots the board pairs (for siamese networks) with their label as a title\n",
        "  \"\"\"\n",
        "\n",
        "  fig, axs = plt.subplots(len(board_pairs), 2, figsize=(8, 3 * len(board_pairs)))\n",
        "\n",
        "  for i, pair in enumerate(board_pairs):\n",
        "    # plot a pair on a given axis\n",
        "    plot_board(pair[0], axs[i, 0]) \n",
        "    plot_board(pair[1], axs[i, 1])\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def display_training_curves(hist, metric='accuracy', with_val=False):\n",
        "  \"\"\"display learning curves for keras history dict, args: history dict, with val --> boolean with/without val\"\"\"\n",
        "  plt.figure(figsize=(18,6))\n",
        "\n",
        "  # accuracy plots\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(hist[metric])\n",
        "  \n",
        "  if with_val:\n",
        "    plt.plot(hist['val_' + metric])\n",
        "    plt.legend(['Train', 'Validation'])\n",
        "  \n",
        "  else:\n",
        "    plt.legend(['Train'])\n",
        "  \n",
        "  plt.title('Model accuracy')\n",
        "  plt.xlabel('EPOCH')\n",
        "  plt.ylabel('Accuracy')\n",
        "\n",
        "  # loss plots\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(hist['loss'])\n",
        "\n",
        "  if with_val:\n",
        "    plt.plot(hist['val_loss'])\n",
        "    plt.legend(['Train loss', 'Val loss'])\n",
        "  \n",
        "  else:\n",
        "    plt.legend(['Train loss'])\n",
        "  \n",
        "  plt.title('Model loss')\n",
        "  plt.xlabel('EPOCH')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9AnAL0VU2Tu",
        "colab_type": "text"
      },
      "source": [
        "# Data generating functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuTeaiVASRhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset_shapes(dataset):\n",
        "  \"\"\"\n",
        "  Returns dataset board shapes. \n",
        "  \"\"\"\n",
        "\n",
        "  shape_0 = []\n",
        "  shape_1 = []\n",
        "  \n",
        "  # check every task\n",
        "  for task in dataset:\n",
        "    boards = get_task_boards(task) # get all boards\n",
        "    shape_0 += [board.shape[0] for board in boards]\n",
        "    shape_1 += [board.shape[1] for board in boards]\n",
        "  \n",
        "  return shape_0, shape_1\n",
        "\n",
        "def get_task_boards(task, threshold_shape=(40, 40), pad=None, test=False, divide_sets=False):\n",
        "  \"\"\"\n",
        "  Get the training / testing boards of every example in a specific task. \n",
        "\n",
        "  Args: threshold_shape --> threshold shape for which \n",
        "  biggger samples won't be returned. \n",
        "  pad --> functional padding function to pad boards with (up tp threshold_size).\n",
        "  test --> bool, whether a test task or not. for output of task example.\n",
        "  divide_sets --> bool, whether to divide boards sets to training - input / output, test - input / output.\n",
        "  \"\"\"\n",
        "  \n",
        "  training_input_boards, training_output_boards, test_input_boards, test_output_boards = [], [], [], []\n",
        "\n",
        "  # train boards\n",
        "  for example in task['train']:\n",
        "    input_board = np.array(example['input'])\n",
        "    output_board = np.array(example['output'])\n",
        "\n",
        "    if((input_board.shape[0] < threshold_shape[0]) and (input_board.shape[1] < threshold_shape[1])):\n",
        "      training_input_boards.append(pad(input_board, output_shape=threshold_shape) if pad else input_board) # check for padding func\n",
        "\n",
        "    if((output_board.shape[0] < threshold_shape[0]) and (output_board.shape[1] < threshold_shape[1])):\n",
        "      training_output_boards.append(pad(output_board, output_shape=threshold_shape) if pad else output_board) # check for padding func\n",
        "\n",
        "  # test boards\n",
        "  for example in task['test']:\n",
        "    input_board = np.array(example['input'])\n",
        "\n",
        "    if not test: # check if test example\n",
        "      output_board = np.array(example['output'])\n",
        "    \n",
        "    # check whether the board is smaller then threshold shape\n",
        "    if((input_board.shape[0] < threshold_shape[0]) and (input_board.shape[1] < threshold_shape[1])):\n",
        "      test_input_boards.append(pad(input_board, output_shape=threshold_shape) if pad else input_board) # check for padding func\n",
        "\n",
        "    if((output_board.shape[0] < threshold_shape[0]) and (output_board.shape[1] < threshold_shape[1]) and (not test)):\n",
        "      test_output_boards.append(pad(output_board, output_shape=threshold_shape) if pad else output_board) # check for padding func\n",
        "  \n",
        "  # whether to divide boards:\n",
        "  if divide_sets:\n",
        "    return training_input_boards, training_output_boards, test_input_boards, test_output_boards\n",
        "  else:\n",
        "    return training_input_boards + training_output_boards + test_input_boards + test_output_boards\n",
        "\n",
        "def pad(mat, output_shape, padder=0):\n",
        "  \"\"\"\n",
        "  Pad a matrix with padder up to output_shape. Insert matrix at upper left corner.\n",
        "  \n",
        "  Args:\n",
        "  mat - np.array matrix of rank 2\n",
        "  output_shape - tuple \n",
        "  padder - int\n",
        "  \"\"\"\n",
        "\n",
        "  output_board = np.zeros(shape=output_shape) + padder # create output board and pad it\n",
        "\n",
        "  # get input shape\n",
        "  input_rows = mat.shape[0]\n",
        "  input_cols = mat.shape[1]\n",
        "\n",
        "  # if random=False, insert input matrix in upper left corner\n",
        "  output_board[:input_rows, :input_cols] = mat\n",
        "  return output_board\n",
        "\n",
        "def random_pad(mat, output_shape, padder=0):\n",
        "  \"\"\"\n",
        "  Pad a matrix with padder up to output_shape. Insert the matrix at a random location\n",
        "  \n",
        "  Args:\n",
        "  mat - np.array matrix of rank 2\n",
        "  output_shape - tuple \n",
        "  padder - int\n",
        "  seed - int\n",
        "  \"\"\"\n",
        "\n",
        "  output_board = np.zeros(shape=output_shape) + padder # create output board and pad it\n",
        "\n",
        "  # get input shape\n",
        "  input_rows = mat.shape[0]\n",
        "  input_cols = mat.shape[1]\n",
        "\n",
        "  # insert mat at a random loacation\n",
        "  # get random location\n",
        "  start_row = np.random.randint(output_shape[0] - input_rows)\n",
        "  start_col = np.random.randint(output_shape[1] - input_cols)\n",
        "  # insert\n",
        "  output_board[start_row:start_row+input_rows, start_col:start_col+input_cols] = mat\n",
        "\n",
        "  return output_board\n",
        "\n",
        "def get_all_boards(training_tasks, eval_tasks, test_tasks):\n",
        "  \"\"\"\n",
        "  Extracts all the boards from all training/eval/testing tasks. \n",
        "  \"\"\"\n",
        "\n",
        "  training_boards = []\n",
        "  for task in training_tasks:\n",
        "    training_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=pad)\n",
        "\n",
        "    # augment training set (random pad)\n",
        "    training_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "    training_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "\n",
        "  eval_boards = []\n",
        "  for task in eval_tasks:\n",
        "    eval_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=pad)\n",
        "\n",
        "    # augment eval set (random pad)\n",
        "    #eval_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "    #eval_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad)\n",
        "    \n",
        "\n",
        "  test_boards = []\n",
        "  for task in test_tasks:\n",
        "    test_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=pad, test=True)\n",
        "\n",
        "    # augment test set (random pad)\n",
        "    #test_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad, test=True)\n",
        "    #test_boards += get_task_boards(task,threshold_shape=BOARD_SIZE, pad=random_pad, test=True)\n",
        "\n",
        "  all_boards = training_boards+eval_boards+test_boards\n",
        "  return all_boards\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLXZsdbYodKl",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oofkpiCmt8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_rotated_views(board):\n",
        "  \"\"\"\n",
        "  Turns a board 90 deg counter clockwise 3 times. Returns a list length 4 with all the rotated views including the original one.\n",
        "  \"\"\"\n",
        "\n",
        "  rotates = [board]\n",
        "  for i in range(3):\n",
        "    board = np.rot90(board)\n",
        "    rotates.append(board)\n",
        "  \n",
        "  return rotates\n",
        "\n",
        "def get_rotated_data_pairs(rotated_views):\n",
        "  \"\"\"\n",
        "  Creates all the possible pairs out of the rotated views tensor and labels them. If a1..a4 are rotated vies (A.C.W) of a1, returns labels according to:\n",
        "   - (a1, a1) = 1 --> same board\n",
        "   - (a1, a2) = 2 --> 90 a.c.w rotate\n",
        "   - (a1, a3) = 3 --> 180 rotate\n",
        "   - (a1, a4) = 4 --> 270 a.c.w rotate\n",
        "   - ...\n",
        "   - (a3, a4) = 2 --> 90 a.c.w rotate\n",
        "\n",
        "  Args:\n",
        "  rotated_views = a list of 4 rotated views of the board\n",
        "\n",
        "  Returns:\n",
        "  pairs: a list of tuples of boards\n",
        "  labels: a list of labels matching each pair\n",
        "  \"\"\"\n",
        "\n",
        "  # stopping rule\n",
        "  if rotated_views == []:\n",
        "    return [], []\n",
        "\n",
        "  anchor = rotated_views[0]  # select board to compare with \n",
        "  pairs, labels = [], []\n",
        "  label = 1 # init label\n",
        "\n",
        "  # iterate over all remaining examples\n",
        "  for view in rotated_views:\n",
        "    pairs.append((anchor, view))\n",
        "    labels.append(label)\n",
        "    label += 1 # update label, views rotate a.c.w\n",
        "  \n",
        "  next_pairs, next_labels = get_rotated_data_pairs(rotated_views[1:]) # recursive call\n",
        "\n",
        "  return pairs + next_pairs, labels + next_labels\n",
        "\n",
        "def get_binary_board(board):\n",
        "  \"\"\"\n",
        "  Returns a binary board. Every non 0 value becomes 1. 0 stays 0.\n",
        "  \"\"\"\n",
        "\n",
        "  return (board != 0).astype('int32')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnKVXvD8TmZu",
        "colab_type": "text"
      },
      "source": [
        "## Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibYTel0ORlJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_shape_board(board):\n",
        "  \"\"\"\n",
        "  Reshapes board for moedl digestion. shape [*BOARD_SIZE, 1]\n",
        "  \"\"\"\n",
        "  return board.reshape([*BOARD_SIZE, 1])\n",
        "\n",
        "def plotting_shape_board(board):\n",
        "  \"\"\"\n",
        "  Reshapes the board for plotting. shape [*BOARD_SIZE]\n",
        "  \"\"\"\n",
        "  return board.reshape([*BOARD_SIZE])\n",
        "\n",
        "def get_all_pairs_reshaped(board, all_boards):\n",
        "  \"\"\"\n",
        "  Creates a list of pairs of data for the board and reshapes the boards to shape [*BOARD_SHAPE, 1] for nn digestion. For all the possible rotates and a false match\n",
        "\n",
        "  Args:\n",
        "  board - np.array\n",
        "  all_boards - all the other boards on the dataset not including this one\n",
        "\n",
        "  Returns:\n",
        "  a list of 5 tuples, 4 for the rotation part and 1 for a false match\n",
        "  \"\"\"\n",
        "\n",
        "  rotated_views = get_rotated_views(board)\n",
        "  rotated_pairs, rotated_labels = get_rotated_data_pairs(rotated_views)\n",
        "  rotated_pairs = [(model_shape_board(t[0]), model_shape_board(t[1])) for t in rotated_pairs] # reshape boards\n",
        "\n",
        "  # create false match pair\n",
        "  different_board = all_boards[np.random.randint(len(all_boards))] # generate random example\n",
        "  false_label = 0\n",
        "\n",
        "  return rotated_pairs + [[board.reshape([*BOARD_SIZE, 1]), different_board.reshape([*BOARD_SIZE, 1])]], rotated_labels + [false_label]\n",
        "\n",
        "def get_dataset_from_lists(pair_list, label_list, for_encoder=False):\n",
        "  \"\"\"\n",
        "  Creates an x,y dataset from a list of pairs and a list of labels. One hot encode labels\n",
        "\n",
        "  Args:\n",
        "  pair list: a list of pairs of images\n",
        "  label list: a list of ints - labels\n",
        "  for_encoder: bool - determains if the dataset if for the encoder\n",
        "\n",
        "  Retrurns:\n",
        "  x - [x1, x2] a list of two numpy array, each of shape [n_samples, *BOARD_SHAPE, 1] containing stacked inputs of one of the siamese twins OR only x1 if for encoder\n",
        "  y - one hot labels, only for siamese networks\n",
        "  \"\"\"\n",
        "\n",
        "  # prepare x\n",
        "  pairs_stacked = np.stack(pair_list)\n",
        "  x1 = pairs_stacked[:,0,:,:] # get one column of inputs\n",
        "  x2 = pairs_stacked[:,1,:,:] # get the second one\n",
        "  x = [x1, x2]\n",
        "\n",
        "  # prepare y\n",
        "  ohe = OneHotEncoder(sparse=False) # sparse = false is super importent!!\n",
        "  stacked_labels = np.stack(label_list).reshape([-1,1]) # stack and reshape label\n",
        "  y = ohe.fit_transform(stacked_labels)\n",
        "\n",
        "  # if the dataset is for the decoder return only one column of images\n",
        "  if not for_encoder:\n",
        "    return x, y\n",
        "  \n",
        "  else:\n",
        "    return x1\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZA3unNtUSBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_siamese_dataset(training_tasks, eval_tasks, test_tasks, get_lists=False):\n",
        "  \"\"\"\n",
        "  Creates a dataset for the siamese networks\n",
        "  \"\"\"\n",
        "\n",
        "  # extract all_boards\n",
        "  all_boards = get_all_boards(training_tasks, eval_tasks, test_tasks)\n",
        "\n",
        "  # binirize all boards\n",
        "  all_boards_binary = [get_binary_board(board) for board in all_boards] ### use all!!!\n",
        "\n",
        "  # create a list of all boards augmentation data\n",
        "  pair_list = []\n",
        "  label_list = []\n",
        "\n",
        "  # iterate over all boards\n",
        "  for i, board in enumerate(all_boards_binary):\n",
        "    board_pairs, board_labels = get_all_pairs_reshaped(board, all_boards_binary[i:]) # augment example. use only boards from here onward\n",
        "    pair_list += board_pairs\n",
        "    label_list += board_labels\n",
        "\n",
        "  ## create dataset\n",
        "  x, y = get_dataset_from_lists(pair_list, label_list)\n",
        "\n",
        "  if get_lists:\n",
        "    return x, y, pair_list, label_list\n",
        "  \n",
        "  else:\n",
        "    return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV-DsYyV-W23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### just for this notebook\n",
        "x, y, pair_list, label_list = get_siamese_dataset(training_tasks, eval_tasks, test_tasks, get_lists=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gHJfVQ69rDn",
        "colab_type": "text"
      },
      "source": [
        "# Siamese networks architecture\n",
        "- getting the right architecture\n",
        "- went for residual networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-kGomzhzlo7",
        "colab_type": "text"
      },
      "source": [
        "## Defining basic blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsL9Hpfazgxr",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title basic blocks\n",
        "def residual_encoder_block(filter_num, kernel_size, bn_moment):\n",
        "  \"\"\"\n",
        "  A functional style residual connection convolutional block.\n",
        "  \"\"\"\n",
        "\n",
        "  def block(x, filter_num, kernel_size=(3,3), bn_moment=0.9):\n",
        "    # first layer\n",
        "    c1 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(x)\n",
        "    c1 = BatchNormalization(momentum=bn_moment)(c1)\n",
        "\n",
        "    # second layer\n",
        "    c2 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(c1)\n",
        "    c2 = BatchNormalization(momentum=bn_moment)(c2)\n",
        "\n",
        "    # third layer\n",
        "    c3 = Conv2D(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(c2)\n",
        "    c3 = BatchNormalization(momentum=bn_moment)(c3)\n",
        "\n",
        "    # residual connection\n",
        "    res = c1 + c3\n",
        "\n",
        "    return res\n",
        "  \n",
        "  return lambda x: block(x, filter_num, kernel_size, bn_moment)\n",
        "\n",
        "def residual_decoder_block(filter_num, kernel_size, bn_moment, first=False):\n",
        "  \"\"\"\n",
        "  A functional style residual connection deconvolutional block.\n",
        "\n",
        "  Args:\n",
        "  first - bool, determains if it is the first block of a network\n",
        "  \"\"\"\n",
        "  \n",
        "  def block(x, filter_num, kernel_size=(3,3), bn_moment=0.9):\n",
        "    \n",
        "    # first layer\n",
        "    if first:\n",
        "      dc1 = Dense(4*4*filter_num, activation='linear')(x) # num neurons is dependent upon the number of blocks\n",
        "      dc1 = BatchNormalization(momentum=bn_moment)(dc1)\n",
        "      dc1 = Reshape(target_shape=(4,4,filter_num))(dc1) # reshaping is dependent upon the number of blocks\n",
        "    \n",
        "    else:\n",
        "      dc1 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(x)\n",
        "      dc1 = BatchNormalization(momentum=bn_moment)(dc1)\n",
        "\n",
        "    # second layer\n",
        "    dc2 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(dc1)\n",
        "    dc2 = BatchNormalization(momentum=bn_moment)(dc2)\n",
        "\n",
        "    # third layer\n",
        "    dc3 = Conv2DTranspose(filter_num, kernel_size=kernel_size, activation='relu', padding='same')(dc2)\n",
        "    dc3 = BatchNormalization(momentum=bn_moment)(dc3)\n",
        "\n",
        "    # residual connection\n",
        "    res = dc1 + dc3\n",
        "\n",
        "    return res\n",
        "\n",
        "  return lambda x: block(x, filter_num, kernel_size, bn_moment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU9XOnUA99dD",
        "colab_type": "text"
      },
      "source": [
        "## Main architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISOAGouf9ym1",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title siamese\n",
        "def get_siamese_networks_model(input_shape):\n",
        "  \"\"\"\n",
        "  Creates siamese networks model. ref paper: https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf\n",
        "  \"\"\"\n",
        "\n",
        "  # define input vectors\n",
        "  input1 = Input(input_shape, name='input1')\n",
        "  input2 = Input(input_shape, name='input2')\n",
        "\n",
        "  def get_encoder():\n",
        "    \"\"\"\n",
        "    A helper function for sharing weights between inputs\n",
        "    \"\"\"\n",
        "\n",
        "    x = Input(input_shape, name='x')  \n",
        "\n",
        "    # first block\n",
        "    block1 = residual_encoder_block(16, kernel_size=(3,3), bn_moment=0.9)(x)\n",
        "    mp1 = MaxPool2D(pool_size=(2,2), name='max_pool1')(block1)\n",
        "\n",
        "    # second block\n",
        "    block2 = residual_encoder_block(32, kernel_size=(3,3), bn_moment=0.9)(mp1)\n",
        "    mp2 = MaxPool2D(pool_size=(2,2), name='max_pool2')(block2)\n",
        "\n",
        "    # third block\n",
        "    block3 = residual_encoder_block(64, kernel_size=(3,3), bn_moment=0.9)(mp2)\n",
        "    mp3 = MaxPool2D(pool_size=(2,2), name='max_pool3')(block3)\n",
        "\n",
        "    # flatten\n",
        "    flat = Flatten()(mp3)\n",
        "\n",
        "    # dense\n",
        "    dense = Dense(DENSE_REP_SIZE, activation='sigmoid', name='dense_rep')(flat)\n",
        "    bn_dense = BatchNormalization(momentum=0.9)(dense)\n",
        "\n",
        "    return tf.keras.Model(inputs=x, outputs=bn_dense)\n",
        "\n",
        "  # get shared encoder\n",
        "  encoder = get_encoder()\n",
        "  \n",
        "  # get feature vectors\n",
        "  v1 = encoder(input1)\n",
        "  v2 = encoder(input2)\n",
        "\n",
        "  # compute L1 loss\n",
        "  L1_Layer = Lambda(lambda tensors: tf.math.abs(tensors[0] - tensors[1]))\n",
        "  L1_diff = L1_Layer([v1, v2])\n",
        "\n",
        "  # compute probs\n",
        "  probs = Dense(5, activation='softmax')(L1_diff)\n",
        "\n",
        "  siamese_net = tf.keras.Model(inputs=[input1, input2], outputs=probs)\n",
        "\n",
        "  # compile\n",
        "  siamese_net.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
        "  return siamese_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEKcGPtKgbM6",
        "colab_type": "text"
      },
      "source": [
        "## Training siamese networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0kKJwuy2t2R",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "sn = get_siamese_networks_model([*BOARD_SIZE, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfvNty6FbxNA",
        "colab_type": "code",
        "outputId": "a8229a67-7eed-460e-f986-060490d00a8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "source": [
        "hist = sn.fit(x=x, y=y, epochs=30, batch_size=32, shuffle=True, validation_split=0.2)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "3084/3084 [==============================] - 31s 10ms/step - loss: 0.1507 - categorical_accuracy: 0.9557 - val_loss: 0.3695 - val_categorical_accuracy: 0.9314\n",
            "Epoch 2/30\n",
            "3084/3084 [==============================] - 31s 10ms/step - loss: 0.0913 - categorical_accuracy: 0.9735 - val_loss: 0.2006 - val_categorical_accuracy: 0.9623\n",
            "Epoch 3/30\n",
            "3084/3084 [==============================] - 30s 10ms/step - loss: 0.0703 - categorical_accuracy: 0.9792 - val_loss: 0.1431 - val_categorical_accuracy: 0.9687\n",
            "Epoch 4/30\n",
            "3084/3084 [==============================] - 31s 10ms/step - loss: 0.0605 - categorical_accuracy: 0.9818 - val_loss: 0.1385 - val_categorical_accuracy: 0.9618\n",
            "Epoch 5/30\n",
            "3084/3084 [==============================] - 30s 10ms/step - loss: 0.0503 - categorical_accuracy: 0.9846 - val_loss: 0.3567 - val_categorical_accuracy: 0.9329\n",
            "Epoch 6/30\n",
            "3084/3084 [==============================] - 31s 10ms/step - loss: 0.0495 - categorical_accuracy: 0.9849 - val_loss: 0.1973 - val_categorical_accuracy: 0.9378\n",
            "Epoch 7/30\n",
            " 519/3084 [====>.........................] - ETA: 22s - loss: 0.0383 - categorical_accuracy: 0.9878"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-8b5f96651772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \"\"\"\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    517\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \"\"\"\n\u001b[1;32m    960\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDuSUF8lRP0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_training_curves(hist.history, metric='categorical_accuracy', with_val=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkiOZvd7gOkP",
        "colab_type": "text"
      },
      "source": [
        "# Board generator (decoder) architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ir0OqLyUPa_",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "def get_encoder_from_siamese(siamese):\n",
        "  \"\"\"\n",
        "  Creates a board encoder from the siamese networks model. Maps board --> feature vector of dim 512\n",
        "  \"\"\"\n",
        "\n",
        "  # create encoder from all layers up to dense\n",
        "  model = tf.keras.models.Sequential([\n",
        "                                      tf.keras.layers.InputLayer(input_shape=[*BOARD_SIZE, 1]),\n",
        "                                      siamese.layers[2] # all sequential layers from siamese networks model\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "def get_decoder(siamese, copy_encoder_weights=True):\n",
        "  \"\"\"\n",
        "  Creates a decoder from siamese netwok model. Maps feature vector of dim 512 --> board. \n",
        "  Same architecture from encoder is preserved\n",
        "  \"\"\"\n",
        "\n",
        "  # build model\n",
        "  inp = Input([DENSE_REP_SIZE])\n",
        "\n",
        "  # first_block\n",
        "  block1 = residual_decoder_block(64, kernel_size=(3,3), bn_moment=0.9, first=True)(inp)\n",
        "  us1 = UpSampling2D(size=(2,2))(block1)\n",
        "\n",
        "  block2 = residual_decoder_block(32, kernel_size=(3,3), bn_moment=0.9)(us1)\n",
        "  us2 = UpSampling2D(size=(2,2))(block2)\n",
        "\n",
        "  block3 = residual_decoder_block(16, kernel_size=(3,3), bn_moment=0.9)(us2)\n",
        "\n",
        "  output = Conv2D(1, kernel_size=(1,1), activation='sigmoid')(block3)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inp, outputs=output)\n",
        "\n",
        "  if copy_encoder_weights:\n",
        "    # set weights from the encoder\n",
        "    enc_layers = siamese.layers[2].layers # get encoder layers\n",
        "    \n",
        "    for i, layer in enumerate(model.layers[::-1]):\n",
        "      if(2 >= len(layer.weights) > 0): # trainable layer and not BN layer\n",
        "        if layer.weights[0].shape[0] < 100: # deconv layers\n",
        "          enc_layer_weigths = enc_layers[i].get_weights() # get weights from encoder\n",
        "          bias = np.zeros(layer.get_weights()[1].shape) # init new bias\n",
        "          w = enc_layer_weigths[0].transpose([1,0,2,3]) # transpose weights\n",
        "          layer.set_weights([w, bias]) # set decoder weights\n",
        "\n",
        "        else:\n",
        "          # dense layer\n",
        "          enc_layer_weigths = enc_layers[i].get_weights() # get weights from encoder\n",
        "          bias = np.zeros(layer.get_weights()[1].shape) # init new bias\n",
        "          w = enc_layer_weigths[0].transpose() # transpose weights\n",
        "          layer.set_weights([w, bias]) # set decoder weights\n",
        "\n",
        "  # compile\n",
        "  model.compile(loss=pixelwise_error_loss, optimizer='adam', metrics=[pixelwise_auc])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def pixelwise_error_loss(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Custom loss function. Pixelwise error from the true image. ||y_true - y_pred||\n",
        "  \"\"\"\n",
        "\n",
        "  return tf.keras.backend.sum((y_true - y_pred) ** 2) / y_pred.shape[0]\n",
        "\n",
        "\n",
        "def pixelwise_accuracy(y_true, y_pred, threshold=0.8):\n",
        "  \"\"\"\n",
        "  Custom accuracy function. Pixelwise accuracy of predicting 0 / 1\n",
        "  \"\"\"\n",
        "  \n",
        "  tp = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred >= threshold), 'float32'), y_true)) # cast boolean to binary\n",
        "  tn = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred < threshold), 'float32'), tf.cast((y_true == 0), 'float32')))\n",
        "  n_predictions = BOARD_SIZE[0] * BOARD_SIZE[1]\n",
        "\n",
        "  return ((tp + tn) / n_predictions) / y_pred.shape[0]\n",
        "\n",
        "\n",
        "def pixelwise_sensitivity(y_true, y_pred, threshold=0.8):\n",
        "  \"\"\"\n",
        "  Pixelwise sensitivity of correctly predicting 1s. true positives / positives\n",
        "  \"\"\"\n",
        "\n",
        "  tp = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred >= threshold), 'float32'), y_true)) \n",
        "  p = tf.cast(tf.math.reduce_sum(y_true), 'float32')\n",
        "  \n",
        "  return tp / p\n",
        "\n",
        "\n",
        "def pixelwise_fpr(y_true, y_pred, threshold=0.8):\n",
        "  \"\"\"\n",
        "  Pixelwise false positive rate of wrongly predicting 1s. false positives / negatives\n",
        "  \"\"\"\n",
        "\n",
        "  fp = tf.math.reduce_sum(tf.math.multiply(tf.cast((y_pred >= threshold), 'float32'), tf.cast((y_true==0), 'float32'))) # cast boolean to binary\n",
        "  n = tf.cast(tf.math.reduce_sum(tf.cast((y_true == 0), 'float32')), 'float32')\n",
        "  \n",
        "  return fp / n\n",
        "\n",
        "\n",
        "def pixelwise_auc(y_true, y_pred, num_thresholds=10):\n",
        "  \"\"\"\n",
        "  Pixelwise auc score. \n",
        "  \"\"\"\n",
        "\n",
        "  sensitivity = []\n",
        "  fpr = []\n",
        "\n",
        "  # compute graph points\n",
        "  for threshold in np.linspace(1, 0, num=num_thresholds):\n",
        "    sensitivity.append(pixelwise_sensitivity(y_true, y_pred, threshold=threshold))\n",
        "    fpr.append(pixelwise_fpr(y_true, y_pred, threshold=threshold))\n",
        "  \n",
        "  # compute trapeze area\n",
        "  trapeze = []\n",
        "  for i in range(len(sensitivity)- 1):\n",
        "    area = tf.multiply(sensitivity[i], fpr[i+1] - fpr[i])\n",
        "    trapeze.append(area)\n",
        "  \n",
        "  auc_score = tf.add_n(trapeze) \n",
        "\n",
        "  return auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNIBD7hmoL-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dec = get_decoder(sn, copy_encoder_weights=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uBo_DSMrXDZ",
        "colab_type": "text"
      },
      "source": [
        "## Decoder dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmNwACcSraxu",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#@title decoder dataset\n",
        "def get_decoder_dataset(siamese, encoder_dataset):\n",
        "  \"\"\"\n",
        "  Creates the decoder dataset from the encoder.\n",
        "  \n",
        "  Args:\n",
        "  siamese - siamese model to extract encoder from\n",
        "  encoder dataset - boards to encoder to feature vectors\n",
        "\n",
        "  Returns:\n",
        "  x - boards feature vectors, predicted by encoder\n",
        "  y - boards \n",
        "  \"\"\"\n",
        "  \n",
        "  enc = get_encoder_from_siamese(siamese) # fetch encoder\n",
        "\n",
        "  # build dataset\n",
        "  x = enc.predict(encoder_dataset) # get feature vectors\n",
        "  y = encoder_dataset # boards to recreate\n",
        "\n",
        "  return x,y\n",
        "\n",
        "# create encoder dataset from the old pair and label lists\n",
        "encoder_dataset = get_dataset_from_lists(pair_list, label_list, for_encoder=True) \n",
        "\n",
        "# create decoder dataset from encoder\n",
        "decoder_x, decoder_y = get_decoder_dataset(sn, encoder_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg9iv1vPtEEe",
        "colab_type": "code",
        "outputId": "6d914ebf-7468-48e1-a3b6-f5bae4b0234f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        }
      },
      "source": [
        "hist_decoder = dec.fit(x=decoder_x, y=decoder_y, batch_size=DECODER_BATCH_SIZE, shuffle=True, epochs=10, validation_split=0.2)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-01e640f77e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDECODER_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    850\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    <ipython-input-43-3a7e5d7cc25d>:66 pixelwise_error_loss\n        return tf.keras.backend.sum((y_true - y_pred) ** 2) / y_pred.shape[0]\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:988 binary_op_wrapper\n        y, dtype_hint=x.dtype.base_dtype, name=\"y\")\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1283 convert_to_tensor_v2\n        as_ref=False)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1341 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:321 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:262 constant\n        allow_broadcast=True)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:300 _constant_impl\n        allow_broadcast=allow_broadcast))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:439 make_tensor_proto\n        raise ValueError(\"None values not supported.\")\n\n    ValueError: None values not supported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pHSQ3pYcTII",
        "colab_type": "text"
      },
      "source": [
        "# Check regular encoder-decoder architecture\n",
        "## For checking some hyperparameters and deciding which adecoder architecture to use\n",
        "\n",
        "## Some takeaways:\n",
        "- adding two sets of randomly padded data helps alot. more doesnt help\n",
        "- small decoder batch size (4-16) really helps\n",
        "- adding more neurons to the compressed represantation (512 --> 1024) really helps\n",
        "- removing the last layers batch normalization really helps\n",
        " - removing bn before compressed representation isnt good\n",
        "- penelizing mistaking 1's for 0's more doesnt help at all\n",
        "- changing last layer activation to linear and clipping to [0,1] helps to some extent\n",
        "- chagning middle to relu - really bad!!!\n",
        "- addind another conv-deconv layer at the beggining helps a little\n",
        "- adding another conv-deconv layer at the end doesnt help at all\n",
        "- residual layers - really cool! used the architecture from this papare -- >https://github.com/omerhac/arc_challenge/blob/master/deep%20residual%20conv-deconv%20network.pdf\n",
        "- more types of data augmentation....\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzGZn1y0vt54",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRvqZ5OczS9J",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#@title encoder-decoder baseline\n",
        "def get_encoder_decoder():\n",
        "  \"\"\"\n",
        "  Create an encoder decoer \"normal\" architecture, with residual connections\n",
        "  \"\"\"\n",
        "  inp = tf.keras.layers.Input([*BOARD_SIZE, 1])\n",
        "  \n",
        "  # encoder\n",
        "  block1 = residual_encoder_block(16, kernel_size=(3,3), bn_moment=0.9)(inp)\n",
        "  mp1 = MaxPool2D(pool_size=(2,2))(block1)\n",
        "\n",
        "  block2 = residual_encoder_block(32, kernel_size=(3,3), bn_moment=0.9)(mp1)\n",
        "  mp2 = MaxPool2D(pool_size=(2,2))(block2)\n",
        "\n",
        "  block3 = residual_encoder_block(64, kernel_size=(3,3), bn_moment=(0.9))(mp2)\n",
        "\n",
        "  # flatten\n",
        "  flat = Flatten()(block3)\n",
        "\n",
        "  # dense representation\n",
        "  dense = Dense(1024, activation='sigmoid')(flat)\n",
        "  dense = BatchNormalization(momentum=0.9)(dense)\n",
        "\n",
        "\n",
        "  # decoder \n",
        "\n",
        "  # first_block\n",
        "  block1 = residual_decoder_block(64, kernel_size=(3,3), bn_moment=0.9, first=True)(dense)\n",
        "  us1 = UpSampling2D(size=(2,2))(block1)\n",
        "\n",
        "  block2 = residual_decoder_block(32, kernel_size=(3,3), bn_moment=0.9)(us1)\n",
        "  us2 = UpSampling2D(size=(2,2))(block2)\n",
        "\n",
        "  block3 = residual_decoder_block(16, kernel_size=(3,3), bn_moment=0.9)(us2)\n",
        "\n",
        "  output = Conv2D(1, kernel_size=(1,1), activation='sigmoid')(block3)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inp, outputs=output)\n",
        "  \n",
        "\n",
        "  # compile\n",
        "  model.compile(loss=pixelwise_error_loss, optimizer='adam', metrics=[pixelwise_auc])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mharVxoMMqJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ed = get_encoder_decoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb0_gTC8nIdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ed_hist = ed.fit(x=decoder_y, y=decoder_y, batch_size=8, shuffle=True, epochs=20, validation_split=0.2) # get image --> predict image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9ii2z4hh9de",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## check predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eytnCHw0iUIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boards = decoder_y[-10:]\n",
        "\n",
        "# predict\n",
        "predictions = ed.predict(boards)\n",
        "\n",
        "# reshape\n",
        "boards = [plotting_shape_board(board) for board in boards]\n",
        "predictions = [plotting_shape_board(board) for board in predictions]\n",
        "pairs = zip(boards, predictions)\n",
        "\n",
        "plot_decoder_boards(list(pairs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWKPRMGbWEBo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "outputId": "15ac36ac-3e86-4ebb-ebee-f905314d21e5"
      },
      "source": [
        "for i, layer in enumerate(ed.layers):\n",
        "  print(i, layer.name)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 input_3\n",
            "1 conv2d_10\n",
            "2 batch_normalization_19\n",
            "3 conv2d_11\n",
            "4 batch_normalization_20\n",
            "5 conv2d_12\n",
            "6 batch_normalization_21\n",
            "7 tf_op_layer_AddV2_6\n",
            "8 max_pooling2d\n",
            "9 conv2d_13\n",
            "10 batch_normalization_22\n",
            "11 conv2d_14\n",
            "12 batch_normalization_23\n",
            "13 conv2d_15\n",
            "14 batch_normalization_24\n",
            "15 tf_op_layer_AddV2_7\n",
            "16 max_pooling2d_1\n",
            "17 conv2d_16\n",
            "18 batch_normalization_25\n",
            "19 conv2d_17\n",
            "20 batch_normalization_26\n",
            "21 conv2d_18\n",
            "22 batch_normalization_27\n",
            "23 tf_op_layer_AddV2_8\n",
            "24 flatten_1\n",
            "25 dense_2\n",
            "26 batch_normalization_28\n",
            "27 dense_3\n",
            "28 batch_normalization_29\n",
            "29 reshape_1\n",
            "30 conv2d_transpose_8\n",
            "31 batch_normalization_30\n",
            "32 conv2d_transpose_9\n",
            "33 batch_normalization_31\n",
            "34 tf_op_layer_AddV2_9\n",
            "35 up_sampling2d_2\n",
            "36 conv2d_transpose_10\n",
            "37 batch_normalization_32\n",
            "38 conv2d_transpose_11\n",
            "39 batch_normalization_33\n",
            "40 conv2d_transpose_12\n",
            "41 batch_normalization_34\n",
            "42 tf_op_layer_AddV2_10\n",
            "43 up_sampling2d_3\n",
            "44 conv2d_transpose_13\n",
            "45 batch_normalization_35\n",
            "46 conv2d_transpose_14\n",
            "47 batch_normalization_36\n",
            "48 conv2d_transpose_15\n",
            "49 batch_normalization_37\n",
            "50 tf_op_layer_AddV2_11\n",
            "51 conv2d_19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kujm22z3y0qa",
        "colab_type": "text"
      },
      "source": [
        "# Interleaved training (decoder/encoder/deocoder..)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly1E5KG8A21u",
        "colab_type": "text"
      },
      "source": [
        "## Loss and gradient functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGfEknXVA5DM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simese_loss(model, x, y, training):\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5XjrxlkyZOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def copy_decoder_to_siamese(siamese, decoder):\n",
        "  \"\"\"\n",
        "  Copys the weights from the decoder to the siamese networks model\n",
        "  \"\"\"\n",
        "\n",
        "  decoder_layers = decoder.layers # get decoder layers\n",
        "  siamese_layers = siamese.layers[2].layers # siamese encoder layers\n",
        "\n",
        "  for i, layer in enumerate(decoder_layers[::-1]):\n",
        "      if(2 >= len(layer.weights) > 0): # trainable layer and not BN layer\n",
        "        if layer.weights[0].shape[0] < 100: # deconv layers\n",
        "          decoder_layer_weigths = layer.get_weights() # get weights from decoder\n",
        "          bias = np.zeros(siamese_layers[i].get_weights()[1].shape) # init new bias\n",
        "          w = decoder_layer_weigths[0].transpose([1,0,2,3]) # transpose weights\n",
        "          siamese_layers[i].set_weights([w, bias]) # set siamese weights\n",
        "\n",
        "        else:\n",
        "          # dense layer\n",
        "          decoder_layer_weigths = layer.get_weights() # get weights from decoder\n",
        "          bias = np.zeros(siamese_layers[i].get_weights()[1].shape) # init new bias\n",
        "          w = decoder_layer_weigths[0].transpose() # transpose weights\n",
        "          siamese_layers[i].set_weights([w, bias]) # set siamese weights\n",
        "\n",
        "\n",
        "def copy_siamese_to_decoder(siamese, decoder):\n",
        "  \"\"\"\n",
        "  Copys the weights from the siamese networks model to the decoder\n",
        "  \"\"\"\n",
        "\n",
        "  decoder_layers = decoder.layers # get decoder layers\n",
        "  siamese_layers = siamese.layers[2].layers # siamese encoder layers\n",
        "\n",
        "  for i, layer in enumerate(decoder_layers[::-1]):\n",
        "      if(2 >= len(layer.weights) > 0): # trainable layer and not BN layer\n",
        "        if layer.weights[0].shape[0] < 100: # deconv layers\n",
        "          siamese_layer_weigths = siamese_layers[i].get_weights() # get weights from siamese encoder\n",
        "          bias = np.zeros(layer.get_weights()[1].shape) # init new bias\n",
        "          w = siamese_layer_weigths[0].transpose([1,0,2,3]) # transpose weights\n",
        "          layer.set_weights([w, bias]) # set decoder weights\n",
        "\n",
        "        else:\n",
        "          # dense layer\n",
        "          siamese_layer_weigths = siamese_layers[i].get_weights() # get weights from siamese encoder\n",
        "          bias = np.zeros(layer.get_weights()[1].shape) # init new bias\n",
        "          w = siamese_layer_weigths[0].transpose() # transpose weights\n",
        "          layer.set_weights([w, bias]) # set decoder weights\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2ucFsFkzIkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create new models with same weights\n",
        "EPOCHS = 30\n",
        "sn = get_siamese_networks_model([*BOARD_SIZE, 1])\n",
        "decoder = get_decoder(sn, copy_encoder_weights=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pfDn78N4ecn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "SN_BATCH_SIZE = 64\n",
        "DEC_BATCH_SIZE = 4\n",
        "DATA_SPLIT = 5\n",
        "SN_STEPS_PER_EPOCH = len(x) // SN_BATCH_SIZE\n",
        "DEC_STEPS_PER_EPOCH = len(decoder_x) // DEC_BATCH_SIZE\n",
        "\n",
        "# even fancier training loop\n",
        "for i in range(EPOCHS):\n",
        "  print(\"training... epoch num: {}\".format(i))\n",
        "\n",
        "  for i in range(40):\n",
        "    # train siamese\n",
        "    _ = sn.fit(x=x, y=y, epochs=1, batch_size=SN_BATCH_SIZE, shuffle=True, steps_per_epoch=(780//DATA_SPLIT))\n",
        "    \n",
        "    # copy weights\n",
        "    copy_siamese_to_decoder(sn, decoder)\n",
        "\n",
        "    # get new decoder dataset\n",
        "    decoder_x, decoder_y = get_decoder_dataset(sn, encoder_dataset) # encoder dataset is the same as before\n",
        "\n",
        "    # train decoder\n",
        "    _ = decoder.fit(x=decoder_x, y=decoder_y, epochs=1, batch_size=DEC_BATCH_SIZE, shuffle=True, steps_per_epoch=(24000//DATA_SPLIT)) #### WOW!!! use smaller batch size, WOHOOO!!\n",
        "\n",
        "    # copy weights\n",
        "    copy_decoder_to_siamese(sn, decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BefQARZeJ6OK",
        "colab_type": "text"
      },
      "source": [
        "# Predicting from difference in boards\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Api-_erL_rs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task_training_input, task_training_output, task_test_input, task_test_output = get_task_boards(training_tasks[0], pad=pad, divide_sets=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmT5I2c2KNS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# toy dataset (x is the dataset for the siamese networks)\n",
        "EXAMPLE = 99\n",
        "anchor = x[1][EXAMPLE]\n",
        "rotate_once_anchor = x[1][EXAMPLE + 1]\n",
        "rotate_twice_anchor = x[1][EXAMPLE + 2]\n",
        "rotate_three_anchor = x[1][EXAMPLE + 3]\n",
        "\n",
        "toy = np.stack([anchor, rotate_once_anchor, rotate_twice_anchor, rotate_three_anchor])\n",
        "\n",
        "# plot\n",
        "fig, axs = plt.subplots(1,4)\n",
        "plot_board(plotting_shape_board(anchor), axs[0])\n",
        "plot_board(plotting_shape_board(rotate_once_anchor), axs[1])\n",
        "plot_board(plotting_shape_board(rotate_twice_anchor), axs[2])\n",
        "plot_board(plotting_shape_board(rotate_three_anchor), axs[3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVmhKDCOOEYJ",
        "colab_type": "text"
      },
      "source": [
        "## Define predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uVadzm3Lw3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictor(decoder):\n",
        "  \"\"\"\n",
        "  Builds a detector which takes a board and a rules vector and predicts output board.\n",
        "\n",
        "  Args:\n",
        "  decoder --> trained decoder \n",
        "  \"\"\"\n",
        "\n",
        "  # inputs\n",
        "  board_vec = Input([DENSE_REP_SIZE])\n",
        "  rules_vec = Input([DENSE_REP_SIZE])\n",
        "\n",
        "  # decoder\n",
        "  decoder = get_decoder(sn, copy_encoder_weights=False)\n",
        "\n",
        "  # predict\n",
        "  transformed_board = board_vec + rules_vec\n",
        "  prediction = decoder(transformed_board)\n",
        "\n",
        "  model = tf.keras.Model(inputs=[board_vec, rules_vec], outputs=prediction)\n",
        "\n",
        "  model.compile(loss=pixelwise_error_loss, optimizer='adam', metrics=[pixelwise_auc])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxNjK2-Y2j1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## encode all toy datset\n",
        "encoder = get_encoder_from_siamese(sn)\n",
        "encoded_boards = encoder.predict(toy)\n",
        "\n",
        "# get diffrance\n",
        "diff = np.abs(encoded_boards[:-1] - encoded_boards[1:])\n",
        "\n",
        "# predict difference\n",
        "predictor = get_predictor(dec)\n",
        "predictions = predictor.predict([encoded_boards[:-1], diff])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS4vRLrn7cVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig,axs = plt.subplots(1,3)\n",
        "plot_board(plotting_shape_board(predictions[0]), axs[0])\n",
        "plot_board(plotting_shape_board(predictions[1]), axs[1])\n",
        "plot_board(plotting_shape_board(predictions[2]), axs[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVk7R9y28IZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = dec.predict(encoded_boards)\n",
        "fig,axs = plt.subplots(1,4)\n",
        "plot_board(plotting_shape_board(p[0]), axs[0])\n",
        "plot_board(plotting_shape_board(p[1]), axs[1])\n",
        "plot_board(plotting_shape_board(p[2]), axs[2])\n",
        "plot_board(plotting_shape_board(p[3]), axs[3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqt_VZ8uKhA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# permutate\n",
        "diffs = np.stack([diff[0], diff[1], diff[2],diff[0], diff[1], diff[2]])\n",
        "eb = np.stack([encoded_boards[0], encoded_boards[0], encoded_boards[0], encoded_boards[1], encoded_boards[1], encoded_boards[1]])\n",
        "t = np.stack([toys[1], toys[1], toys[1], toys[2], toys[2], toys[2]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-9EPJd980k0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_ = predictor.fit(x=[eb, diffs], y=t, batch_size=6, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VSfnRi8HReV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict again\n",
        "predictions = predictor.predict([encoded_boards[:-1], mean_diff])\n",
        "fig,axs = plt.subplots(1,3)\n",
        "plot_board(plotting_shape_board(predictions[0]), axs[0])\n",
        "plot_board(plotting_shape_board(predictions[1]), axs[1])\n",
        "plot_board(plotting_shape_board(predictions[2]), axs[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS2sjylzJjjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}